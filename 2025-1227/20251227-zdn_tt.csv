headline,mainEntityOfPage,image,datePublished,dateModified,author,media_en,media_jp,str_count,body,images,external_links
オープンソースの「甘いわな」　その自由がもたらす“不自由な現実”（TechTargetジャパン）,https://news.yahoo.co.jp/articles/8a38fc98eedbceb671eda073c6a5779bad0396c7,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20251227-00000007-zdn_tt-000-1-view.jpg?exp=10800,2025-12-27T20:05:09+09:00,2025-12-27T20:05:09+09:00,TechTargetジャパン,zdn_tt,TechTargetジャパン,2822,"（写真：TechTargetジャパン）
2025年5月末にシンガポールで開催されたカンファレンス「ATxEnterprise」において、オープンソースソフトウェア（OSS）がテーマとなったパネルディスカッションが行われた。OSSの可能性について壇上で議論が交わされたが、業界のリーダーたちはOSSに対する無邪気な期待にくぎを刺し、導入の際の注意事項とその対策について語った。

　オープンソースデータベースベンダーPingCapのチーフアーキテクト、サニー・ベインズ氏は、企業間でOSSの採用が進んでいる主な理由として、無料で提供されていること以外に、“コントロールのしやすさ”を挙げる。プロプライエタリ（ソースコード非公開の商用製品）なソフトウェアの場合、ユーザー企業はベンダーに縛られがちだが、OSSにはそれがない。

　ベインズ氏は、「全てのプログラムのソースコードを公開し、コピー、改変、再配布を許可するべきだ」という“自由ソフトウェア運動”を推進したリチャード・ストールマン氏の有名なエピソードを引き合いに出す。「ストールマン氏は、不具合を起こしたプリンタのデバイスドライバ（制御ソフトウェア）を自分で修正しようとしたが、ソースコードが非公開だったためにそれができなかった。このことから分かるように、ソースコードが公開されているOSSには、ユーザー企業自らが問題をすぐに修正できるという大きな利点がある。ベンダーのパッチ（修正プログラム）配布を待つ必要がない」と語った。

　しかし、OSSには以下で挙げる課題があり、適切なコントロールを欠けば、重大な問題を引き起こす。
OSSの“見過ごせない”問題点とは？　対策は？
オープンソースのAPI（アプリケーションプログラミングインタフェース）ゲートウェイを提供するKongのエンジニアリングエグゼクティブ、サジュ・ピライ氏は次のように語る。「プロプライエタリソフトウェアは、ベンダーの保証やセキュリティが組み込まれた“製品”だ。しかしOSSは、あくまでも有志による“プロジェクトの成果物”なので、そのままの使用や再配布にはリスクがある。ソースコードに問題があったり、顧客の暗黙の期待に沿っていなかったりすれば、ユーザー企業や公共インフラに深刻なダメージを与える可能性がある。本番環境に投入するには、チェックや改良のための多大な労力が必要だ」

　ソフトウェアのリライアビリティー（信頼性）テストツールを提供するWatermelon SoftwareのCTO（最高技術責任者)、ハルプリート・シン氏は、OSSを導入する場合、リスク軽減のために、結合テスト（複数の機能やコンポーネントを組み合わせたときの動作テスト）や、スケーラビリティ（拡張性）と信頼性のテストなど、多層的な検証の仕組みが必要だと語る。「全てのテストをクリアできなければ、悪用されかねない脆弱（ぜいじゃく）性の懸念を払拭できない」と警告する。
規制の厳しい業界での制限
インドの銀行IDFC FIRST Bankで、生成AI（AI：人工知能）部門のチーフマネジャーを務めるシュバム・アグニホトリ氏は、銀行のような規制の厳しい業界でのOSSの使用は困難だと指摘する。「金融機関は、コンプライアンスやセキュリティ上の要件が多いので、OSSをそのまま使うことは不可能だ。改良のための膨大な作業が必要になる」

　対策として、Kongのピライ氏は、OSSで十分なのか、ベンダーからのサポートが組み込まれたOSSを基にした商用版の方がいいのか、事前の検討を企業に勧める。
ライセンス
別の課題がライセンスだ。シン氏は「利用するオープンライセンスについてしっかり理解しておくべきだ」と語る。同氏は、Watermelon Softwareが提供するツールの中核だったOSSのライセンスが変更されてしまい、やむを得ず古いバージョンを使用した経験を紹介した。

　対策として、シン氏は「OSSを採用する場合、アーキテクチャと設計に、不測の事態に対応できるだけの柔軟性を持たせる必要がある」と語る。そして、こうしたリスクに対する“保険”として、商用版の使用を勧める。

　別の対策として、ピライ氏は、OSSのライセンスを精査する仕組みの導入を勧める。Kongでは、開発者がライブラリ（プログラムの部品群）を導入して、ソフトウェア部品表（SBOM）が変更された場合、関連するライセンスについて法務およびコンプライアンスチームが精査する仕組みがあり、承認なしでは使用できないという。 

　ベインズ氏は、ライセンスを“地雷”に例え、慎重な取り扱いの必要性を強調し、PingCapが取った対策を紹介した。同社はライセンスのトラブルを避けるために、提供する製品の中核だったストレージ技術をオープンソースソフトウェア管理団体Cloud Native Computing Foundation（CNCF）に寄贈したという。これは、特定の企業にライセンスを握られないための逆転の発想とも言える。
生成AIの影響は？
生成AIがOSSに与える影響についても話題として取り上げられた。パネリストたちは、生成AIの有用性を認めつつも、安易に信頼するのでなく、ソースコード検証の必要性を強調した。

　シン氏は次のように語る。「AIモデルが事実に基づかない回答を出力するハルシネーションの可能性がある以上、絶対的な信頼はできない。AIが生成したソースコードには、その機能を保証する何らかの追加の仕組みが必要だ」

　アグニホトリ氏は、単一のコードベースで構成されたモノリシックアーキテクチャのソフトウェアを、生成AIを使ってマイクロサービス化（疎結合で独立した部品に分割する）した自身の経験を紹介した。「ソースコードの見た目は本当に美しかったが、ソフトウェアとしては使えなかった。生成AIは開発者の代替となるレベルに達していない」と語る。

　ピライ氏は、AIによって開発者の役割が変化すると予想する。「ソースコードを書くのではなく、AIが書いたソースコードのレビューを担当するようになるだろう」と語る。

　ベインズ氏もそれに同意する。「PingCapはAIを活用して質問に答えるサポートチームの負担を軽減している。AIが最前線に立つのではなく、一歩引いたサポート役を担う。プログラミングにおいてもそうなるだろう」 

　最終的に、パネリストたち全員が、OSSはソフトウェア開発とイノベーション創出の原動力ではあるものの、導入時の落とし穴を回避するためには、デューデリジェンス（適正な評価手続き）、ライセンス、セキュリティ、継続的な管理への投資が必要であるとの結論で一致した。

翻訳・編集協力：雨輝ITラボ（株式会社リーフレイン）
TechTargetジャパン",[],[]
HDDでAIはもう動かない？　“100TB超えSSD”が必要になる理由（TechTargetジャパン）,https://news.yahoo.co.jp/articles/b6885511892f7cda519f912446d98a70a9958275,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20251227-00000006-zdn_tt-000-1-view.jpg?exp=10800,2025-12-27T08:05:10+09:00,2025-12-27T08:05:10+09:00,TechTargetジャパン,zdn_tt,TechTargetジャパン,3128,"（写真：TechTargetジャパン）
今日の人工知能（AI）関連の技術は、あらゆるものが大規模化している。AIモデルの学習データを格納するSSDも例外ではない。2024年を通じてキオクシア、Micron Technology、Samsung Electronics、SK hynixといったNAND型フラッシュメモリベンダー、Western Digital（2025年2月にフラッシュメモリ事業をSandiskに分社化）が、SSD製品の大容量化を加速させた。

　これほど大容量のSSDが求められる理由は、データの転送路容量（帯域幅）にあるわけではない。AIモデルが扱う巨大なデータセットを格納できる容量と、HDDをはるかに上回る応答速度（レイテンシ）が重要視されているのだ。
なぜAIブームで「大容量SSD」が盛り上がるのか
近年のWestern Digitalにとって、ハイパースケーラー（大規模データセンター事業者）は収益の柱となっている。データセンター市場の成長と大規模なAIシステムの登場は、同社が提供するHDDの平均容量に大きな影響を与えてきた。

　ハイパースケーラーは可能な限り大容量のHDDを大量に調達しているが、さらなる処理速度を追求するために、ストレージとしてSSDを追加し始めている。SSDはHDDと同様に、小さなスペースに大容量を詰め込むことに長けている上、連続したデータの読み出しにおいて、HDDよりもはるかに優れた応答速度を実現する。ハイパースケーラーはSSDにも極めて大きな容量を求めており、NAND型フラッシュメモリはその要求を実現する技術だ。

　大容量SSDに関する取り組みの例として、Meta Platformsが2025年3月に公開した報告書が挙げられる。同社はアクセス頻度の低いデータを保存するニアラインストレージを対象に、1つのメモリセルに4bitを格納するQLC（クアッドレベルセル）方式のNAND型フラッシュメモリを搭載したSSDと、従来のHDDを、費用、性能、電力効率の観点で比較した。その結果に基づいて、同社はデータストレージの構成を最適化する新たなアプローチを提案している。具体的には、アクセス頻度が比較的高い、既存のTLC（トリプルレベルセル）方式のSSDと、大容量だが低速なHDDの間に、大容量のQLC方式SSDで構成された新たなストレージ層を設けるというものだ。TLCは、1つのメモリセルに3bitを格納する方式を指す。

　データセンターは、ハードウェアの設置スペースと消費電力に関する懸念を常に抱えている。QLC方式のNAND型フラッシュメモリは、より多くのデータを少ないチップ数で記録できるため、省スペース化と消費電力の最小化を実現する手段として期待されている。
どれほどの容量が必要か
ベンダーが提唱する大容量SSDとは、具体的にはどの程度の容量なのか。Sandiskが2025年2月に示したロードマップによると、2027年までに128TB、256TB、512TBのSSDが登場し、将来的には1P（ペタ）BのSSDも実現するとの予測がある。同社が同年8月に示したロードマップではさらに情報が追加され、128TBおよび256TBのSSDが2026年前半にも登場することが明らかになった。

　これほどの容量が必要なのかと感じる人もいるだろうが、LLM（大規模言語モデル）の学習用データセットは驚異的な速さで増加している。カリフォルニア大学バークレー校（University of California, Berkeley）の研究者は、論文「AI and Memory Wall」の中で、大規模な「Transformer」モデルのパラメータ数（AIモデルの複雑さを示す指標）が、2018年から2022年にかけて2年間ごとに410倍という驚異的なペースで増加していることを指摘した。Transformerは、文章中の単語間の関係性を効率的に捉えることができるAIモデルの構造で、LLMの基盤技術として広く利用されている。

　この傾向が続けば、大規模Transformerモデルのパラメータ数は2025年までに1京に達することが見込まれる。各パラメータが4バイトを使用すると仮定すると、総データ量は40PBに上る。これは128TBのSSDが300台以上必要になる計算だ。AIモデル学習の途中経過を保存するチェックポイント作成や一時的なデータ保存といったタスクを実行するために、実際にはさらに多くのSSDが必要になる。

　巨大なAIモデルを実行するシステムの数だけ、このようなSSD群が必要になると考えると、用意すべきストレージの規模は計り知れない。一方でSandiskが示すロードマップは、SSDの容量が倍増するペースは1年あるいは2年ごとであり、2年間で410倍というAIモデルのパラメータ数の増加ペースには及ばない。AIシステムの構築担当者は、SSDの台数、費用、電力使用量が指数関数的に膨れ上がるのを避けたければ、AIモデルのサイズを削減する方法を見つける必要がある。
価格はどうなるのか
128TBのSSDは、どのくらいの価格で販売されるのだろうか。HBM（広帯域メモリ）のように、異なる種類の半導体を積層して超小型パッケージに収めるための特殊な製造プロセスではなく、大容量SSDは既存のNAND型フラッシュメモリを垂直方向に積層する技術の延長線上で製造できると考えられている。そのため、小容量SSDの価格を基に単純に掛け合わせて試算すれば、ある程度妥当な数値が得られる。

　128TBのSSDは安価ではないことは容易に想像がつく。しかし、特にハイパースケーラーにとっては、初期費用を上回る価値を持つ可能性がある。HDDをSSDに置き換えることによって、データセンターのラックスペースや消費電力、冷却にかかる費用を大きく削減できる見込みがあるためだ。結果として、システムのTCO（総所有コスト）を削減するコストモデルに適合するため、非常に魅力的な選択肢になる。
物理的なサイズ
Meta Platformsは先述の報告書で、現在主流の2T（テラ）bitのQLC方式NAND型フラッシュメモリチップを32個積層した「32ダイスタック」パッケージを使用すれば、QLC方式の大容量SSDを製造できると説明している。NAND型フラッシュメモリのメーカーはこれまで、数GBの容量をmicroSDメモリカードのサイズに収めるために、同等かそれ以下の積層技術を使用してきたため、これは新しい技術ではない。

　100TBのSSDを実現するには、2TbitのNAND型フラッシュメモリチップが400個以上必要になる。しかし、Meta Platformsが提唱する方式でパッケージ化すれば、パッケージ数はわずか13個に収まる。これによって、「U.2」といった小型のフォームファクターにも搭載可能できるようになる。

　より一般的に普及している「8ダイスタック」（8個積層）を使用する場合は、4倍の約50個のパッケージを搭載できるフォームファクターが必要になる。その場合でも、フォームファクターが標準的な「EDSFF E1.L」（EDSFF：Enterprise and Data Center SSD Form Factor）であれば、十分なスペースを確保できる。

　次回は、データ保管以外に期待されているAI分野での大容量SSDの用途と、その先進技術を紹介する。
TechTargetジャパン",[],[]
