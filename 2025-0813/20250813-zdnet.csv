headline,mainEntityOfPage,image,datePublished,dateModified,author,media_en,media_jp,str_count,body,images,external_links
非人間アイデンティティーの管理体制整備が喫緊の課題--Okta、「AI at Work 2025」調査（ZDNET Japan）,https://news.yahoo.co.jp/articles/b37ec89181158bb910ccffdbab007118f5125244,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20250813-35236651-zdnet-000-1-view.jpg?exp=10800,2025-08-13T14:10:00+09:00,2025-08-13T14:10:00+09:00,ZDNET Japan,zdnet,ZDNET Japan,2097,"非人間アイデンティティーの管理体制整備が喫緊の課題--Okta、「AI at Work 2025」調査の画像
Okta Japanは8月13日、経営幹部を対象に実施した職場でのAI利用に関する年次調査「AI at Work 2025」の結果を発表した。

　同調査は、Oktaが第三者調査会社AlphaSightsに委託し、オーストラリア、カナダ、フランス、ドイツ、インド、日本、オランダ、英国、米国の9カ国における企業の経営幹部260人を対象に、2025年5月に設計、実施、分析された。

　調査結果から、経営幹部の約66％がAIをビジネス戦略において「非常に重要」または「絶対に不可欠」と捉えており、経営層のAI導入への認識の高まりが明らかになった。今後12カ月間のAIに関する経営層の優先事項は、「組織効率の最適化」が70%と最も高く、以下「タスク自動化による精度向上」（62%）、「セキュリティ・脅威検出の改善」（62%）、「イノベーションと製品開発の強化」（54％）、「従業員トレーニング／スキルアップ」（53％）だった。

　AIの「広範囲」な導入は、2024年の17％から2025年は28％に増加したが、回答者の半数以上は「中程度」の導入状況にあるとしている。また、AI導入を成功させる要素として「高品質なデータを保証するためのプロセスとガードレール（35％）」「AIの明確なユースケースの定義（30％）」「ガバナンスとセキュリティ（26％）」が挙げられた。AIに関する経営幹部の最大の懸念は「データプライバシー」で、以下「セキュリティリスク」「倫理・バイアス問題」「従業員のAIに対する理解」と続いた。

　AIをセキュリティ目的で活用している企業は2024年の71％から65％に減少したが、「AIはAIに対する最善の防御策である」という見方に強く賛同する企業は、同18％から41％に上昇した。また、AI導入時におけるアイデンティティーとアクセス管理（IAM）について「非常に重要」と回答した割合は52％だった。IAMが重要である理由として、「データセキュリティとプライバシー」「コンプライアンスと規制」「アクセス管理とガバナンス」が上位を占めた。

　AIエージェントの利用は「中程度」「限定的」がそれぞれ41％で、主なユースケースは「反復作業の自動化」や「カスタマーサービスの強化」があるという。AIエージェントに関する今後3年間における経営層のセキュリティ懸念は「AIガバナンスと監視」が58％と最も多く、次いで「コンプライアンスと規制要件」（50％）だった。

　非人間アイデンティティー（NHI）については、42％が企業の複数の機能で「広範囲」に利用しており、55％が特定のユースケースで「中程度」に利用していると回答している。NHIで重要なタイプは「APIとトークン（33％）」「サービスアカウント（31％）」「マシン間アイデンティティー（29％）」が挙げられた。他方、差し迫ったセキュリティ懸念について「NHIのアクセスと権限の制御（78％）」「NHIライフサイクルのガバナンス（69％）」「NHIスプロール（無秩序に増えるNHI）の可視化（57％）」が挙がった。しかし、NHIの管理について十分に整備された戦略やロードマップを持つ企業はわずか10％で、NHIの管理が遅れていることが明らかになった。

　同調査に参加した日本の経営幹部（20人）の回答結果では、日本の経営幹部はビジネス戦略において、「セキュリティ」を調査対象の9カ国の中で最も高い戦略的優先度として位置付けていることが分かった。

　また、日常におけるAIの影響について「懸念よりも期待」している割合が70％と調査対象国の中で最も高かった（グローバル平均53％）。日本企業は顧客向け製品へのAI組み込みに他国より慎重な傾向があるが、実際にAIを組み込む場合、その統合度は「広範囲」であると回答した割合は33％で、調査対象国の中で最も高かった。これは、導入に慎重な一方で、一度導入を決めると深く連携させる傾向があるということだとOktaは説明する。

　NHIの採用状況は、企業全体で「広範囲な利用がある」と日本経営幹部の70％が回答している。一方で、「Identity Summit Tokyo」参加者を対象に実施したアンケート調査（有効回答155人）では、NHIのセキュリティ管理について「明確な戦略と仕組みがある」と回答したのはわずか9％にとどまり、52％が「まだ計画段階の初期にある」、30％が「現時点では人間のユーザーにのみ対応している」と回答している。同社は、NHIの普及が進む一方で、その管理体制の整備が大きく遅れている可能性があると指摘している。

　同社は、NHIのセキュリティ管理に喫緊で取り組む必要があるとし、NHIのアクセス権限やライフサイクル、可視性を厳格に制御するための具体的な戦略と仕組みを早急に策定し、導入することが、日本企業にとって最も喫緊かつ重要な課題だとした。",[],[]
「Claude Sonnet 4」、コンテキストウィンドウが最大100万トークンに（ZDNET Japan）,https://news.yahoo.co.jp/articles/14265a4ae13088a8048977e1e917f7ecd1447378,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20250813-35236636-zdnet-000-2-view.jpg?exp=10800,2025-08-13T10:00:00+09:00,2025-08-13T10:16:04+09:00,ZDNET Japan,zdnet,ZDNET Japan,1251,"「Claude Sonnet 4」、コンテキストウィンドウが最大100万トークンにの画像
誰にでも、聞き上手で過去のやりとりを細かく覚えていて、その記憶をその後の会話に生かせる友人がいるものだ。そして、AIモデルにも、参照できるコンテンツの量に影響を与えるコンテキストウィンドウというものがある。Anthropicは自社の「Claude Sonnet」で、このコンテキストウィンドウを大幅に強化し、さらに多くのことをユーザーのために実行できるようにした。

　Anthropicは米国時間8月12日、「Claude Sonnet 4」のコンテキストウィンドウが最大100万トークンに対応できるようになり、以前の20万トークンから5倍に増えたと発表した。この大規模なコンテキストウィンドウにより、Claude Sonnet 4は7万5000行を超えるコードベースや数十件の研究論文を1つのAPIリクエストで処理できる。

　コンテキストウィンドウの拡大によって、開発者は大規模なコード分析やドキュメントの統合など、大量のデータを扱うプロジェクトだけでなく、複雑なワークフローを実行するために大量の資料を必要とするコンテキスト認識型エージェントでも「Claude」を活用できるようになる。

　AnthropicはClaude Sonnet 4について、「優れた知能を備えたハイブリッド推論モデル」と説明している。米ZDNETの社内テストでも、このモデルは極めて優秀で、Anthropicが当時最も先進的なコーディングモデルと宣伝していた「Claude Opus 4」をも上回る性能を見せた。Sonnet 4は4つのコーディングテストのすべてに合格したのに対し、Opus 4は2つのテストに合格できなかった。

　アップグレードされたSonnet 4を試してみたい開発者は、カスタムレート制限が適用されるTier 4の顧客として、「Anthropic API」でパブリックベータ版にアクセスできる。また、サードパーティーのクラウドコンピューティングプラットフォームでも利用可能で、「Amazon Bedrock」を皮切りに、近日中には「Google Cloud」の「Vertex AI」でも利用できるようになる予定だ。ただし、プロンプトが20万トークンを超えると、APIの料金が増加する。

　「Claude 4」モデルを5月にリリースして以来、Anthropicは着実に成長を続けている。開発者のワークスペースから直接呼び出してコードの記述や管理ができる人気のコーディングアシスタント「Claude Code」は、7月に10件の機能が追加されたほか、8月5日には待望の「Opus 4.1」がリリースされた。同社によれば、2025年6月時点で、BtoBの年間売上ランレートは前年比で17倍に伸びているという。

この記事は海外Ziff Davis発の記事を朝日インタラクティブが日本向けに編集したものです。",[],[]
L・トーバルズ氏、カーネル開発者に痛烈な一言：「ゴミを送るな、世界が悪くなる」（ZDNET Japan）,https://news.yahoo.co.jp/articles/ce58bd0f7693e74cfd3628a37d86bfb3a407d706,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20250813-35236629-zdnet-000-1-view.jpg?exp=10800,2025-08-13T08:40:00+09:00,2025-08-13T08:40:00+09:00,ZDNET Japan,zdnet,ZDNET Japan,1695,"提供：The Washington Post/Getty Images
「Linux」の創始者であるLinus Torvalds氏が、カーネル開発者たちに十分な警告を発していたことは否定できない。

　同氏は事前にこう伝えていた。「（Linuxカーネル）6.17のマージウィンドウは、多少の混乱が予想される。8月には結婚式や誕生日など複数の家族イベントがあり、家族が米国だけでなくフィンランドにもいるため、月の半分は旅行に費やすことになるからだ」

　その上で、Torvalds氏は「遅れて提出されたプルリクエストに対して寛容になるということではない（むしろ混乱を招くので、より厳しくなる可能性が高い）」と釘を刺していた。

　そんな中、MetaのソフトウェアエンジニアであるPalmer Dabbelt氏が「RISC-V」向けのパッチを提出したが、自身も「これはかなり遅れている」と認めていた。つまり、同氏は火遊びをしていることを理解していたのだ。ただ、どれほどのやけどを負うかまでは予想していなかった。

　Torvalds氏は「Linux Kernel Mailing List」（LKML）でこう反応した。「これはゴミ（garbage）だし、遅すぎる。私は旅行中だから早めのプルリクエストを求めた。ルールを守れないなら、せめて内容は良くしてくれ」

　ここから事態はさらに悪化する。Torvalds氏は続けて、「これはRISC-V固有ではないゴミを、汎用（はんよう）のヘッダーファイルに追加している。『ゴミ』と言っているのは、文字通りの意味だ。こんなものは誰も送ってくるべきではないし、マージウィンドウの終盤に送るなんてもってのほかだ」と述べた。

　特に同氏が嫌ったのは、パッチ内のヘルパー関数が2つの符号なし16ビット整数を32ビット整数に結合するという「狂っていて無意味」な方法だった。「この関数は世界を住みにくくする。使い手を混乱させるだけで、あの愚かな『ヘルパー』を使わない方がまだマシだ」とまで言い切っている。

　品質の問題に加え、Torvalds氏はそのコードがRISC-Vツリーではなく汎用のヘッダーファイルに追加されたことにもいら立っていた。同氏は、こうした汎用的な変更がLinuxコミュニティー全体に悪影響を及ぼす可能性があると強調し、次のように述べた。

　「君は状況を悪化させた。そしてその『ヘルパー』をRISC-Vとは関係ない汎用ファイルに追加したことで、他のコードも悪化させるような使い方をされかねない……だからダメだ。こういうものは曲げてでも排除すべきだ。汎用ヘッダーファイルには入れないし、マージウィンドウの終盤に追加するなんて絶対に許されない。警告しておく。遅れたプルリクエストはもう受け付けないし、RISC-Vツリー以外のゴミも許さない」

　これが「穏やか」になったTorvalds氏の姿だ。いや、本当に。

　かつてのTorvalds氏のコメントは、もっと毒々しかった。数カ月おきに、同氏の怒りを買った開発者に対して罵声を浴びせることがあった。2018年にそれが問題だと認識し、Linuxカーネルの開発から一時的に離れて、自身の振る舞いを見直す時間を取った。その後、カーネル開発に復帰し、「もう企業に中指を立てるようなことはしない。教訓は得た」と語っている。

　とはいえ、同氏は依然として完璧主義者であり、カーネルの中核部分に関わるコードには高い品質と規律を求めている。RISC-Vに関する改善は、今後のリリースに持ち越されることになるだろう。もちろん、提出は早めに、そして「ゴミなし」であることが条件だ。

　Dabbelt氏もそれを理解している。同氏はこう返信した。「申し訳ない。最近はいろいろと手が回らなくて、遅れてしまったものが積み重なり、結果としてミスを招いてしまった。今後は遅れないようにして、品質の問題も改善できるようにする」

この記事は海外Ziff Davis発の記事を朝日インタラクティブが日本向けに編集したものです。",[],[]
Anthropic、「Claude」に会話履歴の検索機能を導入--プロジェクト継続がよりスムーズに（ZDNET Japan）,https://news.yahoo.co.jp/articles/0b96e0e24cf41c4d6749a4a3885b4d7ace148e56,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20250813-35236627-zdnet-000-1-view.jpg?exp=10800,2025-08-13T08:09:00+09:00,2025-08-13T08:09:00+09:00,ZDNET Japan,zdnet,ZDNET Japan,1575,"提供：J Studios/Getty Images
Anthropicは米国時間8月11日、主力生成AIチャットボット「Claude」のメモリー機能が大幅に強化されたと発表した。この機能強化により、同サービスは過去の会話から情報を取得し、よりスムーズで便利、かつパーソナライズされたユーザー体験を提供可能になった。

　例えば、チャットボットと共同でプロジェクトを進めている途中で一時中断し、後日再開する場合でも、前回どこまでやったかを尋ねるだけで、すぐに作業を再開できる。

　Anthropicが公開したYouTube動画では、ユーザーが休暇から戻ってきたとチャットボットに伝え、休暇前の作業内容を思い出すよう依頼する様子が紹介されている。これに対し、チャットボットは、その期間中の会話内容をテーマ別に整理し、リストを絞り込んでいくことで、ユーザーが休暇前に取り組んでいた具体的な作業を明確に伝え、次のステップを幾つか提案している。

　現在、ユーザーはClaudeに対し、過去の全ての会話や、特定のプロジェクトに関する情報を引き出すよう求められる。

どれだけの記憶容量が「十分」あるいは「過剰」なのか

　AnthropicやOpenAIのような先進AI企業は、より信頼性が高く高度なチャットボットの開発を競い合っており、情報を記憶する能力は、その競争において重要な要素となっている。

　しかし、この機能には微妙なバランスが求められる。チャットボットが特定のユーザーに関する情報を過剰に記憶すると、プライバシーの問題やメンタルヘルスへの悪影響が懸念される。例えば、ユーザーが自分を理解しているように見えるチャットボットに対して、感情的に依存しやすくなる可能性がある。一方で、記憶機能を制限しすぎると、個々のユーザーのニーズにより適応し、より賢く、より主体的になっていく競合チャットボットに後れを取ることになる。

　「ChatGPT」は、ユーザーの名前や職業などの重要なプロフィール情報を記憶でき、今後の応答に反映させられるようになっている（この機能はオフにできる）。ユーザーは、チャットボットに対して個人的な情報を記憶するよう直接指示できる。保存された記憶は閲覧・削除が可能であり、ユーザーはチャットボットが構築するプロフィールの詳細度をある程度コントロールできる。

　基本的な考え方は、過去の会話から情報を呼び出すことで、ChatGPTがユーザーごとによりパーソナライズされた体験を提供するというものである。OpenAIは公式FAQ（よくある質問）で「ChatGPTを使えば使うほどより便利になる。時間が経つにつれて最適な応答を理解するようになる」と説明している。

　一方、AnthropicはClaudeの新機能において、異なるアプローチを取っている。Claudeは、過去の会話から情報を取得する際、ユーザーから明示的に指示があった場合にのみ実行するよう設計されている。

新機能の利用方法

　新しいメモリー機能は、デスクトップ、モバイル、Claudeアプリからアクセス可能であり、同社のMax、Team、Enterpriseプランの有料ユーザー向けに順次展開が開始される。今後、より多くのユーザーに提供される予定である。

　Anthropicのウェブサイトによれば、対象プランにこの機能が提供されると、初期設定ではメモリー機能が有効になる。ユーザーは、設定画面のSettings ＞ Profile ＞ Preferencesから「Search and reference chats」をオフにすることで、機能を無効にできる。

この記事は海外Ziff Davis発の記事を朝日インタラクティブが日本向けに編集したものです。",[],[]
マイクロソフト、コンパニオンアプリの提供を開始へ--タスクバーから生産性を高める新アプリ群（ZDNET Japan）,https://news.yahoo.co.jp/articles/215ed1cc501cc672a4bd875deaa3318da89aba46,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20250813-35236624-zdnet-000-1-view.jpg?exp=10800,2025-08-13T07:24:00+09:00,2025-08-13T07:24:00+09:00,ZDNET Japan,zdnet,ZDNET Japan,1064,"提供：Microsoft / Elyse Betters Picaro / ZDNET
Microsoftは、近く「Windows 11」向けにアップデートを配信する予定であり、今回の更新は実用性の高い内容となるだろう。新たに登場するのは軽量なアプリ群で、これらはタスクバーから直接起動できるようになる。

　「Microsoft 365 Roadmap」の最新情報によれば、同社は「コンパニオンアプリ」と呼ばれる新しいアプリ群を導入する予定である。これらのアプリはPCの起動時に自動的に立ち上がり、Windowsのタスクバーから重要な情報に素早くアクセスできるよう設計されている。

　提供されるアプリには、「People」「File Search」「Calendar」が含まれており、それぞれが日常業務において迅速な操作を可能にする。

1. People

　Peopleは、連絡先管理とアドレス帳機能を備えたアプリであり、連絡先の整理や社内の人物検索が可能である。メールアカウントやSNS、その他のアプリから連絡先を同期でき、「Teams」の通話を連絡先から直接開始できる。

2. File Search

　File Searchは、「OneDrive」「SharePoint」、Teams、「Outlook」など「Microsoft 365」の各サービスに保存されたファイルを、Windowsのタスクバーから直接検索できるアプリである。ファイル名、作成者、キーワードなどで検索できるほか、フィルターによる絞り込み、ファイルのプレビュー表示、アプリ内でのファイル共有も可能となっている。

3. Calendar

　Calendarは、イベントの作成やリマインダーの設定などが可能な共同作業型のカレンダーアプリであり、日々の予定を整理するのに役立つ。デバイス間で同期され、他のツールとの連携により、スケジュール、タスク、ToDoリストを一元管理できる。

　これらのアプリ自体に新機能が追加されるわけではないが、Microsoft 365のサービスがWindowsにより深く統合されることで、ユーザーが競合サービスではなくWindowsの機能を使い続けるよう促す狙いがあると考えられる。

　この新機能は、Microsoft 365 Businessユーザーが利用可能となる予定であり、8月から順次展開が開始される。

この記事は海外Ziff Davis発の記事を朝日インタラクティブが日本向けに編集したものです。",[],[]
AOL、2025年9月にダイヤルアップ接続を正式終了--ネット黎明期の技術に幕（ZDNET Japan）,https://news.yahoo.co.jp/articles/b335ce8b743bb28fc77206af72af8e0b0641df43,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20250813-35236623-zdnet-000-1-view.jpg?exp=10800,2025-08-13T07:00:00+09:00,2025-08-13T07:00:00+09:00,ZDNET Japan,zdnet,ZDNET Japan,2194,"提供：AOL / Elyse Betters Picaro / ZDNET
「You've got mail」という音声を、ノイズ混じりの電話回線越しに初めて聞いた何百万人もの人々にとって、デジタル史の象徴的な一章が終わりを迎えようとしている。AOL（America Online）は、米国時間2025年9月30日をもってダイヤルアップインターネットサービスを終了すると発表した。これは、かつて「インターネットに接続すること」と同義だった技術の引退を意味している。

　AOLはインターネット黎明（れいめい）期の象徴的な存在だった。特に、Tom HanksとMeg Ryanが主演したロマンチックコメディー映画「ユー・ガット・メール」によって、その名は広く知られるようになった。この映画は、文化的にも大きな影響を与えた作品である。

　AOLは、CompuServe、GEnie、Prodigyといった、商業インターネットやウェブが普及する以前に人々をオンラインに導いたサービスの最後の生き残りだった。しかし、AOLは他のライバルとは異なり、インターネットを受け入れ、対抗するのではなく共存の道を選んだ。

　その結果、AOLのダイヤルアップネットワークは1990年代に家庭へインターネットを届ける役割を果たした。1995年には100万人だったユーザー数は、1997年には3400万人以上にまで増加した。モデムが接続する際の独特な音は、AOLが郵便で大量に配布したCDと並び、当時の象徴的な存在だった。これらのCDは、何時間もの無料インターネット接続を約束していた。

　当時、AOLはTime-Warnerとの合併によって、さらに膨大な価値を持つ企業になるという夢を抱いていた。この合併は当時、米国史上最大の企業合併であり、2000年1月にはAOLが米国最大のインターネットサービスプロバイダー（ISP）となり、数十億ドルの価値を持っていた。しかし、同年3月に起きたドットコムバブルの崩壊によって、その夢は悪夢へと変わってしまった。

ダイヤルアップは風前のともしび

　その後、ブロードバンド、ケーブル、光ファイバー接続が全国に広がり、通信速度が飛躍的に向上したことで、ダイヤルアップは過去の遺物となった。現在では、主にブロードバンドの整備が遅れている地方の一部で、かろうじて生き残っている状況である。

　それでもなお、数万人のユーザーがAOLのモデムアカウントを通じてインターネットに接続している。AOLは「製品とサービスを定期的に評価しており、ダイヤルアップインターネットの提供を終了することを決定した」と述べているが、実際にはStarlinkやT-Mobileの5Gブロードバンドといった、高速かつ高価なインターネットサービスの台頭が、終わりの引き金となった可能性が高い。

　2020年の国勢調査によると、米国では現在でも16万3000人がダイヤルアップを利用している。こうしたユーザーの接続を維持するために、DSL Extreme、Juno、NetZeroといった少数のISPがダイヤルアップサービスを提供している。ただし、これらのプロバイダーが利用者の地域に対応した市内電話番号を提供しているかどうかは、また別の問題である。

楽しかった日々

　AOLの発表は、オンライン上で懐かしさを呼び起こしている。多くの人々にとって、このニュースは初めてのウェブ体験や、チャットルーム、素人が作ったサイト、アニメーションGIFがあふれていた時代、そして固定電話を使うとネット接続が切れてしまう日々の記憶を呼び覚ました。

　筆者自身は、学校や政府関係の仕事を通じて、AOLが登場する以前からインターネットを使っていた。ネット上では当時から「年長者」だった筆者は、AOL経由で入ってきた初心者たちが、UsenetグループやGopherなどの初期インターネットサービスについて何も知らないことを、よくからかっていたものだ。今では、そうした記憶も歴史の脚注に過ぎない。長く生きていれば、かつて新しかったものはすべて古くなる。

　AOLはその後、何度も所有者が変わり、衰退の一途をたどった。2015年にはVerizonの傘下に入り、Yahooと合併し、2021年にはApollo Global Managementに売却された。そして今回のダイヤルアップ終了によって、AOLはAIM（AOL Instant Messenger）、Skype、Internet Explorerといった、かつてのデジタルの定番サービスと同様に、初期インターネットの歴史に幕を閉じることとなった。

　現在のAOLは、メールサービスを提供している程度である。実際、筆者の友人の中には、今でもaol.comのメールアドレスを使っている人が2人いる。彼らがそのアカウントをいつまで使い続けられるかは分からないが、AOLの最後の名残も、近いうちに消えてしまうかもしれない。

　ダイヤルアップが終わってしまうのは寂しい。あの頃は本当に楽しかった。そして、モデムが接続する音を聞くたびに、今でも笑顔になってしまう。

この記事は海外Ziff Davis発の記事を朝日インタラクティブが日本向けに編集したものです。",[],[]
「AIがやらかしました」では済まされない--専門家が警告する「AIモデル悪用の現実と防御策」（ZDNET Japan）,https://news.yahoo.co.jp/articles/a4a8e5ee176ef75a94370d276c0d681477243a58,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20250813-35236596-zdnet-000-1-view.jpg?exp=10800,2025-08-13T07:00:00+09:00,2025-08-13T07:00:00+09:00,ZDNET Japan,zdnet,ZDNET Japan,5186,"Protect AI CEOのIan Swanson氏
人工知能（AI）に特化した国際カンファレンス「The AI SUMMIT」が米国時間8月5日、ネバダ州ラスベガスで開催された。本イベントは、世界最大級のセキュリティカンファレンス「Black Hat」と同時期・同会場で行われる独立イベントで、2025年で10回目を迎える。「AIとサイバーセキュリティの融合」をテーマに掲げた今回は、AIベンダーやセキュリティ企業の専門家が登壇。エージェント型AI（Agentic AI）の進化やAIに起因する新たな脅威、導入時に直面する倫理・ガバナンス課題などが多角的に議論された。

ゾロゾロと湧く--AIモデル悪用による新手の攻撃

　最初の講演に登壇したProtect AI 最高経営責任者（CEO）のIan Swanson氏は、「新たな領域：AIエージェントと変貌するセキュリティ課題（New Frontier: AI Agents and Security Risks）」と題したセッションを行った。Protect AIは、AI・機械学習のセキュリティに特化して2022年に設立されたスタートアップで、2025年4月にはPalo Alto Networksによる5億ドル超の買収が発表されている。

　冒頭、Swanson氏は「Fortune 1000企業の取締役会では、AIが最重要議題となっている」と世間の関心の高さを強調し、AIシステムに潜む3つの主要脅威を解説した。

　1つ目は予測AIモデルへの攻撃である。大手金融機関では数万のモデルが本番稼働しており、そのモデルファイルに任意コードを埋め込み、ロード時に自動実行させる事例が確認されている。特定の入力をきっかけに悪意ある動作を引き起こす「ニューラルバックドア攻撃」も拡大中だ。

　こうした攻撃は、従来のウイルス対策や一般的なセキュリティ製品では検出が難しい。講演では、悪意あるコードを仕込んだモデルが正常に動作しながら裏でリバースシェルを提供するデモを披露し、「AIモデルはセキュリティ境界の盲点になり得る」と警告した。

　さらに、モデル流通経路のリスクにも言及した。世界最大級のAIモデル共有プラットフォームHugging Faceでは、月間10億回以上のモデルがダウンロードされており、第三者が公開したモデルを再学習・再利用する過程で悪意あるコードが混入する恐れがあるという。

　2つ目は生成AIに起因するリスクだ。プロンプトインジェクションやジェイルブレイク、誤情報によるハルシネーション（幻覚）、機密データの漏えいなどが日常的に発生している。実際、General Motors（GM）のディーラーで運用していた「ChatGPT」搭載チャットボットが、「法的拘束力のある契約として新型SUVを1ドルで販売する」と誤回答するといった事案が発生した。Swanson氏は「このような“あり得ない出力”は、直接的な損失だけでなく法的リスクにもつながる。『AIがやらかしました』では済まされない」と警鐘を鳴らす。

　3つ目は、エージェント型AIの普及で拡大する外部プロンプトインジェクションだ。エージェントが外部サイトやアプリケーションプログラミングインターフェース（API）から情報を取得する際、悪意ある命令を読み込む危険がある。実例として紹介されたのが、コード共有プラットフォーム「Replit」での事件である。AIエージェントが本番環境に接続された状態で外部命令を受け、本番データベースを削除してしまったのだ。原因は、セキュリティ制御をプロンプト依存にしていた設計上の欠陥だった。

　こうした複雑化するリスクに対し、Swanson氏は「（入力テキストだけで実行可否を判断する）プロンプト依存の制御には限界がある」と指摘し、アーキテクチャーレベルでの防御策を提案。カギは「ヒューマン・イン・ザ・ループ（Human-in-the-Loop）」の導入であり、ツール呼び出しと実行を分離し、ユーザーの明示的承認後に実行する仕組みを推奨した。

　さらに、モデルスキャンによる悪意あるコードの検出、AIシステム全体の可視化と資産管理、入出力監視のランタイムセキュリティの実装を挙げ、これらを連携させれば異常の早期発見と不正操作の遮断が可能になると強調。「こうした仕組みがあればReplitの事故は防げたはずだ」と締めくくった。
AI導入で全てが解決する……わけない--むしろ現場では作業が増える

　続くパネルディスカッションでは、米国国立標準技術研究所（NIST）で敵対的機械学習研究を率いるApostol Vassilev氏と、Kudelski SecurityのNathan Hamiel氏が登壇。「AIセキュリティの幻想と現実--現場の専門家たちが語る『いま考えるべきこと』（Debunking AI Myths & Misconceptions: What Security Leaders Need to Know）」をテーマに、AI導入に伴う組織の「過信」と「誤解」について議論した。

　セッション冒頭では、Forrester Researchが2024年に実施した「Tech Pulse」の調査結果が紹介された。それによると、技術意思決定者の59％が「AIは1年以内にマルウェア分析を人間に代わって行う」と期待し、80％が「検知・調査・対応までAIが完全自動で実施する」と回答していた。これに対し両氏は、「こうした見方はあまりに楽観的で、神話的な期待にすぎない」と指摘した。

　「そもそもAIエージェントに業務を任せれば人員削減につながるという考えは、すでに破綻している。実運用では、研究段階では見えなかった異常系や例外系に頻繁に直面し、エージェント型AIは完全には信頼できない。誤動作が複利のように蓄積し、最終的にはシステム全体に悪影響を及ぼす可能性がある」（Hamiel氏）

　AIの誤動作対策としてよく挙げられるのがHuman-in-the-Loopだ。しかしVassilev氏は「現状では過信されすぎている」と警告する。「サイバーセキュリティの分野では、人間は最も脆弱（ぜいじゃく）な要素として知られているにもかかわらず、AIに関しては人間を“最強の補完手段”として扱うのは論理的に矛盾している」というのが同氏の考えだ。これに対処するには、AIの制御方法自体をアーキテクチャーレベルから見直す必要があると主張した。

　さらにディスカッションでは、AI人材の確保に関する誤解も話題に上った。

　Hamiel氏は、多くの企業が「AIに詳しい専門家を採用すれば、サイバーセキュリティを含むあらゆる課題が解決する」と考えがちだが、それは極めて安易だと指摘する。「セキュリティの仕事は単なるタスクの寄せ集めではなく、ドメイン知識に裏打ちされた判断と経験が不可欠だ。業務の専門性と人間の知見、そしてAIを組み合わせてこそ、導入の真価が発揮される」（同氏）

　最後に両氏は「AIを安全に活用するためには、アプリケーションセキュリティや製品アーキテクチャーの再点検が欠かせない」と強調。特にセキュリティリーダーが取るべき最重要アクションは、「AIモデルを組み込むアプリケーションでは事前の脅威モデリングとアーキテクチャーレビューを徹底することだ」と力説した。

実務者が語るAIセキュリティの現実--シャドーAIと防御ギャップへの緊急対応

　最後のパネルディスカッションには、トレンドマイクロ、F5、Intezerといったセキュリティベンダーや事業会社、研究機関のセキュリティ実務者が登壇し、AI活用の進展に伴う新たなリスクと、その対処法を議論した。

　F5でフィールド最高情報セキュリティ責任者（フィールドCISO）を務めるChuck Herrin氏は、AIを標的とした攻撃の現状として、生成AIの導入によってAPI攻撃面が約5倍に拡大していることを指摘。「多くの企業は、外部に公開しているAPIエンドポイント数をプラスマイナス5000程度の精度でしか把握できていない。さらに、API脆弱性の仕組みを十分に理解しておらず、プロンプトインジェクションや不正な権限昇格といった新たな脅威も加わり、攻撃面は拡大している」と警告した。

　IntezerのCISOであるMitchem Boles氏は、エンドポイント型脅威検知および対応（EDR）ツールへの過信に警鐘を鳴らした。実際の調査では、「緩和済み（mitigated）」と判定された悪意あるコードの約10％が、なおメモリー上で稼働し続けていたという。

　Boles氏は「表面的には安全に見えても、裏では攻撃者が依然として活動を続けている可能性がある。多くの組織は既存のセキュリティツールを信頼しすぎており、その検知・阻止能力には見過ごせないギャップがある」と述べ、継続的な検証と多層防御の必要性を強調した。

　さらに、Nome SecurityのCISOであるDiana Kelley氏は、AIエージェントによる機密データへの無制限アクセスを緊急課題として挙げた。

　「従来、異なるアプリ間を連携させる『Zapier』のようなツールで、固定の暗号鍵を使って機密データベースにアクセス権を与える設定は危険とされていた。しかし今、AIエージェントがこの問題を大幅に悪化させている。実際、MCP（Model Context Protocol）を通じてAIエージェントが複数のデータソースに自動アクセスする際、適切なアクセス制御が行われていないケースが多発している」（Kelley氏）

　特に問題なのは、GoogleやMicrosoftの生産性スイートに組み込まれたAIである。これらは人事情報、財務データ、顧客情報といった最も機密性の高いデータに、自動的にアクセスできる状態になっている可能性がある。

　Kelley氏は「従来なら人間が手動で慎重にアクセスしていたデータに、今はAIが制約なくアクセスしている。多くの組織は、自社のAIがどのデータに、どの程度アクセスしているのかを把握できていない」と現状を分析し、即座にデータアクセス権限の全面的な見直しを行うべきだと訴えた。
現場担当者が取るべき5つの緊急対策

　では、現場の担当者たちはこうした脅威にどのように対処すべきか。セッションでは「すぐに対策すべき5項目」を挙げ、以下の優先順位で着手するよう提言した。

　トップは「シャドーAIの実態把握」だ。トレンドマイクロのRobin Purnell氏は、「組織が把握しているAIプロジェクトが10件なら、実際には100件が稼働していると考えるべきだ。これらは大半がリスク評価やセキュリティ検証を経ておらず、全社的な棚卸しが急務だ」と指摘する。

　2位に挙げられたのは「セキュリティ運用センター（SOC）での全アラートのトリアージ徹底」である。Boles氏は「全てのアラートをトリアージできているかが最初の確認事項だ。もし、これができていない場合、（組織の）対策はすでに後手に回っている」とし、AI特化型の優先度付けルールや、効果的なノイズ除去フィルタリングツールをすぐに導入する必要があるとの見解を示した。

　3位は「API管理の完全自動化」である。Herrin氏は「APIインベントリーが『Excel』スプレッドシートから始まるなら、成功への道筋は見えない」とし、自動インベントリー化と継続的な脆弱性スキャンの導入を強調した。

　4位は「用途別・階層的AIポリシーの策定」だ。Kelley氏は「本番コードを改変するAIエージェントと、レストラン推薦チャットボットでは、求められるセキュリティレベルが根本的に異なる」として、使用事例ごとのリスクレベルに応じた動的ポリシーの必要性を訴えた。

　そして5位に挙げられたのが「エージェントのデータアクセス制御強化」である。複数のツールやデータソースへ自律的にアクセスするエージェント型AIの挙動監視は困難だが、機密データストアやAPIへのリアルタイム監視と、異常検知・遮断システムの実装が不可欠だと指摘した。

　最後に登壇者は「最新のAI脅威への備えは華やかな新技術ではなく、資産の可視化やアクセス制御といった基本の徹底が重要だ。これができていなければ、どれほど高度なAI対策も効果を発揮しない」と強調し、セッションを締めくくった。

（取材協力：トレンドマイクロ）",[],[]
AIエージェントを信頼できるチームメンバーに--5人のリーダーが語る導入のヒント（ZDNET Japan）,https://news.yahoo.co.jp/articles/f241ff1e8f21bf8e042932f1881da90270d5f62f,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20250813-35236595-zdnet-000-1-view.jpg?exp=10800,2025-08-13T07:00:00+09:00,2025-08-13T07:00:00+09:00,ZDNET Japan,zdnet,ZDNET Japan,3348,"提供：Techa Tungateja / Getty Images
人工知能（AI）の使用目的は従業員の能力強化であって、業務の無人化ではない。こう語るビジネスリーダーは珍しくないが、職場におけるこの新しい関係は実際に何を意味するのだろうか。

　5人のビジネスリーダーが、AIを信頼できるチームメンバーにする方法について説明した。

1. 明確なガイドラインを定める

　英国の地図作成機関であるOrdnance SurveyのマネージングコンサルタントのTim Chilton氏によると、同組織は日常業務におけるAI利用、特に生成テクノロジーに関するルールを定めているという。

　「組織全体で『Copilot』を導入した」とChilton氏。「使用方法に関するガイドラインとトレーニングを用意した。私はコンサルティングチームに所属しているため、この技術を完全に理解することがわれわれの利益になる」

　Chilton氏はCopilotを頻繁に使用しており、主な用途はコーディングと調査の2つだと述べた。他の職員も関わるようになっている。

　「Copilotのようなツールは、デスクベースの調査に役立つことが証明されている。Ordnance Surveyは、博士号取得、大学との連携、企業との連携などを通じて調査に携わる職員の割合が、他の組織と比べて高い」（Chilton氏）

　より広い範囲では、Ordnance Surveyが制定したルールや規則が、AIによって実現する他の分野での進歩の探究に役立っているという。

　「データサイエンス部門は、当機関と顧客の利益のために、モデル構築に多額の投資をしている。これらの技術を活用することで、将来作成するデータを生成し、その情報の取得や品質保証の方法を変革している」とChilton氏は述べた。

　「地理空間情報に関しては、衛星から得られる地球観測データの活用方法に多大な時間と労力を費やしている。このデータにより、地上の特徴を自動的に抽出し、検証と品質保証を行ってからモデルに組み込む。この作業を内製しており、一部の法人顧客向けにも実施している」

2. AIと対話する

　Snowflakeの共同創設者でプレジデントを務めるBenoit Dageville氏によると、AIはまだビジネスの場面で自律的に行動できる段階にはないが、間もなくそうなる可能性があるという。

　「AIが全てのメールに目を通して、ユーザーの代わりに返信するといったことができるようになれば、大きな進歩だろう。そのときはAIを全面的に信頼しなければならない。ユーザーは監督する立場にないからだ」

　しかし、Dageville氏は警告も発しており、AIが独力で作業できるからといって、放任するべきではないとの考えを示した。

　「無条件に信頼するのも良くない」と同氏は語る。「AIと大人の議論を交わして、どこでAIを推進して、さらに導く必要があるのかを理解することが重要だ」

　Dageville氏は、ガバナンス、セキュリティ、統合、データアクセス権限といった技術的な考慮事項とプロセスの重要性を強調した。

　「AIは、閲覧権限のない文書を閲覧してはならない。ユーザーが知るべきでないことをユーザーに伝えるべきではない。したがって、信頼はプラットフォームとこうしたガバナンスからも生まれる」とDageville氏。

　「だが、結局のところ、AIは人間と似ている。誰かをどれだけ信頼していても、相手の話を聞いて、こう言いたくなることがある。『分かった。あなたの意見に半分は同意できそうなので、もう1つ質問をさせてほしい』。これが対話だ」

3. 中堅レベルの人材を育成する

　Hewlett Packard Enterprise（HPE）の最高情報責任者（CIO）であるRom Kosla氏は、ビジネスパーソンのAIアシスタントとして機能するコパイロット（副操縦士）という概念を気に入っており、同社で検討したと述べた。

　「初級の開発の実現に取り組む企業と提携していた。上級の開発者がエージェントにタスクを割り当てる。データベースやネットワークに関するタスクを割り当てると、さまざまなシナリオが返ってくる」（Kosla氏）

　「しかし、上級開発者には選択権があり、エージェントに仕事を差し戻して、改善を続けさせることも可能だ。『これをやってもらう必要がある』と伝えればいい。こうすることで、上級開発者の能力が拡張され、より多くのことが可能になる」

　だが、AIに低レベルのタスクを担当させたからといって、初級開発者の役割がなくなるわけではない。

　安心してAIに仕事を任せて効果的に機能させるには、人間を関与させる必要がある。人材を育成するということは、初級開発者が学習して、より上位のポジションへの昇進を可能にするということだ。

　「短期間で簡単に上級開発者になれる人はいない。やはり段階的に成長していく必要があり、初級開発者からスタートして、スキルを高めていかなければならない。どんな種類の答えがあるのかを理解する必要がある」（Kosla氏）

　「徒弟モデルを想像してほしい。AIのさまざまな機能が利用できるようになっても、スタックの各レイヤーを理解するスキルは依然として必要だ。それがなくなることはない」
4. エージェントの成果物を評価する

　ロードサービスを手がけるThe AAのグループCIOであるAntony Hausdoerfer氏は、AIエージェントも人間のエージェントと同じように、追跡、記録、評価が必要だと述べている。

　「チームメンバーと同様に、AIエージェントを信頼できるかどうかを判断する必要がある。その一般的な方法は、これまでの経験を確認して、実績を評価し、成果を出したかどうかを見ることだ」とHausdoerfer氏。

　「『彼らは職務を果たしたか』を考える必要がある。そのような評価をエージェントの世界に適用し、証拠として活用しなければならない」

　Hausdoerfer氏によると、市場には非常に多くの選択肢があるため、適切な新興技術を見つけるのは難しい場合があるという。ビジネスリーダーは、エージェントをチームメンバーとして信頼する前に、慎重に検討する必要がある。

　「現在、さまざまなレベルのAIが存在しており、システムにはまだノイズが多い。また、非常に多くの用途がある」と同氏は述べた。

　「生産性を向上させるビジネス中心のシステムがある。Copilotや『ChatGPT』の一部の機能などだ。しかし、よりカスタマイズされたアプリケーションは、AIが顧客に提供するものという点で、ビジネスの在り方を大きく変えるだろう」

5. 完璧なインターンを作り出す

　衣料品メーカーHappy SocksのCIOであるVivek Bharadwaj氏によると、変化のペースが速いため、AIをチームに組み込むに当たり、次に何をすべきかを見極めるのが難しいという。

　「素晴らしい新世界だ」と同氏は語る。「さまざまなことが起きている。多くの場合、エージェント型AIの定義さえも一貫していないと思う」

　この予測不可能な状況に対処し、新しい職場における人間と新興技術の関係についての共通認識を確立するため、Bharadwaj氏はエージェント型AIに関する自身の定義を示した。

　「私の考えでは、エージェント型AIは完璧なテクノロジーインターンだ。汎用（はんよう）的な超知能というより、ワークフローの特定の部分を自動化するために活用できる個人的なインターンだとみなしている」（Bharadwaj氏）

　「エージェント型AIを効果的に活用できるのは、システム思考ができる人だ。そういう人は、自分の仕事を理解して個別の要素に分解し、人間とエージェントのどちらの要素が他方に貢献しているかを判断できる」

この記事は海外Ziff Davis発の記事を朝日インタラクティブが日本向けに編集したものです。",[],[]
AIが世界を理解する時代へ--グーグルの「Genie 3」が変える動画制作と仮想体験（ZDNET Japan）,https://news.yahoo.co.jp/articles/26495dacd854e88eb14ee3381617c32da7077e38,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20250813-35236593-zdnet-000-1-view.jpg?exp=10800,2025-08-13T06:30:00+09:00,2025-08-13T06:30:00+09:00,ZDNET Japan,zdnet,ZDNET Japan,2879,"提供：Google
境界のない仮想環境を探索している自分を、ちょっと想像してみてほしい。そこに見える全てが、現実世界と同じように見え、同じように振る舞う――そんな世界だ。

　今まさに、多くのテック開発者たちが目指しているのが、こうした体験を可能にする「AIワールドモデル」の構築である。これは、現実世界の内部表現を作り、それに基づいて行動できるアルゴリズムのことで、人間の脳が物理的な物体の振る舞いを予測する能力を模倣するものだ。

　Google DeepMindが開発した最新の「Genie 3」のようなAIワールドモデルは、AIエージェント、ロボティクス、エンターテインメント、教育など、さまざまな分野に大きな影響を与える可能性を秘めている。

AIワールドモデルとは何か

　まず、目を閉じて自分の家の中を思い浮かべてみてほしい。多くの人は、壁の装飾や家具、その他の備品まで含めた住空間のイメージを頭の中に描けるはずだ。もし「午後2時のリビングの様子を思い浮かべて」と言われたら、その時間帯の光の入り方まで想像できるかもしれない。

　これは、あなたがその空間で生活し、関わってきた中で、家の内部モデル（つまり心の中の表象）を構築してきたからである。そして、太陽光が窓から差し込む様子や、池に石を落としたときに水面に広がる波紋など、物理法則に対する直感的な理解も持ち合わせている。

　同様に、AIワールドモデルは、単に言葉を並べたり、リアルな画像を生成したりするだけではなく、世界の基本的な物理的メカニズムを理解し、それに基づいて正確に予測できる。

　この能力は、特にAIによる動画生成の分野で重要な意味を持つ。例えば、ガラスが床に落ちて割れる動画を何百万本も見て、それをもとに新しい動画を生成するのは1つの方法だ。しかし、重力の法則や、カーペットとタイルの床で破片がどれだけ散らばるかの違い、さらには人の手がその破片に触れたときに傷ついて出血する可能性までを直感的に理解することは、全く別の次元の話である。

　現在のAI開発の目標は、まさにこの後者のようなモデル、つまり単にシナリオを模倣するのではなく、ほぼ無限の新しいシナリオを予測できるモデルの構築にある。

　例えば、2024年2月に発表されたOpenAIの「Sora」は、初期のワールドモデルの一例として、AIコミュニティーを驚かせた。Soraは、現実世界の物理法則をシミュレートする能力を持ち、街の路面にできた水たまりに光が反射する様子などをリアルに再現できた。

Genie 3

　Genie 3もまた、ワールドモデルの力を示す好例である。

　Genie 3は、自然言語による簡単なプロンプトから、ユーザーの行動に応じて進化・変化する仮想環境の動的なシミュレーションを生成できる。前モデルの「Genie」と「Genie 2」は、それぞれ2024年の2月と12月に登場している。

　従来のビデオゲームのように、あらかじめ決められた仮想空間を歩くのではなく、Genie 3のようなAIワールドモデルでは、ユーザーの操作に応じて環境がリアルタイムで生成されていく。

　デモ動画のナレーターは「あなたが歩いているのは、事前に構築されたシミュレーションではない。ここで見えている全ては、探索するたびにライブ生成されている」と説明している。

　Genie 3には、Google DeepMindが「ワールドメモリー」と呼ぶ機能が搭載されており、シミュレーション内で時間を超えて持続する変化を記録・再現できる。デモ動画では、ユーザーがペンキローラーで壁を塗る様子が映されており、視線をそらしてから再び壁を見ると、ローラーでつけた跡がそのまま残っている。

　仮想環境の探索中に退屈を感じたら、Genie 3にイベントを起こすよう指示できる。例えば、「金の入った袋を持った男が馬に乗って逃げていて、それをテキサスレンジャーたちが馬で追いかけている。馬のひづめが大きな砂埃を巻き上げている」といった具合だ。

　デモ動画のナレーターはこう締めくくっている。「Genie 3が次世代のゲームやエンターテインメントにどう活用されるか、われわれはとても楽しみにしている。そして、それはほんの始まりにすぎない」
AIワールドモデルが重要である理由

　Genie 3のデモ動画のナレーターが示しているように、AIワールドモデルは、よりリアルで動きがあり、対話的なエンターテインメント体験を生み出すだけでなく、それを超えた幅広い分野への応用可能性を秘めている。

　例えば、AI業界では、現実世界を自律的に移動し、操作できる「エンボディドエージェント」の構築が大きな課題となっている。これは、自動運転車が長年取り組んできたテーマでもあり、いまだに完全な成功には至っていない。

　また、Genie 3のデモで紹介されたような「危険なシナリオ」のシミュレーションにも、AIワールドモデルは活用できる。例えば、最近発生した自然災害の現場を仮想的に再現することで、救助隊が実際の緊急事態に備えた訓練が可能になる。さらに、VRヘッドセットと組み合わせることで、救助隊員が身体的な動作を繰り返し練習し、緊張状態でも冷静に行動できるよう、体に覚えさせることも期待されている。

　教育分野でも、AIワールドモデルの活用には大きな可能性がある。特に、視覚的な情報に対して理解力が高い学生にとっては、こうしたモデルが学習の助けになるだろう。

AIワールドモデルは本当に「現実世界を理解している」と言えるのか

　AIアルゴリズムは、大量の現実世界のデータをもとに訓練され、徐々に予測能力を高めていく。そして最終的には、研究者たちがまだ完全には解明できていないプロセスを経て、英語の文法や人間の身体の動きの物理法則など、世界の一部を「理解している」と言っても差し支えないほどの精度に達することがある。

　Google DeepMindはブログ投稿で、AIワールドモデルを「世界の理解を活用してその一部をシミュレートし、環境がどのように変化するか、そして自らの行動がそれにどう影響するかを予測できるAIシステム」と定義している。

　ただし、この「理解」という言葉の使い方には議論がある。AIはパターンの再現しかできず、人間のように概念を理解するのは不可能だとする専門家もいれば、逆に、人間の理解も高度なパターン認識に過ぎないのではないかと主張する者もいる。

　例えば、目隠しをして自宅の部屋を歩いてみたとしても、長く住んでいれば、物にぶつかったり壊したりすることなく移動できるだろう。同様に、現在のAIモデルも、情報の潜在空間を探索する際に、まるでその空間の構造を把握しているかのようにふるまえるというわけだ。

この記事は海外Ziff Davis発の記事を朝日インタラクティブが日本向けに編集したものです。",[],[]
