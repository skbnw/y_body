headline,mainEntityOfPage,image,datePublished,dateModified,author,media_en,media_jp,str_count,body,images,external_links
「AIは怖い」を払拭　信頼できるAIを構築するためのポイントをおさらい（TechTargetジャパン）,https://news.yahoo.co.jp/articles/b111a9877bd87c1e8307e8c43fe516addbe2aac7,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20250722-00000002-zdn_tt-000-1-view.jpg?exp=10800,2025-07-22T08:00:13+09:00,2025-07-22T08:00:13+09:00,TechTargetジャパン,zdn_tt,TechTargetジャパン,2940,"（写真：TechTargetジャパン）
人間の知能を再現した真の人工知能（AI）である「汎用（はんよう）人工知能」（AGI）が、人間の脅威となることを危惧する声がある。2023年3月には、高度なAIシステムの開発や運用の一時停止を求める署名活動が展開された。一方、その活動の先に何があるのか疑問を呈する専門家がいる。AIに対するエンドユーザーの不信感を払拭し、信頼できるAIを構築するため、企業は何をすればよいのか。取り組むべき内容を専門家の意見を基に紹介する。
信頼できるAI作り　検討すべきポイントとは
モントリオールAI倫理研究所の創設者、故アビシェック・グプタ氏は、一時停止の呼び掛けを「効果がない」と指摘したAIの専門家の一人だ。2023年のインタビューで同氏は、「具体的な行動や必要な裏付けのない、建前としての正義を示すだけの書簡には署名しにくい」と述べた。そのような書簡は、「実質的な変化をもたらすことなく注目を集めるだけで、逆効果になることが多い」とも指摘した。

　「メディアがあおる終末論的な物語は、議論を混乱させ、適切な政策決定に必要な冷静な対話の機会を妨害する可能性がある」。グプタ氏と同様の考えを持つ専門家はこのように説明する。「AIの使用と脅威について検討するのであれば、漠然としたリスクではなく、実際の課題について学ぶこと。そのためには、AIシステムの開発経験や理論的な知見を持つ専門家の協力が必要だ」（同氏）

　民泊サービス「Airbnb」のアソシエイトジェネラルカウンセル（副法務責任者）で、Appleのセキュリティカウンセル（セキュリティ法務担当）を務めたエリオット・ベハー氏は、「『責任あるAI』（公平性や透明性、安全性の確保を考慮したAI技術）を推進するため何に取り組むべきか、合意を形成することはそれほど難しくないはずだ」と指摘する。
AIの脅威を整理する
AIに対する悲観的な見方が広がる中、信頼できるAIの構築に企業はどのように取り組めばいいのか。

　サンタクララ大学（Santa Clara University）の技術倫理ディレクターであるブライアン・グリーン氏は、AIの脅威を、AIそのものから生じる脅威と、AIを使うことで生じるリスクに分けて考えることが有効だと述べる。

　グリーン氏によると、AIそのものから生じる脅威は、計算の過程で生じたエラーや、AIが独自の意思を持って人間を攻撃するといったものまで多岐にわたる。ただし、後者が起こり得る明確な道筋は見当たらないと同氏は強調する。

　人間がAIを使うことで生じる脅威として、人間が自動化可能だと考えるあらゆる行為がAIによって悪用される可能性が挙げられる。例えば、AIが核兵器を管理し、いつでも発射できるようにする、誤情報を大量に流す、既存の兵器よりも致命的な生物兵器を開発する、より効果的に社会を統制するといった具合だ。

　「人間の知能でできるあらゆる恐ろしいことはAIが学習でき、人間と同等かそれ以上の能力で実行できる可能性がある」とグリーン氏は述べる。「AIが自発的にこれらの行動を実現するよりも、人間がリスクを伴う目的のためにAIを利用する可能性の方が圧倒的に高い」（グリーン氏）

　グリーン氏は、「私たちが直面している現実的なAIのリスクは身近に存在している」と話す。消費者の注意を引くように訓練された生成AIのコンテンツが、重要な問題から目をそらさせたり、AIを活用したモバイルアプリが消費者に製品やサービスの購入を誘導したりするといった具合だ。「AIのリスクは100%現実のものとなっている」（グリーン氏）
ガイドラインを順守する
ABBYY DevelopmentのAI倫理エバンジェリストであるアンドリュー・ペリー氏は、信頼できるAI技術の原則やガイドラインを順守することを企業に提案する。信頼できるAIは、以下の要素で構成されている。

・AIシステムの出力に公平さや正確さが確保されている
・AIシステムが倫理的に正しい方法で運用される
・AIシステムが学習するデータが適切に保護され、個人のプライバシーを毀損（きそん）していない
・AIシステムのセキュリティが確保されている
・AIシステムの出力について誰が責任を持つのかを明示している
・AIシステムの仕組みや出力に透明性がある

　信頼できるAIの構築に関する枠組みを提供する組織には以下がある。

・経済協力開発機構（OECD）
・ハーバード大学クラインセンター（Harvard University Berkman Klein Center for Internet & Society）
・スタンフォード大学人間中心のAI研究所（HAI：Stanford Institute for Human-Centered Artificial Intelligence）
・AI Now Institute

　他にも、米国立標準技術研究所（NIST）は、信頼できるAIシステムを開発、導入する企業に向けて、リスク管理フレームワーク「Artificial Intelligence Risk Management Framework」（AI RMF）を提供している。
AIを監視する
AIシステムのパフォーマンスを人間の目で継続的に監視することも一つの手だ。AIシステムが予想していた結果とは異なる動作をしたり、誤ったデータを提供したりした場合に問題を特定する。その対策として、アルゴリズムの調整やデータの修正をする。AIシステムが自動化に下した判断を人間が変更するといったプロセスを組み込むことも含まれる。

　データ分析ベンダーDAS42の創業者で取締役を務めるニック・アマビレ氏は、データガバナンス、データリテラシー、トレーニングなどに精通する従業員を配置し、実行するプロセスを構築することが重要だと述べる。データのセキュリティやプライバシーを管理するツールを、アルゴリズムの管理に拡張することも一つの手だ。

　アナリティクスツールベンダーSAS Instituteのストラテジックアドバイザー、キンバリー・ネバラ氏は、以下の問いを検討することを企業に推奨している。

・自社のAIシステムはどのような条件でどのようにエラーを引き起こす可能性があるか
・AIシステムが意図的にエンドユーザーをだますことは起こり得るか、それはどのような状況で起こり得るか
・AIシステムが誤解されたり、誤用されたりする可能性はあるか
・その影響は何か、それはどのように拡大するか
・AIシステムは、誤用や誤解を発生させるように設計されていないか
・AIシステムは、他のシステムとどのように連携できるか、連携した結果どのような影響が発生するか

本記事は米国Informa TechTargetの記事「AI existential risk Is AI a threat to humanity」を翻訳・編集したものです。一部、翻訳作業に生成AIを活用しています。
TechTargetジャパン",[],[]
