headline,mainEntityOfPage,image,datePublished,dateModified,author,media_en,media_jp,str_count,body,images,external_links
"NVIDIA独占のAIチップ市場、高速推論で新たな攻防か？対抗するスタートアップ、1秒間で4,000万トークンを処理するデータセンター設置へ（AMP［アンプ］）",https://news.yahoo.co.jp/articles/2deaf812f055884467b45c83eb56ed6bc77418c1,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20250506-00010000-ampreview-000-1-view.jpg?exp=10800,2025-05-06T06:01:43+09:00,2025-05-06T06:01:43+09:00,AMP［アンプ］,ampreview,AMP［アンプ］,3055,"AIデータセンター市場の現状、NVIDIAが市場を席巻
VIDIA独占のAIチップ市場
AIデータセンター向けチップ（GPUを含める）市場は急激な成長を見せている。2024年の180億ドルから2034年には1,830億ドルへと拡大が予測され、年平均成長率は14.2%に達する見込みだ。特に北米市場は全体の37.1%を占め、66億ドルの市場規模を誇る。

この市場で圧倒的な存在感を示すのがNVIDIAだ。2023年のデータセンター向けGPU出荷台数は約376万台に達し、前年の264万台から大幅に増加。市場シェア98%を維持しつつ、データセンター向けGPU収益は前年の109億ドルから362億ドルへと3倍以上に急増した。

NVIDIAの市場支配力は、2025年にさらに強まると見られている。AI向けプロセッサ用ウェハー消費における同社のシェアは、2024年の51%から2025年には77%まで拡大する見込みだ。これに対し、AWS（アマゾン・ウェブ・サービス）は10%から7%へ、グーグルは19%から10%へ、AMDは9%から3%へと、それぞれシェアを落とす公算が高いとされる。

この支配的な地位を支えているのが、大型のGPUとなる。特に2025年に投入されたB200 GPUは、ウェハー消費量22万枚に上り、収益も58億4,000万ドルに達する見込みだ。また。H100・H200・B300といった他のAI向けGPUも需要を押し上げる要因となっている。これらの製品は、TSMCの4nmクラスのプロセス技術を使用したものだが、演算チップのサイズは814平方ミリメートルから850平方ミリメートルに及ぶ大型のものになるという。

注目されるのは、AIモデル開発に加え、推論向けのチップ需要が急速に伸びていることだろう。推論向けセグメントは、2024年に56.5％を占め、今後さらに伸びると予想されている。これには、AI技術の実用化が進み、自動運転や医療診断、金融分析など、リアルタイムで高速な推論処理を必要とするアプリケーションが増えていることが背景にある。
今後予想される展開：NVIDIAが強いが、棲み分けのシナリオも、Cerebrasが示す可能性
AIデータセンター向けチップ市場では、NVIDIAが独占的地位を維持すると見られるが、推論に特化するプレイヤーらによるシェア争奪の可能性もゼロではない。前述したように、AI実用化の進展に伴い、推論（運用）向けのチップ需要がさらに増加するためだ。

この需要を見越し、高速推論に特化したCerebras Systemsが活発な動きを見せる。

同社は、北米と欧州で6つの新しいAIデータセンターを設置し、推論処理能力を現在の200万トークン毎秒から4,000万トークン毎秒へと20倍に拡大する計画を発表したのだ。

これらは、Cerebrasが開発したWafer-Scale Engine（WSE-3）プロセッサをベースとするAIデータセンターで、従来のGPUベースのソリューションと比較して10〜70倍高速にデータを処理することができるとされる。

データセンターが新設されるのは、ダラス・ミネアポリス・オクラホマシティ・モントリオール・ニューヨーク・フランスの6拠点で、その85%が米国内に設置されることになる。中でも注目されるのがオクラホマシティの施設だ。竜巻の多い地域性を考慮し、観測史上最大規模の竜巻にも耐えられる堅牢な設計を採用し、300台以上のCerebras CS-3（WSE-3搭載）システムを設置、停電対策として3系統の独立した電源設備を備える。さらに、高性能AIチップの冷却に必要な特殊な水冷システムも導入し、24時間365日安定稼働する体制を整える計画だという。

さらに、Cerebrasは英エジンバラ大学のスーパーコンピューティングセンターEPCCにも4台のCS-3システムを設置。これにより、EPCCは2,400億から1兆のパラメータを持つモデルのトレーニングが可能となり、700億パラメータのモデルを1日で微調整できる体制を整えた。

一方、モントリオールでは、データセンター運営会社のBit Digitalが新たな施設を建設中だ。この20万2,000平方フィートの施設は、Cerebrasとの5メガワットのコロケーション契約に基づいて整備されており、2025年7月に稼働開始予定だという。

こうした一連の動きは、AIインフラ市場における新たな展開を示唆する。NVIDIAが依然として市場を主導する一方で、高速推論に特化したプレイヤーによる市場の棲み分けが進む可能性が見えつつある。
推論向けデータセンター市場の新たな展開
CerebrasとともにAI推論市場で注目を集めているのが、SambaNovaだ。同社の最新AI処理チップ「SN40L」は、3種類のメモリを階層的に組み合わせた独自の設計を採用。それぞれのメモリは、高速データ転送用の「高帯域メモリ」、大容量データ保存用の「DDR DRAM」、頻繁に使用するデータの一時保存用の「オンチップSRAM」と、異なる役割を持つ。この3層構造により、膨大なAIデータを効率的に処理することを可能にした。

SambaNovaチップの優位性は、DeepSeek R1 671Bモデルを量子化（データ圧縮）なし、かつサーバー1ラックのみで、毎秒250トークン以上の高速処理を実現した点からうかがうことができる。また、電力消費も1ラックあたり8〜15キロワット、平均10キロワットと効率的な運用が可能となっており、サステナビリティの観点からも注目される存在となっている。

Cerebrasとは直接競合する関係だ。Cerebrasの「WSE-3」は、スマートフォンの数百倍にあたる4兆個のトランジスタと90万個の演算コアを搭載した大規模なチップ。44ギガバイトの高速メモリを内蔵し、毎秒21ペタバイトという超高速なデータ処理を可能とするものの、課題もある。

SambaNovaによると、Cerebrasチップは内蔵の高速メモリのみを使用する設計のため、大容量の外部メモリを追加できないという。また、一度に一人のユーザーの処理に特化した設計となっており、複数のユーザーが同時に利用する一般的なデータセンター環境では効率が低下する可能性がある。さらに、大規模なチップ設計のため、消費電力が23キロワットとSambaNovaの2倍以上に達する点も指摘している。

アジア市場では、SambaNovaはソフトバンクとの提携を拡大し、日本のAIデータセンターにSambaNova製チップを搭載したラックを導入する計画だ。これにより、アジア地域の開発者は、東京科学大学が開発した日本語オープンソースモデル「Swallow」や、メタのLlama、アリババのQwenなどにSambaNova Cloudを通じてアクセスできるようになる。

このように、推論向けデータセンター市場では、CerebrasとSambaNovaが異なるアプローチで高速推論の実現を目指している。両社の技術革新により、AIモデルの処理性能は飛躍的に向上し、より多くのユーザーが高性能な推論処理を利用できる環境が整いつつある。特に、アジア市場の開拓が本格化することで、市場のさらなる成長が期待される。
文：細谷元（Livit）",[],[]
