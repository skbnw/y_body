headline,mainEntityOfPage,image,datePublished,dateModified,author,media_en,media_jp,str_count,body,images,external_links
AIも半導体も出遅れ？ 経産省が迫られる「選定」と「切り捨て」とは（ビジネス＋IT）,https://news.yahoo.co.jp/articles/1dbb591019222bece065198ddf73417887b9c90f,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20250522-00164036-biz_plus-000-1-view.jpg?exp=10800,2025-05-22T07:10:05+09:00,2025-05-22T07:10:05+09:00,ビジネス＋IT,biz_plus,ビジネス＋IT,3259,"国家標準戦略素案で重点分野が絞られつつある（出典：経産省「イノベーション拠点としての 国際競争力強化に向けて 」資料より）
「選択と集中」が日本の産業政策の鍵とされる今、経産省が描く“集中投資”の戦略が注目を浴びている。だが、その裏には“切り捨て”という避けがたい現実がある。米中が巨額投資で狙いを定める中、日本はどう動くべきか。研究開発の柔軟性、人材の再配置、そして規制の壁…。成長分野に注がれる光の陰で、見過ごされがちな課題に迫る。
【詳細な図や写真】各国で「選択と集中」が進んでいる（出典：経産省「イノベーション拠点としての 国際競争力強化に向けて 」資料より）
世界で進む重点分野への集中投資
国際社会で競争力のある稼ぎ柱となるような産業を育てるため、日本全体として「選択と集中」を進めるべきだ、という声が高まりつつあります。

　3月に政府が開いた有識者会議（経済産業省産業構造審議会イノベーション・環境分科会イノベーション小委員会）では、他の主要国と比べて日本が「選択と集中」の波に乗り遅れている現状を問題視する声が上がりました。会合での議論をベースに、国際的な動きと比較したこの国の立ち位置と課題について確認していきましょう。

　世界の有力国では、重要技術となる分野を絞り込み、戦略文書にリスト化して、重点的に支援する政策を進めています。

　たとえば米国では「CETs（Critical and Emerging Technologies、重要な新興技術）」と題するリストを作成し、その最新版（2024年2月）では人工知能（AI）やバイオテクノロジー、半導体、宇宙技術などを重要分野として挙げています。

　トランプ大統領は、再就任後、CETsをベースにした集中投資に前向きです。同氏は2025年3月下旬、マイケル・クラツィオスOSTP（科学技術政策局）局長に対し、重点分野をめぐる政策について指示しました。その中で、原子爆弾を開発する「マンハッタン計画」と、宇宙開発をめぐる「アポロ計画」、そしてインターネットの誕生を引き合いに出し、イノベーションが米国の原動力だったと指摘したのです。

　この上で、「CETsにおいて、潜在的な敵対国に対する優位性を維持しながら、比類なき世界のリーダーとしての地位を確保するにはどうすればよいのか」「米国の科学技術事業を活性化させるにはどうすればよいのか」「科学の進歩と技術的イノベーションが、経済成長を促し、米国民の生活を向上させるにはどうすればよいのか」の3点に取り組み、「科学の新たなフロンティアを切り開く」よう求めました。


　また、中国は、2021年3月に定めた「第14次五か年計画」の中で、「国家実験室の再編や国家科学センターの建設の対象分野」として、AIや量子情報、バイオメディカルなどを認定。「ブレイクスルー強化のための重要な先端科学技術分野」と位置づける次世代AIや脳科学、遺伝子、臨床医学などとともに、戦略的な投資を行っています。

　同様の戦略文書はそのほか、イギリスやオランダ、ドイツ、韓国、オーストラリアといった国々でも作成されています。いずれも2020年代に入ってから最新版が策定され、AIや半導体から「深宇宙、深地球、深海、極地探査」（中国）、「次世代原子力」（韓国）、「循環型経済の基盤」（ドイツ）まで対象領域はさまざまです。


　では、各国が定めた重点分野には、どのような支援が行われているのでしょう。半導体分野を例にとると、米国では2022年、設備投資などへの補助基金（5年で390億ドル＝約5兆3,000億円）や研究開発基金（5年で110億ドル＝約1兆5,000億円）、設備投資に対する25％の減税措置などを行っています。

　中国では地方政府を合わせて10兆円超の半導体向け産業の基金があり、ヨーロッパでは2030年までに累計430億ユーロ（約6兆2,000億円）規模の官民投資を計画。台湾では2023年5月時点で累計2.1兆台湾元（約9兆4,000億円）の投資を行い、韓国では2026年までに340兆ウォン（約35兆7,000億円）以上の投資を達成する方針です。
なぜ出遅れ？ 日本の現状と課題
日本も、成長が期待できる分野を対象にした税制優遇や補助金といった支援は行われています。が、「選択と集中」という観点では他の主要国に比べて改善の余地があると考えられています。


　経産省の担当官は会合で、グローバルな「投資誘致政策競争」の中、日本の研究開発投資に関する国際収支が大幅赤字となっていると指摘。日本の1人負けを避けるため、「国際的に遜色のない競争環境を確保するためのインセンティブ施策の整備」「企業の中長期目線での野心的な挑戦や、大学などと連携を深めていくためのインセンティブ施策の強化」などの方向性を提示しました。


　企業が戦略分野への研究開発投資を拡大するための施策強化を打ち出した上で、「量子、AI、バイオなど」といった具体的なジャンルを示唆。また、中堅企業の成長やオープンイノベーションの活用による研究開発力の底上げ、博士人材などの育成や“越境人材”の交流促進、知的財産分野における国際標準化戦略の策定といった方向性も示しました。

　また、研究開発を推進するインセンティブの在り方について、企業連携、スタートアップ連携、大学連携という3つの観点で課題を分析しています。

　企業連携については、強みがある分野に投資を継続する傾向があると指摘し、中堅企業を含め中長期目線で「野心的」な挑戦を促す仕組みを検討する必要があるとしました。

　スタートアップ連携に関しては、スタートアップを対象としたM&Aがまだまだ低調である現状を指摘した上で、「出口・成長経路の多様化に向けて、M&Aの一層の加速が必要」との認識を打ち出しました。大学連携については、産学の大型連携を促進するインセンティブ強化や、博士号取得者の積極活用を促す仕組みについて検討する方向性を提示しています。
「切り捨てるための環境整備」とは？
選択と集中というと、集中投資の資金が向かう先が注目を浴びがちですが、選択から漏れる分野が政策的に「切り捨てられる」という現実もあります。

　会合である有識者は、日本企業に見られる「企業年齢の加齢」に伴う利益率の低下傾向と、多くの企業が過去10年間、業績に関わらず同じ研究開発を続けている現状を指摘。対照的に、米国企業はより柔軟に研究開発の方向性を変化させていると言及されました。

　この状況を踏まえ、研究開発への投資を促進するためには、不採算事業からの撤退を容易にすること、そして技術面以外の参入障壁（業界の商慣習や法令・許認可など）を取り除く必要があるといった提言がなされました。

　少子化、高齢化、人口減少が進む中で、限られた資源を重点分野に投入する流れそのものは、この時代状況で不可避の選択なのかもしれません。一方、集中投資の恩恵を受ける分野と、そうでない分野をどのような判断基準で分けるのか、撤退分野に携わってきた人材を成長分野にいかに円滑にスライドさせるかについては、十分な議論が必要でしょう。

　また、企業の柔軟な事業転換や研究開発の方向修正を促すためには、単なる資金投入だけでなく、規制緩和や人材育成といった包括的な環境整備が求められます。「選択と集中」の戦略をいかにバランス良く、そして社会全体としての付加価値創出と経済成長につなげていけるかがポイントになりそうです。
＜参考資料＞
　経済産業省産業構造審議会イノベーション・環境分科会イノベーション小委員会
　https://www.meti.go.jp/shingikai/sankoshin/sangyo_gijutsu/innovation/008.html
執筆：小達 紀治、編集：ジャーナリスト 川辺 和将",[],[]
NVIDIA最大イベントでジェンスン・フアンCEOが語った「重要すぎる戦略転換」（ビジネス＋IT）,https://news.yahoo.co.jp/articles/86f3bf990a3046c2d9a1c21d315bc369c56f90f3,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20250522-00162177-biz_plus-000-1-view.jpg?exp=10800,2025-05-22T06:50:05+09:00,2025-05-22T06:50:05+09:00,ビジネス＋IT,biz_plus,ビジネス＋IT,4473,"NVIDIAのジェンスン・フアンCEO、推論モデルからロボティクスまで包括戦略を発表（Photo： jamesonwu1972 / Shutterstock.com）
NVIDIAの年次カンファレンス「GTC（GPU Technology Conference）」において、同社のジェンスン・フアンCEOが大幅な戦略転換を明らかにした。「Dynamo」などのオープンソースによるエコシステム構築、推論モデルやエージェンティックAI向けの新型GPUプラットフォーム「Blackwell Ultra」に加えて、ロボット向け基盤モデルも公開、自動車産業のデジタル化も推進する。AI半導体メーカーの枠を超えた存在へと舵を切る同社の取り組みを取り上げたい。
【詳細な図や写真】NVIDIAは徹底的にオープンソースを重用している（Photo/Shutterstock.com）
オープンソースによるエコシステム構築
2025年3月17021日、NVIDIAの年次カンファレンス「GTC」が開催された。同社のジェンスン・フアンCEOが明らかにしたのは、エージェンティックAIとロボット分野を軸に、AI半導体／インフラ需要を押し上げつつ、総合的なAIプラットフォーマーとしてのポジションを確立する新戦略だ。

　エージェンティックAI分野では、オープンソースモデルとソフトウェアの提供によるエコシステム拡大を狙う。

　具体的な取り組みの1つが、AIモデルの推論処理を最適化するオープンソースソフトウェア「Dynamo」の公開だ。これは複数のGPUを使って行われる推論処理（AIモデルが回答を生成する処理）を最適化するソフトウェア。

　たとえば、ユーザーからの質問に対する「理解」と「回答生成」という異なる処理を、それぞれ別のGPUに振り分けて最適化。さらに、過去の質問に関する知識をGPUのメモリに保持し、類似の質問が来た際に再計算を避けることで、処理効率を大幅に向上させる仕組みを備えている。AI検索のPerplexityは、Dynamoを活用した取り組みを進めているという。

　フアンCEOはDynamoに関して、「産業界は異なる方法でAIモデルを考え、学習させることで、時間とともにより洗練されたものへと進化させている。カスタマイズされた推論AIの未来を実現するため、NVIDIAのDynamoはこれらのモデルをスケールさせながら、AIファクトリーのコスト削減と効率化を支援する」と説明した。

　同社はまた、メタのオープンソースモデル「Llama」をベースに、推論機能を強化した「LlamaNemotron」シリーズを発表。同シリーズは、エッジ向けの「Nemotron Nano」、単一GPUデータセンター向けの「Nemotron Super」、マルチGPUデータセンター向けの「Nemotron Ultra」の3モデルで構成される。Anthropicの最新モデルClaude 3.7と同様に、推論モードをオン・オフできる機能を備えており、クエリの処理負荷を最適化することが可能だ。

　これらのオープンソース戦略には、Together AIやCohere、Perplexityといった企業が賛同を表明。Together AIのチェ・ジャン最高技術責任者は、「推論モデルを効率的にスケールさせるには、分散サービングやコンテキストを意識したルーティングなど、高度な推論技術が必要となる。NVIDIAのDynamoのオープン性とモジュール性により、我々のエンジンにシームレスに統合でき、リソース利用を最適化しながら、より多くのリクエストに対応できるようになる」と評価している。

　このように同社は、推論モデルとエージェンティックAIの普及を見据え、オープンソース戦略を通じてエコシステムの拡大を推進。これにより、自社のハードウェア需要の底上げを図る構えだ。
2つの需要を取り込むハードウェア戦略
NVIDIAは、オープンソース戦略と並行して、推論モデルとエージェンティックAIの普及を見据えたハードウェアの強化も進めている。その中核となるのが、次世代AI処理基盤「Blackwell Ultra」プラットフォームだ。

　同プラットフォームの特徴は、推論モデルやAIエージェントシステムの処理に最適化された設計にある。72台のBlackwell Ultra GPUと36台のArmベースのGrace CPUをラックスケールで接続した「GB300 NVL72」を提供。複雑な課題を複数のステップに分解し、異なる解決策を探る推論モデルの処理に対応する。

　また16台構成の「HGX B300 NVL16」では、前世代のHopperと比較して、大規模言語モデルの推論速度を11倍、計算能力を7倍、メモリを4倍に向上させた。

　こうした性能の大幅アップは、処理トークン数が何十倍にも増える推論モデルやエージェンティックAIの普及に不可欠なもの。

　GTCでは、このことを示すデモが披露された。

　このデモでは、従来型モデルとなるメタのLlamaモデルと、推論モデルとなるDeepSeekのR1に対し、結婚式での7人掛けテーブルの座席配置という課題が与えられ、回答精度と処理トークン数の比較が実施された。新郎・新婦の両親を隣り合わせにしないなどの制約条件付きの複雑な課題だ。

　メタのLlamaは、即座に439トークン（約329ワード）の回答を生成したものの、その内容は誤りを含んでいた。一方、DeepSeek R1は制約条件を一つずつ検証しながら慎重に推論を重ね、すべての条件を満たす正確な座席配置を導き出すことに成功した。しかし、そのトークン数は、Llamaの20倍近い8559トークンに及んだ。

　こうした推論モデルは、さまざまなエージェントシステムに組み込まれており、クエリ次第では処理トークン数は、何百倍にも膨れ上がる可能性もある。たとえば、史上最速で年間経常収益1億ドルを達成したと言われるコーディングアシスタントCursorでは、OpenAIのo3-miniやDeepSeekのR1などが利用可能だ。

　フアンCEOは、推論モデルやエージェントシステムの応答時間を、既存モデルと同等に速さに維持するには10倍早く処理する必要があり、最終的に処理するデータ量は少なくとも100倍にはなるだろうと指摘している。

　Blackwellは、こうしたニーズに対応できるように設計されており、テック企業／AI企業からの注目度は高い。実際、Blackwell GPUの初年度出荷数は360万台に達する見込みで、前世代「Hopper」の最盛期における主要クラウド4社（AWS、マイクロソフト、グーグル、オラクル）向け出荷数130万台を大きく上回る規模となる。
ロボティクス分野でも取り組み加速
NVIDIAは、AI開発の次のフロンティア、ロボティクス分野への展開も加速させている。

　GTCで発表された世界初のオープンソースの人型ロボット向け基盤モデル「Isaac GR00T N1」は、その足がかりとなる。

　フアンCEOは、「ジェネラリストロボットの時代が到来した」と宣言。世界的な労働力不足（推定5000万人以上）への対応を見据え、ロボット開発の加速を目指すという。

　GR00T N1の特徴は、人間の認知原理に基づく二重システムアーキテクチャにある。「システム1」は人間の反射や直感に相当する高速思考アクションモデル、「システム2」は熟考を要する意思決定のための低速思考モデルとして機能する。システム2は視覚言語モデルを活用して環境や指示を理解し行動を計画、システム1がその計画を正確な動きに変換する仕組みだ。これにより、物体の把握や両腕での移動、片方の腕から他方への受け渡しなど、一般的なタスクに柔軟に対応できる。

　同社は、グーグル・ディープマインドおよびディズニー・リサーチと共同で、ロボット向けの新たな物理演算エンジン「Newton」の開発も推進している。Newtonは、ロボットが物体を扱う際の挙動をより正確にシミュレーションできる仕組みだ。このエンジンを既存のロボット開発用ソフトウェアに組み込んだところ、学習処理の速度が70倍以上向上。ロボットの動作の精度を高めながら、開発期間の大幅な短縮を実現している。

　さらに、NVIDIAは実データの収集に伴う課題を解決するため、「Isaac GR00T Blueprint」を発表。同社のシミュレーションプラットフォーム「Omniverse」と世界基盤モデル「Cosmos Transfer」を活用し、少数の人間によるデモンストレーションから大量の合成モーションデータを生成する。実際、わずか11時間で78万件の合成軌道（人間の実演データ換算で約6500時間、9カ月分に相当）の生成に成功。これを実データと組み合わせることで、GR00T N1の性能を40％向上させている。

　GTCでは、これらの技術が活用されたとみられるロボット「Blue」が登場し、その自律的な動きで会場からの喝采を浴びた。
自動運転分野での提携が示す「新展開」
NVIDIAは自動運転分野でも存在感を示したい考えだ。

　GTCでは、米国最大の自動車メーカーであるゼネラルモーターズ（GM）との包括的な提携を発表。次世代の自動車開発、工場、ロボットの各分野でAI、シミュレーション、高性能コンピューティングを活用する計画という。


　具体的な取り組みは、大きく2つの領域に分かれる。

　1つ目は、工場のデジタル化だ。NVIDIAのOmniverseとCosmosを活用し、生産ラインのデジタルツインを作成。これにより、ダウンタイムを削減するためのバーチャルテストや生産シミュレーションを実現する。また、資材の運搬・輸送や精密溶接などの作業を行う既存のロボットプラットフォームの訓練を通じて、製造の安全性と効率性の向上も目指す。

　2つ目は、次世代の車載コンピューター「NVIDIA DRIVE AGX」を活用した自動運転システムの開発だ。同システムは、NVIDIAの最新半導体技術と専用の制御ソフトウェアを組み合わせることで、1秒間に1000兆回もの演算処理を実行することができる。現行の車載コンピューターと比べて圧倒的な処理能力であり、より安全で高度な自動運転システムの構築を目指す。

　今回のGTCで示されたNVIDIAの戦略は、半導体メーカーの枠を大きく超えるもの。推論モデルやエージェンティックAIの基盤を強化しながら、ロボティクスや自動運転まで、AIプラットフォーマーとしての存在感を一段と高める内容となった。今後の展開が注目される。
執筆：細谷 元、構成：ビジネス＋IT編集部",[],[]
【10分完全攻略】AI規制をわかりやすく解説：日本・EU・主要6ヵ国の“注意点”とは（ビジネス＋IT）,https://news.yahoo.co.jp/articles/d8386d2dfd34fb3681ef32da7dcc3744c75bc8c7,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20250522-00163222-biz_plus-000-1-view.jpg?exp=10800,2025-05-22T06:10:06+09:00,2025-05-22T06:10:06+09:00,ビジネス＋IT,biz_plus,ビジネス＋IT,11906,"日本企業はグローバルと比較してAI規制対応について「自信がない」。日本とグローバルでは「非常に自信あり」「かなり自信あり」の回答の差が20ポイント以上開く結果に（出典：PwC「Global Digital Trust Insights 2025」を基に編集部作図）
AIの進化はビジネスのあり方を劇的に変化させる一方で、法的な課題や倫理的な問題も浮上しています。そのため、国内外でAI規制が急速に整備されつつあり、企業はこれらの規制に適応する必要性が高まっています。本記事では、日本と欧州などの主要地域におけるAI規制の現状とその違いを比較し、ビジネスにおける影響を考察するとともに、各国の規制が企業活動に与える具体的な影響や、遵守するための課題について包括的に解説します。各国のAI規制のポイントをまとめたExcelデータも用意しているので、ぜひチェックしてください。
【詳細な図や写真】欧州AI法におけるハイリスクAIシステムに対するガードレール概要（出典：PwCコンサルティングの資料を基にビジネス＋IT編集部作図）
AI規制の現状と背景
AI技術はビジネスのさまざまな分野で広く活用され、企業の成長に寄与する一方で、個人情報の漏えい、偏見、意思決定の透明性の欠如、倫理問題などリスクと課題をもたらしています。そのため、各国はAIに特化した包括的な規制を制定し始めています。

　現状では、最も厳しい「欧州AI法」、それに続いてAI産業促進を主として成立した「韓国AI基本法」、主に既存法規制で対応する米国、欧州同等に厳しい規制内容である「包括的AI規制」を制定した中国、ブラジル、そしてAI規制と技術革新の両立を目指すことを基本とする日本などがあり、今後もこうした動きは増え続ける見込みです。

　各国の規制内容にはばらつきがみられ、企業はそのコンプライアンス対応に苦慮しています。

　これらの法令は部分的には異なる特徴がありますが、AI規制準拠のための制度的ガードレールを構築している点は類似しています。また、各国は既存法令による規制を強化しています。そのため、AIの利活用などにおける違反行為は、適用される法令によってペナルティーが高額化する傾向にあります。

　また、製品面でも、AIシステムに使用するデータ（事前）、AIシステムの処理過程（事中）、AIシステムのアウトプット（事後）など、AIシステム利活用の全ライフサイクルにおいて、透明性、説明責任、差別禁止、個人情報保護、著作権保護、製造物責任などさまざまな既存法令の適用が考えられます。

　したがって、AI規制コンプライアンス対応は、AIに特化した包括的な規制への準拠のみでは十分と言えず、AIシステムに関係する法令を幅広く調査し、対応を検討する必要があります。


　ここからは、欧州、米国、中国など主要国のAI規制を中心に、多層的で複雑なAI規制への準拠について、グローバルに事業を展開する企業は何をすべきかを解説します。そして、日本のAI規制動向についても解説します。
【EU】「欧州AI法」とは？概要・特徴・注意点を解説
欧州AI法は、2024年5月21日に成立し、8月1日に発効しました。今後、規制内容に応じて2030年12月31日までに段階的に施行されていきます。リスクベースのアプローチが採用され、AIをリスクの程度で分類し、その程度に応じた規制が適用されます。

　欧州AI法は、生成AIを含む世界初の包括的なAI規制です。ペナルティー金額が最高3,500万ユーロ（執筆時点の為替レートで55億円強）または全世界売上高の7％のいずれか高い方と、非常に厳しい罰則を設けています。

　欧州AI法は、EU域内で上市されるAIシステムおよび汎用目的型AIモデルと関連するステークホルダーを規制対象としています。適用対象は欧州においてAIシステムを提供、販売、展開するプロバイダー、デプロイヤー、インポーター、ディストリビューターで、サプライチェーン上の関わり方に応じて規制されます。

　欧州においてAIシステムのアウトプットが使われる場合など、関わり方により欧州域外の事業者も適用対象となる点に特に注意が必要です。

　欧州AI法は、リスクに応じて主に「3つのAI」について規制しています。

　1つ目が人命や基本的人権に侵害をもたらす可能性の高い「許容できないリスクを伴う行為」に該当し、活用が禁止されるAI。2つ目がAI法上の要件を満たせば利活用可能な「ハイリスクAI」。最後が透明性・情報提供など、一部の要件を課される「限定リスクの特定用途AI」です。

　許容できないリスクは、禁止されるAIシステムとして語られることが多いですが、ハイリスクAIや限定リスクの特定用途AIと異なり、主にAI技術を悪用する行為に対する禁止となります。

　これは、欧州委員会が欧州AI法案を検討するに先立って、2019年4月に「AIに関するハイレベルの専門家グループ（AI HLEG）」が策定した「信頼できるAIのための倫理ガイドライン（Ethics guidelines for trustworthy AI）」の3原則とも一貫性を有しています。


「信頼できるAIのための倫理ガイドライン」における3原則

・適用されるすべての法律と規制を尊重
・倫理原則と価値観の尊重
・技術的、社会環境の両方を考慮した堅牢性


　欧州AI法は、この倫理ガイドラインの3原則への違反行為を禁止されるAI行為として「人の意識に対する意図的操作行為」「弱者に対する搾取行為」、社会的スコアリングとして「一般的な利用行為」「顔認証データの無差別な収集と利用行為」「人の生体認証データからその思想信条などを推測する行為」など具体化しています。

　一方、ハイリスクAIに対しては詳細な規制を設けています。ハイリスクAIは、許容できないリスクを冒さないように利活用する必要があり、欧州AI法は新たに包括的な共通の法的要件を課することで実装すべきガードレールを提供しています。

　具体的には、上記の「信頼できるAIのための倫理ガイドライン」における7つの要件と評価要求を以下のとおり法制化しています。


　ハイリスクAIには、主に2種類のAIシステムが含まれています。

　1つは、欧州AI法「付属書1」で規定される医療機器、玩具、機械など既存規制の対象となる特定製品が使用する安全コンポーネントにおけるAIシステムです。これらの製品における安全コンポーネントは人の健康と安全に深く関わるからだと考えられます。

　もう1つは、欧州AI法が「付属書3」で列挙する自然人の感情認識、重要インフラの管理と運用、教育職業訓練、雇用関連などの分野に活用されるAIシステムになります。これらの分野におけるAIシステムは、その使用方法により許容できないリスクを冒しやすいからだと考えられます。

　ハイリスクAIについて、その開発者の主な義務は、AIシステムの要件や品質などの技術面と、CEマーク貼付や登録など制度面において欧州AI法に適合することとし、導入者／利用者への要求事項は、権利侵害を防ぐための人間による監視、個人情報保護のためのデータ保護影響評価、データガバナンスなどとなっています。

　生成AIなどの汎用AIモデルについては、人の権利や安全に大きく悪影響を及ぼすなどシステミックリスクがあるため、ハイリスクAIに適用される義務のほか、リスク軽減のために必要となる情報提供、透明性に関する追加要件を課しています。

　欧州AI法は、AIの開発、利活用についてガードレール的な制度的フレームワークを提供するに過ぎません。

　2025年2月4日に、欧州委員会は欧州AI法において禁止されるAI利活用行為に関する拘束力のないガイドラインを公表し、当局としての法解釈観点を示しました。禁止されるAI利活用行為に該当するかという問題は、おおむね既存法の解釈に大きく依拠する内容になっています。

　当該ガイドラインが示すように、欧州AI法は、欧州デジタル法体系全体の中で理解する必要があります。

　たとえば、AIシステムに取り込むデータ、とりわけ個人情報の保護は欧州の一般データ保護規制（GDPR）によって規制され、違反した場合のペナルティーは非常に高額になっています。また、欧州の改正製造物責任指令（PL指令）は、AIシステムを規制対象として追加しており、欠陥あるAIシステムは消費者による訴訟の対象となる可能性があります。さらに、AIシステムは製品セキュリティ規制としての欧州サイバーレジリエンス法上のサイバーセキュリティ要件を満たすことが求められます。

　したがって、欧州AI法対応は関連する複数の法令への複合的な対応が必要となります。
【米国】AI規制の現状：法はないが大統領令と州ごとの規制
米国は連邦レベルにおける包括的なAI法はなく、大統領令による限定的なAI規制を試みてきました。

　2025年1月23日、トランプ米大統領は「AIにおける米国のリーダーシップへの障壁を取り除く大統領令（Removing barriers to American leadership in Artificial Intelligence）」を出しています。

　これにより、AIがもたらすメリットの把握とそのリスク管理を目的としたバイデン政権の「AIの安全・安心・信頼できる開発と利用に関する大統領令（The Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence）」（2023年10月30日）を覆しました。

　AIに対する大統領令による規制は、時の政権のAI規制に対する態度に左右され、安定性に欠けるところがありますが、米国のAI規制はアルゴリズムによる差別禁止、プライバシー保護など、主にAI倫理を中心に規制する傾向にあります。

　たとえば、安全で効果的なシステム、アルゴリズム由来の差別からの保護、プライバシー保護、ユーザーへの通知と説明、人間の介入の5原則を提唱した2022年10月の「AI権利章典（AI Bill of Rights）」はその典型と言えます。


「AI権利章典」5つの原則

・安全で効果的なシステム
・アルゴリズム由来の差別からの保護
・プライバシー保護
・ユーザーへの通知と説明
・人間の介入


　また、2023年1月に米国商務省国立標準技術研究所（NIST）が発表したAI技術のリスク管理のためのガイダンス「AIリスクマネジメントフレームワーク（AI RMF）」は、信頼できるAIシステムのリスクコントロールのために「7つのリスク要点」を提示し、その対応策を提唱しています。

　NISTの当該フレームワークは、欧州の「信頼できるAIのための倫理ガイドライン」と中身はほとんど同じであり、AIシステム、サービス、システムの設計、開発、利活用、評価に信頼性の考慮を組み込むことでAIリスクをコントロールし、その信頼性を向上するための方法論を提供しています。


　一方、米国各州におけるAI規制法の制定は活発で、主に、プライバシー保護、ディープフェイク、消費者保護などの分野における規制が顕著です。

　たとえば、2024年においてカリフォルニア州のニューサム知事は、AIの透明性規制、政府の生成AI使用要件、AIによる選挙関連のディープフェイクコンテンツ生成規制、児童虐待防止法、ヘルスケア分野におけるAIの利活用、個人情報および消費者プライバシーの保護などを規定した「AI関連法案」に署名し、多くのAI規制法が成立しました。AIを開発する企業に広範な安全性と透明性の要件を課す、より厳しい内容の「SB 1047法案」は否決しましたが、AI規制の傾向にそれほど影響はないと言えます。

　また、2024年5月に成立したコロラド州の「AIに対する消費者保護法（SB24-205 Consumer Protections for Artificial Intelligence）」は、欧州AI法の理念を取り入れ、主に「ハイリスクAI」を規制する米国州レベルで初めてとなる包括的なAI規制です。

　コロラド州法は、教育、雇用、金融サービス、政府サービス、医療、住宅、保険、法律サービスなどの重要な分野で重大な意思決定に影響を与えるAIシステムをハイリスクAIとし、その開発者にリスク管理ポリシーとプロセスの実施、アルゴリズムによる偏見リスクの識別、記録、軽減などを求めています。

　また、開発者と導入者の責任を明確にし、消費者に対して使用するAIシステムの具体的な情報、機能、制限、可能性のあるリスクを明確に公開することも求めています。他にも、2024年7月にテネシー州では人々の声や肖像の不正使用、特にAIによって生成された声や肖像の不正使用を防ぐための「ELVIS法」が施行されました。

　米国は連邦レベルにおける包括的なAI法はないとはいえ、AIに対する既存法令による規制は存在しているため、欧州同様に、米国における既存法体系の中でAI規制を理解する必要があります。

　たとえば、2024年9月に米国連邦取引委員会（FTC）は既存法に基づいてAIを悪用した商行為に対して法執行を行った事例を紹介するプレスリリース「Operation AI Comply」を公表しました。

　このリリースでは、製品、サービスの説明、広告などにおいて消費者を誤解させるようなAIの利活用行為に対してFTCが訴訟提起した事例を紹介しています。

　FTCはまた、2025年1月に公開した「AIと消費者被害のリスク（AI and the Risk of Consumer Harm）」という記事において、AIの安全性や差別の禁止、プライバシー保護、サイバーセキュリティの重要性について解説し、企業が消費者にAIシステムを利活用させる前、利活用中、利活用後の全ライフサイクルにおいてそのリスク防止に努める必要があると主張しており、FTC法によるAI規制を強調しています。

　州レベルでは、2025年1月13日に、カリフォルニア州司法長官がAI規制になり得る可能性のある既存法令に関して非常に参照価値のあるアドバイザリーを公開しました。このアドバイザリーは、2025年1月1日からカリフォルニア州で発効するAI規制を紹介した上で、米国の連邦レベルのAI規制となる医療関連法令、FTC法（米国連邦取引委員会法）などについて言及しています。

　このように、米国においてAI規制は既存法に依存している部分が大きく、違反した場合には訴訟提起され高額な損害賠償金ないし和解金の支払いに直面するため、その代償も看過できません。
【中国】AI規制の現状：アルゴリズムの種類別の要件も
中国はAIに関する包括的な規制法を制定する予定ですが、現状ではAI産業発展を促進する政策と同時に、既存法令や個別のアルゴリズム規制、AI倫理規制、標準制定などを組み合わせる形でAI規制を行っています。

　政策として2017年7月に国務院（中央政府）が公表した「新一代人工知能発展規画」によれば、2025年中にAIに関する基本的な法律、倫理枠組みの形成とAIに対する安全評価と管理能力を構築することを目標としています。中国のAI規制の全体像は、下図で示す5つのカテゴリーで構成されています。


　AI規制について中国は、2022年1月に「インターネット情報サービスアルゴリズム・レコメンデーション管理規定」、2022年12月に「インターネット情報サービス深度合成アルゴリズム管理規定」を、2023年４月には生成AI向けに「生成AIサービス管理暫行弁法」を相次いで制定しました。

　これらのAI規制は、中国向けに上記のAIシステム・サービスを提供する場合に適用されます。主な特徴として、ブログ、ミニブログ、チャットルーム、チャットグループ、パブリックアカウント、ショート動画、ライブなど情報サービス機能を備える場合には届け出をしなければいけません。

　届け出にあたっては、AIシステム・サービスの開始10日以内に国家インターネット情報弁公室に対して、プロバイダーの情報、アルゴリズムの紹介、アルゴリズムのリスクアセスメント結果などをWEBフォーム経由で提出します。届け出に関するガイドラインも公開されています。手続きが完了すると当局のWEBで公開され、誰でも閲覧できるようになります。

　当局が公表した2022年から2024年の間の届け出済みリストによれば、国内の外資企業を含めてすでに3000件を超えるアルゴリズム、200件を超える生成AIの届け出がありました。届け出る内容についてリスクアセスメントを行う必要があり、アルゴリズムが悪用されるリスク、脆弱性評価、望ましくない情報の拡散を助長するリスク、データないし個人情報漏えいリスク、その他関連法令への適合性評価などが該当します。

　中国のAI規制には、共通する一般的なコンプライアンス要件の他に、アルゴリズムの種類別に透明性要件など追加の要件が設けられています。透明性については、使用するアルゴリズムやAIが生成したアウトプットであることを、分かりやすく表示することなどが求められます。


　2023年8月に「科技倫理審査弁法（試行）」が施行され、AIの研究開発を含む生物学、医学など科学研究開発を行う大学、企業などの組織に対して、倫理委員会の設立、研究開発活動の設計に倫理的観点をあらかじめ組み込む「エシックス・バイ・デザイン（Ethics by Design）」を求めています。

　倫理委員会の組織的・経済的独立性を担保する、倫理委員会メンバーは特定分野の専門家以外に社会倫理学、法学、社会学などから性別、民族などを多様に構成するなどの要件が含まれています。

　また、AIなどのハイリスクの科学研究活動については専門家の審査が必要であり、審査状況について毎年報告書を届け出ることなども求められています。

　中国におけるAI規制に関する既存法としては、上記以外にも中国データ3法（サイバーセキュリティ法、データセキュリティ法、個人情報保護法）があり、これらの要件にも注意する必要があります。ペナルティーについては、主に中国データ3法によるため、高額になる可能性があります。
【英国】イノベーション優先の方針、「AI規制法案」を審議中
その他の国におけるAI規制の動向についても、簡単に紹介していきます。

　英国は、AI技術開発、イノベーションを優先した規制枠組を構築しています。

　2023年3月に英国政府は、AI技術の発展を優先することを前提に、AIに関して次の5つの原則のもと、個人情報保護法（UK GDPR）、2018年データ保護法、英国オンライン安全法（Online Safety Act）、2010年平等法、消費者保護法などの既存法令との組み合わせで規制することを示唆する「AIイノベーション優先の規制アプローチ（A pro-innovation approach to AI regulation）」を公表しました。


「AIイノベーション優先の規制アプローチ」5つの原則

・安全性、セキュリティ、堅牢性
・適切な透明性と説明可能性
・公平性
・説明責任とガバナンス
・争議可能性と是正


　既存法においても、特に個人情報保護がより注目されており、英国の個人情報保護機関であるInformation Commissioner’s Office（ICO）は個人情報とAIとの関連でAIによる個人情報の乱用を防ぐためのガイドライン、ツールキットなどを公表しています。

　さらに、英国は2023年11月に上記AI規制に関する5原則を法制化する「AI規制法案（Artificial Intelligence（Regulation）Bill）」を提出し、審議しています。
【カナダ】AIとデータに関する法案「AIDA」が廃案に
カナダは、英国と同様にAI技術開発やイノベーションを優先とする「責任あるAIフレームワーク」の構築を宣言しています。AI規制については、既存法令である消費者製品安全法、食品医薬品法、自動車安全法、銀行法、カナダ人権法および各州人権法などによる規制を強調しています。

　一方、2022年6月に提出された「AIとデータに関する法律（The Artificial Intelligence and Data Act（AIDA））」案は、欧州同様に、雇用、個人へのサービス、生体認証、医療、司法など7つの分野で利活用されるAIシステムについて「高い影響のあるAI」として分類し、厳格なコンプライアンス要件と重い罰則を規定していました。

　しかし、AIDAを含む包括的な法案「Bill C-27」は、2025年1月のカナダ議会閉会により廃案となり、正式に審議は終了しています。

　AIDAでは、「高い影響のあるAI」に該当する場合、違反行為に対して、法人へのペナルティーは最高27億円強（執筆時点の為替レート）あるいはグループ全体の前年度売上の5％となっていましたが、現在この規制枠組みは存在していません。現時点では、連邦レベルで拘束力のあるAI規制法は未整備であり、政府は自主行動規範や研究機関の設立など、ソフトローによる対応を進めています。
【ブラジル】「過度なリスクのあるAI」は開発自体を禁止
2024年末に、ブラジル国会上院は包括的AI規制法案を可決し、下院での審議に送付しました。審議中のブラジルAI法規制の大原則は、人間中心、人権と民主主義の尊重、人格の自由、サステナビリティ、技術イノベーションの促進、自由競争、個人情報保護、情報提供などです。

　欧州ＡI法同様にリスクベースの規制アプローチを採用し、自動運転、人の安全や健康に害がある場合、教育、雇用、司法、生体認証など特定分野におけるAIシステムをハイリスクとして特に規制しています。

　一方、欧州AI法がAIを悪用する行為を禁じていることに対して、ブラジルは「過度なリスクのあるAI」に分類されるAIそのものの開発を禁止すると明確に規定しています。法案は、AIに関するリスクアセスメント、重要インシデントの報告、既存の個人情報保護法などに対する遵守義務についても強調しています。

　法令違反行為については、1件につき最高13億円強（執筆時点の為替レート）まで、法人の場合グループ全体の前年度売上の2％までのペナルティーを科しています。
【韓国】欧州に続く2番目の包括的なAI法「AI基本法」とは？
2024年12月26日に韓国の包括的なAI法「AI発展および信頼の確立に関する法律（AI基本法）（Basic Act on the Development of Artificial Intelligence and the Establishment of Trust）」が成立しました。

　韓国AI基本法は、欧州に続く2番目の包括的なAI法となります。主な内容は、AI産業を発展させるための国家制度の構築、産業政策の基礎作りを規定した上で、安全性、信頼性、アクセシビリティ、人間中心などの倫理原則を設けています。

　「高い影響のあるAI」という分類があり、該当する場合に透明性の確保に努める必要があります。透明性の要件として、ユーザーに対してAIが使用されていることを事前通知する、AIによるアウトプットであることを明示するなどを義務付けています。

　エネルギー、水道、医療、司法、生体認証、雇用、交通、教育などの分野については、欧州AI法と同様に高い影響のあるAIとして規定しており、該当する場合はAIのライフサイクルにおいてそのリスクを評価する必要があります。

　韓国にAIシステム・サービスを展開しているが拠点を持たない事業者は、同国内に代表者を置かなければなりません。いくつかの特定の違反行為に対して300万円強（本稿執筆現在の為替レート）の罰金が科せられており、他の国地域に比べてかなり低い金額となっています。

　AIの個人情報保護法への準拠方法について、2024年12月に韓国個人情報保護委員会が「安全なAI・データ活用のためのAIプライバシーリスク管理フレームワーク」を公開しており、AIに関する個人情報リスク管理制度の構築、個人情報リスクの分類およびリスク緩和措置などについて提言しています。
【日本】成立間近の「AI法案」のポイントは？背景も解説
日本政府はAI規制を検討するためにAI戦略会議内にAI制度研究会を設置し、AI規制と技術革新の両立を目指し、また既存の法令やガイドラインなどを生かしながら、主に政府調達における制度や仕組み、医療機器、自動運転など既存の業法の見直し、広島AIプロセスに基づくAI規制枠組の構築と政府の司令塔機能の強化など、具体的な対応の必要性を検討してきました。

　その結果として「AI関連技術の研究開発・活用推進法案（AI法案）」をまとめ、国会に提出し、6月22日までの今国会で成立する見通しです。この法案は、AIの開発者、提供者、利用者に対し、リスクに応じた対応を求める内容となっており、AIの利活用を促進しつつ適切な規律を図ることを目的としています。そのため、技術革新を妨げないために事業者への罰則は見送られています。

　一方、上記AI法案に先立ち、日本政府は2017年の総務省「国際的な議論のためのAI開発ガイドライン案」、2019年の「AI利活用ガイドライン」や、2022年の経済産業省「AI原則実践のためのガバナンス・ガイドライン Ver. 1.1」などを整備してきました。

　2023年5月のG7広島サミットにおける「広島AIプロセス包括的政策枠組み」のような国際的コンセンサスの取りまとめを経て、2024年4月に経済産業省と総務省はAIの開発者、提供者、利用者それぞれを対象に「AI事業者ガイドライン（第1.0版）」を策定しました。

　AI事業者ガイドラインは、「人間中心」「安全性」「公平性」「プライバシー保護」「セキュリティ確保」「透明性」「アカウンタビリティ」「教育・リテラシー」「公正競争確保」「イノベーション」の10原則に基づいています。リスクを抑え適切に利用するためのガードレール的な内容となっており、グローバルの動向と一致しています。


「AI事業者ガイドライン」の共通指針

・人間中心
・安全性
・公平性
・プライバシー保護
・セキュリティ確保
・透明性
・アカウンタビリティ
・教育・リテラシー
・公正競争確保
・イノベーション


　AI制度研究会においても頻繁に言及されている通り、個人情報保護法など既存の法令によるAI規制は可能であり、その法執行の強化は考えられます。一方、各業法への依存のみならず、AI産業の発展に合わせて柔軟に運用可能な包括的なベースラインとなる法制度の構築が必要になります。
企業に求められる対応
AI規制は複雑化しており、企業は、AIの開発や利活用において多層的な法令準拠を実施する必要があります。

　一方、上記の通りグローバル各国におけるAI規制への対応策は、「信頼できるAIのための倫理ガイドライン」や「広島AIプロセス包括的政策枠組み」のような、国際的コンセンサスでもある信頼できるAIのための諸原則に沿ってコンプライアンス枠組を構築した上で、自社AIに対して規制となる各国の既存法を確認・準拠するという二重構造とする必要があります。

　既存法については、安全コンポーネントや人事労務などAIを利活用する分野や場面によってコンプライアンス要件の差異が生じるため、自社AIに適用される分野別既存法に基づく詳細な社内制度、規定などを通じたコンプライアンス対応を考えなくてはいけません。

　特に、個人情報を含むデータの利活用を前提としている場合、各国の個人情報保護法への準拠が必須です。AIシステムのサイバーセキュリティについては、たとえば欧州サイバーレジリエンス法など製品セキュリティ法規制などに準拠する必要もあります。

　AIにまつわるリスクは「技術リスク」にとどまることなく、「法律リスク」や「倫理リスク」と広範にわたるため、複雑で多層的なAI規制に対するコンプライアンス要件の実施など、これらのリスクへの対応状況について設計、実装、運用にわたって入念に検証する必要があります。
執筆：PwCコンサルティング マネージャー／法学博士／外国法事務弁護士 エレドン・ビリゲ",[],[]
