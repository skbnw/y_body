headline,mainEntityOfPage,image,datePublished,dateModified,author,media_en,media_jp,str_count,body,images,external_links
STNet、遠隔GPUクラスターの実証実験--液冷設備の運用課題も評価（ZDNET Japan）,https://news.yahoo.co.jp/articles/9111b21e364f9fa16ad5f8715994f7205d00fcc4,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20251104-35240032-zdnet-000-1-view.jpg?exp=10800,2025-11-04T17:58:00+09:00,2025-11-04T17:58:00+09:00,ZDNET Japan,zdnet,ZDNET Japan,601,"STNet、遠隔GPUクラスターの実証実験--液冷設備の運用課題も評価の画像
四国電力グループのSTNetは11月4日、離れた場所に設置されたGPUサーバーを高速・低遅延な通信ネットワークで連係した「遠隔GPUクラスター」などの実証実験を実施すると発表した。2026年度上期中に開始する予定だ。

　今回の実証実験は、データセンター事業の高度化に向けて、分散配置されたGPUサーバーを高速・低遅延ネットワークで連係し、巨大な計算資源として活用する技術の検証を目的に実施するもの。生成AIや高度なシミュレーションの普及により、高速・大容量な計算基盤のニーズが急増する中、GPUの高性能化に伴い、電力消費・発熱量も増加しており、冷却技術の進化が求められている。

　実証実験では、2拠点のGPUサーバーを高速・低遅延ネットワークで接続し、分散した計算資源を統合。1つの大規模なコンピューティングリソースとして活用するほか、ソフトウェア開発企業などと連携し、性能・使い勝手・運用課題を評価する。

　GPUサーバーの運用では、発熱量の多いGPUサーバーの効率的な運用も検証していく。液冷設備の運用課題を評価し、2026年度下期の本格対応を視野に入れる。

　今後は、今回の実証実験の結果を通じて得られた「遠隔GPUクラスター」や「液冷設備」の構築・運用ノウハウの知見を生かし、各種サービスの検討を進めていく。",[],[]
誤情報・偽情報対策への企業支出が300億ドル超へ--ガートナーが発表（ZDNET Japan）,https://news.yahoo.co.jp/articles/cacdee1b9ef4b563d8eb67853e22a3bdd3376060,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20251104-35240030-zdnet-000-1-view.jpg?exp=10800,2025-11-04T17:20:00+09:00,2025-11-04T17:20:00+09:00,ZDNET Japan,zdnet,ZDNET Japan,1095,"誤情報・偽情報対策への企業支出が300億ドル超へ--ガートナーが発表の画像
Gartnerは10月30日、2028年までに誤情報／偽情報対策への企業支出が300億ドルを超えるとの展望を発表した。マーケティング／サイバーセキュリティ予算の10％が多面的な脅威に対応するために費やされるようになるという。

　これは、Gartnerの新刊「World Without Truth（真実なき世界）」において明らかにされたもの。共著者であるハイテク・リーダー／プロバイダーのプラクティスを担当するディスティングイッシュト バイス プレジデント アナリスト 兼 GartnerフェローのDave Aron氏、マーケティング・リーダーのプラクティスを担当するディスティングイッシュト バイス プレジデント アナリストのAndrew Frank氏、元GartnerアナリストのRichard Hunter氏は、虚偽の情報が組織に重大な財務リスクやレピュテーション（評判）リスクをもたらすと説明している。

　2024年12月にGartnerがビジネスおよびテクノロジーの上級経営幹部200人を対象に実施した調査によると、回答者の72％が、誤情報、偽情報、悪意ある情報を、経営委員会にとって「極めて重要な課題」または「優先度の高い課題」として認識していることが明らかになった。その一方で、これらを最重要課題の上位5つに位置付けた割合は30％にとどまっている。

　Aron氏は「人間社会の歴史の中で、虚偽の情報は文化の一部であり続けてきた。しかし、3つの異なる現象が重なり、その力と危険性が大きく増幅されている」とし、（1）インターネット、SNS、モバイルアプリの普及によって、コストをほとんどかけずにマルチメディアコンテンツが多くの人々に瞬時に届くようになった。これらのツールは、受け取る個人や顧客／市民グループごとに、異なるメッセージを届けられる、（2）生成AIの進化により、ますます説得力のある会話の作成だけでなく、ディープフェイクの画像／音声／動画コンテンツを簡単に作成できるようになっている、（3）行動科学、ビッグデータ、アナリティクス、AIを組み合わせることで、思考モデル、意思決定、行動の変化を目的とした、個人にとって最も説得力のあるメッセージの作成が可能になる――と偽情報の拡大を促進させる3つの要因を説明している。

　Gartnerは、偽情報に対抗するために、ルール／ガバナンス／プロセス、教育、ナッジ／インセンティブ、テクノロジー／ツールという4つの手段を活用することを推進している。",[],[]
LPI-Japan、Linux技術者認定「LinuC」の上位認定を刷新（ZDNET Japan）,https://news.yahoo.co.jp/articles/425d175ee9dbf15722d52197d4a07c0913dc24d9,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20251104-35240027-zdnet-000-1-view.jpg?exp=10800,2025-11-04T16:54:00+09:00,2025-11-04T16:54:00+09:00,ZDNET Japan,zdnet,ZDNET Japan,1611,"LPI-Japan、Linux技術者認定「LinuC」の上位認定を刷新の画像
IT技術者の認定試験を実施するエルピーアイジャパン（LPI-Japan）は11月4日、Linux技術者認定「LinuC」の上位認定を刷新し、新認定「LinuC レベル 3 プラットフォームスペシャリスト」と「LinuC レベル 3 セキュリティスペシャリスト」の提供を開始すると発表した。生成AI・デジタルトランスフォーメーション（DX）時代に対応するIT技術者の成長と活躍を支援する。

　新認定となるLinuC レベル 3 プラットフォームスペシャリストとLinuC レベル 3 セキュリティスペシャリストは、現行のスペシャリスト認定である「LinuCレベル 3 300試験（Mixed Environment）」「303試験（Security）」「304試験（Virtualization & High Availability）」を再構成し、エンタープライズレベルのシステム構築に不可欠な専門技術力を証明する2つの新認定としてリリースするもの。現行のレベル3試験は、新試験と1年間並行配信し、2026年11月30日に配信を終了する予定だ。

　あわせて、最上位認定となる「LinuCシステムアーキテクト」も「LinuCレベル4 システムアーキテクト」に改称する。

　LPI-Japan 理事長の鈴木敦夫氏は「レベル1（操作・運用）からレベル4（システム全体のアーキテクチャ設計）まで、技術者が段階的にキャリアを形成できる一貫した認定体系が完成した」と話す。

　新たな体系に刷新した背景には、上位技術者の不足がある。LPI-Japanが2019年にヒアリング調査を実施したところ、「現場で上位技術者が育っていない、問題解決や技術議論ができる人がいない、便利な機能だけ使い仕組みを理解せず解決できない」といった問題が浮かび上がってきたという。

　鈴木氏は「技術者が知るべき基礎知識を体系化し、応用につなげるための中核的な技術としてLinuCをリニューアルした。昨今、生成AIが急速に普及しているが、生成AIは知能ではなく、膨大な学習データから確率的に回答を生成するもの。最終的な責任は人間が負うため、AIをコントロールする力が技術者には必要になる。一方で、生成AIの便利さにより、思考停止やゼロから考える力の劣化を招くリスクがある」と基礎技術の重要性を訴えた。

　新認定は、スペシャリストとしての立ち位置を維持しつつ、個別技術の掘り下げに偏らないように範囲を定義し作成した。システム全体を見通す力と領域横断を重視し、試験問題はトラブル解決など、実践的な内容を盛り込んでいるという。

　試験問題の作成に当たっては、30人の技術者が協力。LPI-Japan ITエキスパート・試験開発責任者の安良岡直希氏は「クラウド、仮想化、コンテナーといった基盤技術の一般化や、システムの複雑化・アタックサーフェスの拡大による多層的な防御の必要性など技術動向の変化を盛り込んでいる」と設計で考慮した点を挙げる。

　試験は、12月1日に配信され、受験料は2万7500円（税込）。テストセンターまたはオンライン受験の選択ができ、団体受験用にペーパーテストも用意する。問題はマウスを使った選択方式がほとんどだが、記述方式の問題も多少出題される場合があるとのこと。実技や面接は実施しない。設問数は約60問で、試験時間は90分になる。

　2025年12月1日から2026月5月31日までの期間中であれば、対象試験が不合格の際に一回に限り無料で受験できるキャンペーンも実施する。

　LPI-Japanでは、今後LinuC レベル 3認定者を年間5000人輩出することを目標に掲げ、学習環境の整備と上級技術者への学習意欲向上に向けた取り組みを強化していく。",[],[]
アカマイ、インターネットエッジのGPUでAIワークロードの遅延を軽減（ZDNET Japan）,https://news.yahoo.co.jp/articles/db0e648af1171da8542ed8e7563d0e27d2a988b2,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20251104-35240025-zdnet-000-1-view.jpg?exp=10800,2025-11-04T16:15:00+09:00,2025-11-04T16:15:00+09:00,ZDNET Japan,zdnet,ZDNET Japan,1814,"アカマイ・テクノロジーズ プリンシパルメジャーアカウントエグゼクティブ Davy Chen氏
アカマイ・テクノロジーズは10月31日、「Akamai Inference Cloud」を発表した。これは、米国で10月28日に発表されたもの。従来、コアデータセンターで実行されていたAIワークロードが、よりユーザーに近い同社の分散型ネットワークで実行できるという。

　同社は、祖業であるCDN事業のために整備したグローバルなPoP（Point of Presence）ネットワークに加え、2022年にLinodeを買収して同社のデータセンターネットワークを獲得し、クラウドコンピューティング基盤として活用している。今後GPUの配備が拡大していけば、同社の分散基盤を活用して実行されるユーザーのワークロードでGPUが活用できるようになる、というイメージだ。

　概要説明を行った同社のプリンシパルメジャーアカウントエグゼクティブのDavy Chen（デイビー・チェン）氏は、Akamai Inference Cloudの目的を「AIの推論ワークロードを、従来のコアのデータセンターからユーザー直近の『エッジ』へ移行させ、AIの応答遅延を最小化」することだと説明。エージェント型AIや、自動運転車やドローンなどのフィジカルAIに不可欠となるリアルタイム性や低遅延でのAI応答がグローバルで実現できるようになるとした。

　なお、実際にGPUが配備されたのはグローバル20拠点で、今後ユーザーニーズに応じる形で順次拡大予定となっている。日本ではまず東京のデータセンターで2025年10月からテストが開始されている。

　同社は以前からエッジネットワークへのGPUの配備に取り組んでおり、日本では大阪リージョンにおいて「NVIDIA RTX 4000 Ada」が2024年12月から1時間当たり80円で利用可能となっており、主要なワークロードとしては「生成AI、3Dレンダリング、マルチメディア」が想定されている。

　一方、Akamai Inference Cloudとして新たに東京リージョンに配備されたのは最新世代GPUである「NVIDIA RTX PRO 6000 Blackwell Server Edition」で、こちらはNVIDIA自身が推論用途を強く意識して製品化したGPUとなる。主な用途として「LLMを含むAI推論、ファインチューニング、科学計算、マルチメディア」となり、前世代に当たる「L40S」との比較では推論性能が最大6倍に向上している点が特徴となる。

　同社はAkamai Inference Cloudアーキテクチャの進化のプロセスとして、「Phase ONE」から「Phase FOUR」の4段階を構想している。Phase ONEは「GPUをグローバルに分散し、計算処理をユーザーの近くへ配置」で、今回の発表でこの取り組みが開始された形だ。次のPhase TWOでは「コストと性能に応じて最適なモデルとロケーションへリクエストをルーティング」することが考えられている。こちらは、GPU配備拠点の数が十分に増えてきた段階で現実的になってくるだろう。

　さらにPhase THREEでは「マルチステップAIエージェントのワークフローを並列処理で高速化」し、Phase FOURで「ステートとコンテキストを活用しリクエストを最適化」することを目指す。ユーザーに近いエッジ拠点でGPUが活用できるようになる、というシンプルなメリットから始まり、最終的にはグローバルに展開する分散ネットワークのメリットを最大限に活用できる形でよりインテリジェントなワークロードの分散や並列化の機能を実現していく構想だ。

　もともとCDN事業のために、比較的小規模なPoPをグローバルに4200拠点以上展開していることが同社のインフラの特徴で、大規模拠点中心の主要クラウド事業者のネットワーク展開とは異なる発想で構築されていることから、レスポンスを高めたいAIワークロードの実行プラットフォームとして注目される。

　現時点での初期配置は大規模なデータセンターを中心とした20拠点に限られており、まだ大規模な分散環境とは言えない状況ではあるが、今後配備拠点が拡大していくことで独自の魅力を発揮できるようになることが期待される。",[],[]
選択肢の広さが価値を生む--ニュータニックスの差別化戦略（ZDNET Japan）,https://news.yahoo.co.jp/articles/b5cf1644a4f646cbb3aab5da00ca6dffb526b82d,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20251104-35240017-zdnet-000-1-view.jpg?exp=10800,2025-11-04T15:40:00+09:00,2025-11-04T15:40:00+09:00,ZDNET Japan,zdnet,ZDNET Japan,3541,"Nutanix プロダクト＆ソリューションマーケティング担当シニアバイスプレジデント Lee Caswell氏
ニュータニックス・ジャパンは、先頃開催した記者説明会において、インフラ市場の変化と、それに伴うハイブリッドクラウドやAI導入の課題に対する同社の戦略を説明した。米Nutanixのプロダクト＆ソリューションマーケティング担当シニアバイスプレジデントであるLee Caswell氏に、VMwareの動向を踏まえた顧客への提供価値、そしてAI時代におけるハイパーコンバージドインフラストラクチャー（HCI）の役割について聞いた。

企業が直面するAI導入の課題

　Caswell氏は、IT市場のダイナミクスについて言及し、現在の最高情報責任者（CIO）が価格の上昇やバンドル戦略、サポートパスの変更といった市場の変化に対応するため、今後5～10年のIT戦略をどのように構築すべきかという課題に直面していると指摘した。

　これに対し同社では、「リスクとコストの最小化」「コンテナーや『Kubernetes』を含むモダンアプリケーションの拡張」「サイバーセキュリティへの対応」という、現代のIT環境における主要な要求に対し、ハイブリッドクラウド全体で共通のオペレーティングモデルを提供することで貢献すると述べている。

　Gartnerの「インフラ・テクノロジのハイプ・サイクル：2024年」によると、HCIは既に市場に浸透し、生産性の安定期にあるという。他方、黎明（れいめい）期にあるのは「分散型ハイブリッドインフラストラクチャー」である。これは、仮想プライベートデータセンターからエッジ、そしてパブリッククラウドへと分散された共通の運用モデルを持つための方法であり、このハイブリッドモデルでは従来の仮想マシン（VM）に加えて、コンテナーのサポートが非常に重要になるという。

　同氏は「コンテナーはVMを補完するものだ」との見解を示す。VMがインフラのレジリエンスを提供する一方で、コンテナーは開発者に柔軟性と俊敏性を提供する。VMとコンテナーの融合により、顧客は従来のVM環境を維持しつつ、コンテナーを活用したモダンアプリケーションを容易に導入できるようになると説明した。

　さらに、VMwareの動向に関しては、「コストに関する懸念からVMwareのエンタープライズ顧客の70％が、2028年までに仮想ワークロードの50％を移行することを検討している」というGartnerのデータを示し、これは「仮想化の導入以来、エンタープライズITで見られた最大の変化」であると述べた。

　Nutanixは、これらの顧客にとって、現在のワークロードのリスクを低減し、コンテナー、ハイブリッドクラウド、そして生成AIといった将来のワークロードに備えるための最適な移行先が同社の統合プラットフォームであると強調した。そのビジョンと投資は、「インフラストラクチャーのモダナイズ」「アプリケーションのモダナイズ」「生成AIの実現」という3つの主要な指針に沿って特徴付けている。

　Caswell氏は、企業がAI活用を進める中で「既存のチームに新しいテクノロジーを追加する必要があることに気付き、それがストレスを生み出している」と説明する。例えば、多くの企業はデータベースを使うために必要なCPUやメモリー、ネットワーキングなどのサーバーのサイジングには精通しているが、AIアプリケーションを使うために必要なGPUのサイジング方法にはなじみがないのが実情であるという。

　さらに、新しい大規模言語モデル（LLM）を使うには、既存のGPUメモリー内で確実に実行されるように工夫する必要があり、加えて、企業が対処しようとしている問題をそのLLMが本当に解決できるかを見極める必要がある。

　そこでNutanixは、AIを導入したい企業がスムーズにLLMを使えるように支援している。具体的には、同社が認定しているLLMを動かすために必要なGPUの性能や容量が最初から準備されており、問題なく動くことを確認している。そのために、同社はハードウェアパートナーと協力して、GPUがサーバーパートナーのハードウェアで正しく使えるように認定を受けたり、LLMを提供する企業との連携を通して、顧客が単一のオペレーティングモデルでLLMを導入・運用できたりするようにしている。

　ほかにも、企業はオンプレミス環境でコンテナーを実行する方法についても課題を抱えているという。同社は、DevOpsやプラットフォームエンジニアが、コンテナーのエンタープライズアプリケーションを支援できるようにすることで、VMとコンテナーを単一のプラットフォームで管理できるようにしている。

　また、多くのAIアプリケーションは、機密性の高いデータを使うため、企業はその取り扱いに対してもストレスを感じているという。そこで、NutanixはAIの処理をデータの近くで行うことで、データの移動を減らし、処理を簡素化することに注力している。この取り組みの一環として、統合型クラウドコンソール「Nutanix Central」を用いて、顧客の安全なAI運用を支援している。これにより、顧客はソブリンクラウドを導入でき、エッジからコアデータセンター、そしてパブリッククラウドに至るまで、共通の方法でデータを保護できるようになる。
HCIが提供する価値と差別化

　NutanixはGPUの可用性とコスト面でも貢献している。GPUを搭載した複数のサーバーをまとめて使えるようにすることで、社内の異なるチーム間でAIアプリケーションを試用・導入する際に、GPUを効率よく共有できる環境を提供している。

　オンプレミス環境でのAI開発はまだ初期段階にあるが、同社は、導入時のリスクや懸念点、費用の問題を減らすことで、顧客のAI導入への障壁を軽減し、AIを開始できるよう支援できると考えている。

　HCIの強みについて、「これらの新しいハードウェア技術を非常に迅速にサーバーに取り入れられる点だ」と述べる。HCIはクラウドのような柔軟性と拡張性を持つインフラを、サーバー上に構築しているため、最新のハードウェアテクノロジーをより迅速にユーザーに提供できる。

　また、AIはHCIの「ブロックストレージ」から恩恵を受けてきたが、今後は高性能なAI処理のために高帯域幅のスループットに応答する「ファイルストレージ」や「オブジェクトストレージ」からも恩恵を受けることができるようになる。

　Nutanixは、ブロックストレージ、ファイルストレージ、オブジェクトストレージに対して統一された管理方法を提供しており、「スナップショット」「レプリケーション」「災害復旧（DR）」などにも一貫して対応している。

　例えば、NVIDIAのAIデータプラットフォームの事例では、「Nutanix Unified Storage」（NUS）と呼ばれるソフトウェア定義ストレージが活用されており、AIを成功させるためには、データを整理して処理できる環境が、データ戦略の基盤として非常に重要な出発点になっているという。

　さらに、AIアプリケーションが成長する中で、HCIの主要な価値提案の一つである、「最初からやり直すことなく、クラスターにノードを追加することでスケールアップできる簡素な仕組み」が、AIの進化に非常に適していると強調した。

　Nutanixが常に目指してきた価値の1つは「顧客の選択肢」であり、同氏は「お客さまがVMやコンテナー、ファイル、ブロック、オブジェクト、エッジ、コア、そして仮想プライベートデータセンターといった選択肢を持つことは、極めて重要だと私たちは考えている」と話す。

　選択肢は、サーバーからクラウド、VM、コンテナーへと移行している。最近の拡張は、HCIまたはDellやPure Storageなどの認定パートナーからの外部ストレージの選択肢がある。これにより、顧客がインフラストラクチャーのより広範な部分でNutanixの価値を利用できる機会がさらに増えることになる。

　Caswell氏は、Nutanixと他社との差別化について、「単一のハイパースケーラーとは異なり、複数のクラウドをサポートすること、VMwareとは異なり外部ストレージまたはHCIの選択肢をサポートすること、そしてRed Hatとは異なりVMまたはコンテナーの選択肢をサポートすることで差別化を図り、価値を示している」と説明した。",[],[]
NTT、自動運転社会の実現を目指し、新会社「NTTモビリティ」設立（ZDNET Japan）,https://news.yahoo.co.jp/articles/32fef304f1ea56c4689e423c8ccee25670969417,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20251104-35240023-zdnet-000-1-view.jpg?exp=10800,2025-11-04T15:12:00+09:00,2025-11-04T15:12:00+09:00,ZDNET Japan,zdnet,ZDNET Japan,617,"NTT、自動運転社会の実現を目指し、新会社「NTTモビリティ」設立の画像
NTTは11月4日、自動運転社会の実現を目指し「NTTモビリティ株式会社」を設立すると発表した。安心・安全でサステナブルな自動運転の仕組みを確立する。

　運転手不足が社会問題になる中、特定条件の下での自動運転レベル4における公道走行が可能となり、自動運転移動サービスについては、2025年度に約50カ所、2027年度までに100カ所以上の地域での実現が政府目標に掲げられている。

　しかし、安心・安全な自動運転車両の提供体制の確立や標準化されたオペレーションの実現、日本の道路環境に適応するインフラ環境の整備など、解決すべき課題も多い。

　NTTモビリティは、NTTグループ各社が推進してきた全国各地での自動運転サービスの導入・運用を、包括的に支援するワンストップサービスを提供する。NTTグループが強みを持つ通信ネットワークサービスなどの活用を通じて、自動運転レベル4に求められる要件・課題を解決するソリューションや関連サービスも展開していく。

　設立日は12月15日を予定。運行支援システムの共通化、業務プロセスの標準化・効率化などを推進し、全国での自動運転ワンストップサービス提供に向けた体制を整備するほか、2027年度までに、遠隔監視システムやインフラ協調システムなどをはじめとするNTTグループの強みを生かした自動運転サービスの提供を目指す。",[],[]
札幌市消防局、救急隊向けアプリで搬送業務をデジタル化--「iPad」とローコード開発ツールを活用（ZDNET Japan）,https://news.yahoo.co.jp/articles/8d3feb6cdcc529804492ca8c836bd2790288f4fb,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20251104-35240019-zdnet-000-1-view.jpg?exp=10800,2025-11-04T14:55:00+09:00,2025-11-04T14:55:00+09:00,ZDNET Japan,zdnet,ZDNET Japan,1084,"札幌市消防局、救急隊向けアプリで搬送業務をデジタル化--「iPad」とローコード開発ツールを活用の画像
札幌市消防局は、TXP Medicalが開発した救急隊向けアプリ「NSER mobile」を導入した。このアプリは、Claris Internationalが提供するローコード開発プラットフォーム「Claris FileMaker」を基盤に構築されており、Clarisが11月4日に発表した。

　人口約200万人を抱え、年間1500万人以上の観光客が訪れる札幌市では、年間約12万件の救急出動がある。高齢化の進行に伴い、救急搬送の件数は年々増加しており、現場の隊員の負担が大きな課題となっていた。こうした状況を受け、札幌市消防局はTXP Medicalと連携し、救急現場で活用できるデジタルソリューションの導入を決定した。

　NSER mobileは、「iPad」や「iPhone」を使って患者情報の入力や受け入れ可否、搬送先の医療機関情報などをリアルタイムで共有できる。これまで電話と紙で行っていた医療機関とのやりとりをデジタル化することで、救急搬送のスピードと精度が大きく向上した。

　現在、札幌市内の36救急隊（約350人）と66の医療機関がNSER mobileを活用しているほか、札幌市周辺の恵庭市消防本部や石狩北部地区消防事務組合とも同じプラットフォームで連携している。

　従来は1件ずつ電話で医療機関に受け入れ確認をしていたが、アプリ導入後は複数の病院に一括で搬送要請を送信できるようになり、搬送時間の大幅な短縮につながっている。

　また、搬送時には外傷の写真を医療機関に送信できるようになり、衝突の衝撃方向など事故現場の状況も把握可能となった。これにより、医師は受け入れ前に損傷の程度を予測できるようになり、言葉では伝えきれなかった情報が画像で正確に伝わるようになった。医師からも高く評価されている。

　さらに、iPadで免許証・保険証・お薬手帳などを撮影するだけで、自動的に光学文字認識（OCR）処理され、文字データとして病院に送信されるため、受付準備もスムーズに行えるようになった。

　iPadとアプリの導入によって、救急隊員が報告書を作成する事務処理の時間が大幅に短縮され、休憩時間の確保や後輩の教育など、本来必要な業務に時間を充てられるようになった。

　札幌市だけでなく、周辺地域の消防本部や医療機関とも同じプラットフォーム上で情報を共有できる体制が整い、救急搬送の効率化と医療機関の受け入れ体制の強化が同時に実現している。",[],[]
日立、上期は増収増益--「Lumada 3.0」が成長を加速、「HMAX」でAIエコシステムを拡大（ZDNET Japan）,https://news.yahoo.co.jp/articles/b1638be4b57ff5b1fdbb64ea4322c77e44beac1e,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20251104-35240005-zdnet-000-2-view.jpg?exp=10800,2025-11-04T11:39:00+09:00,2025-11-04T14:04:20+09:00,ZDNET Japan,zdnet,ZDNET Japan,3582,"日立、上期は増収増益--「Lumada 3.0」が成長を加速、「HMAX」でAIエコシステムを拡大の画像
日立製作所が発表した2025年度上期（2025年4～9月）連結業績は、引き続き、同社の好調ぶりを示す内容となった。

　売上収益は前年同期比5.3％増の4兆7874億円、調整後営業利益は同25.5％増の5080億円、Adjusted EBITA（調整後営業利益－買収に伴う無形資産などの償却費）は同21.5％増の5618億円、税引前当期利益は同54.8％増の6801億円、当期純利益は同61.8％増の4728億円と、増収増益を達成した。

　力強い成長を支えたのが「Lumada」である。Lumada事業の第2四半期（2025年7～9月）の実績は、売上収益が前年同期比47％増の1兆550億円となり、全社売上収益に占める割合は42％に上昇。Adjusted EBITA率は12.8％となっている。

　Lumadaは、2025年度から構成項目を変更し、新たにマネージドサービス事業やプロダクト事業、AIを活用したシステムインテグレーター（SI）事業などを、Lumada事業の中に含めており、その点を考慮する必要があるが、力強い成長は継続している。

　Lumadaの内訳は、デジタルサービスの売上収益が前年同期比37.4％増の4670億円、デジタライズドアセットが同54.7％増の5880億円となっており、いずれも高い成長率を達成している。

　また、Lumada事業のセクター別売上収益を見てみると、Lumadaの推進役となるデジタルシステム＆サービス（DSS）では前年同期比44％増の4390億円、Lumada事業比率は62％に到達。一方で、エナジーは前年同期比147％増の1750億円、Lumada事業比率が23％。モビリティーは前年同期比52％増の1000億円、Lumada事業比率が33％。そして、コネクティブインダストリーズが同23％増の3410億円で、Lumada事業比率が42％となった。

　Lumadaの売上収益比率は、2027年度には、日立製作所全体で、約65％にまで高める計画だ。DSSで既にそれに近い水準にまで高まっているが、全社規模で見ても、2024年度第2四半期の31％から、2025年度第2四半期は42％へと着実に増加している。

Lumadaの拡大で重要な役割を果たす「HMAX」とは

　今後のLumadaの拡大において、重要な役割を果たすのが、「HMAX」である。

　日立製作所では、HMAXを「Lumada 3.0」を体現するソリューションに位置付けているほか、自らが最初の顧客となって自ら実証を行う「カスタマーゼロ」の取り組みも進めている。デジタルサービスの強化や、フィジカルAIの実装を加速するための戦略的ソリューションともいえる。

　HMAXは、NVIDIAとの協業によって生まれた「デジタルアセットマネジメントサービス」であり、2024年11月に第1弾として、鉄道分野向けソリューションを投入。鉄道にセンサーを搭載し、走行データを収集するとともに、天候データや、部品の摩耗状況などの情報を組み合わせて、最適な部品交換や保守要員の手配などを実現している。

　日立製作所 執行役専務 最高財務責任者（CFO）の加藤知巳氏は、「HMAXによる鉄道ソリューションは徐々に注文が増えている。モビリティー領域におけるサービス比率を高めることにもつながっており、収益率の改善にもつながっている」とした。

　さらに、HMAXはインダストリー領域にも展開。日立ビルシステムでは、NVIDIAのAI技術を活用し、ウェアラブルカメラ映像をリアルタイム解析して危険行為を検知、警告する「HMAX for Building : BuilMirai」を開発。エレベーターや電源装置の点検業務、広域災害発生時の技術者配置の支援などに活用しているほか、日立プラントコンストラクションでは、日立のAIエージェント「Frontline Coordinator - Naivy」を活用した「リスク危険予知支援システム」を開発。自らがカスタマーゼロとなり、現場の安全性や効率性の向上といった成果を実証している。今後、HMAXを支える主要技術として進化させ、Naivyを活用したアプリケーションの拡充も図っていくことになる。

　また、NVIDIAとの連携強化により、フィジカルAIソリューションを開発するための「NVIDIA AI Factory」を立ち上げているほか、日立レールが、NVIDIAのリアルタイムAI向けソリューション「IGX Thor」を、モビリティー企業として世界で初めて採用。HMAXを通じて顧客に提供することも発表している。IGX Thorは、AIコンピューティングの速度を最大8倍に、接続性を2倍にでき、リアルタイムのセンサー処理やAIの思考力向上、機能の安全性、企業に対する長期的なサポートを提供するという。

　HMAXのユースケースの拡大が着実に進んでいることが示される。
NVIDIA、Google Cloud、OpenAIらと手をくみ新ビジネスへ

　Lumada 3.0の取り組みにおける、もうひとつのポイントが、GlobalLogicの動向である。GlobalLogicの第2四半期の売上収益は、前年同期比2％増の5億300万ドルと、成長にはやや減速感がみられるが、「米国市場での投資抑制が続いているものの、稼働率の向上によって収益性を改善。各セクターとのシナジー創出も続いている」と説明した。

　実際、GlobalLogic単体の売上収益に、ほかのセクターとのシナジー創出の成果を加えると、第2四半期の売上収益は前年同期比17％増の10億5300万ドルと高い成長を遂げている。

　今後はAIによる高付加価値サービス事業を強化することで、前年度と同等水準の利益を維持する見通しを示した。

　GlobalLogicでは、新たに独synvertの買収を発表している。

　データアドバイザリーやデータ基盤の設計および構築の強みを生かすことができ、データバリューチェーンのケイパビリティーを強化できると説明。エージェンティックAIや、フィジカルAIのソリューション開発を強化するとともに、HMAXの展開にも貢献できるという。

　「GlobalLogicの重点施策は、AIサービスを提供するための技術力を強化し、収益性を高めていくことである。独synvertの買収によって、GlobalLogicのソリューション提供力を引き上げることができる」とした。

　今回の決算発表の中で、Lumada事業拡大に向けた施策として強調したのが、パートナーとの連携強化によるAIエコシステムの拡大だ。

　先に触れたNVIDIA AI Factoryの立ち上げをはじめとしたNVIDIAとの連携強化のほかにも、Google Cloudとは、2025年10月に発表した「Gemini Enterprise」を、制御・運用技術（OT）領域に適用し、AIエージェントのノーコード開発によるフロントラインワーカー業務の変革に着手することを発表している。具体的には、日立パワーソリューションズが行う受変電設備の保守作業において、目視で判断していた現場の映像をAIに学習させてエージェント化。フロントラインワーカーの作業ミスをなくし、生産性向上に貢献できるという。今後、HMAXのソリューションとして展開する予定だ。

　さらに、OpenAIとは、次世代AIインフラの構築や、OpenAI技術の活用に関する戦略的パートナーシップを結んだ。

　「OpenAIとの次世代AIインフラの構築では、送電を中心に、データセンターへの電力の効率的な供給において貢献。冷却技術やストレージ技術でも貢献ができる。また、LumadaソリューションやHMAXにおいて、価値提供のレベルを高めるための協業が可能になる」（日立製作所 インベスターリション本部長の玉井信一郎氏）とした。

　2025年4月に、日立製作所 代表執行役 執行役社長 兼 代表経営責任者（CEO）の徳（漢字は旧字）永俊昭氏が、新経営計画「Inspire 2027」を発表し、その中核となる「Lumada 3.0」を打ち出してから、約半年を経過した。

Lumada 3.0を取り巻く環境が整備され、Lumada 3.0を体現するソリューションとするHMAXによる成果も着実に生まれている。

　Lumada 3.0の着実な広がりを印象付けた2025年度上期決算だったといえる。",[],[]
インフォマティカのAPJ担当が説く、AI時代に備えたデータ品質の高め方（ZDNET Japan）,https://news.yahoo.co.jp/articles/c5d8414910989f8383c9a678da9fd396f7f66b1c,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20251104-35239958-zdnet-000-1-view.jpg?exp=10800,2025-11-04T10:40:00+09:00,2025-11-04T10:40:00+09:00,ZDNET Japan,zdnet,ZDNET Japan,3852,"Informatica アジア太平洋・日本地域担当シニアバイスプレジデントのRchard Scott氏
AIの活用に注目が集まる中、AIに学習させるための企業内データの価値が再認識され、多くの企業がデータ管理プラットフォームに改めて注目している。

　2022年11月にOpenAIが「ChatGPT」を公開したことで一気にブームとなった生成AIに関しては企業の対応も早く、自然言語による対話的な問い合わせによって、社内規定などに基づく「正しい」回答を得られるようなチャットボット型のサービスが多くの企業で利用されるようになった。その後にベンダー各社が積極的に取り組み始めたAIエージェントやエージェンティックAIに関しては、ユーザー企業の対応は進んでおらず、提供ベンダー側とユーザー企業側のギャップが拡大し、ユーザー企業が置き去りになっているかのような印象も受ける。

　これは、生成AIによるインテリジェントなチャットボットのレベルであれば、関連するドキュメント類を学習させて適切なチューニングを実施することで、実用的なシステムとして活用できるケースが多いのに対し、一部とは言え、業務プロセスを任せられるAIエージェントを準備するためにどのようなデータがどれくらいあればいいかを直感的に把握することができない上、データの品質が低いなどの問題が出てきてしまい、難易度が一気に跳ね上がってしまうためだろう。

　さらに、歴史の長い日本企業ではデジタルトランスフォーメーション（DX）の実現を妨げている要因ともなっているレガシーシステムの問題もある。適切なモダナイズが実施されないまま延命を繰り返しているレガシーシステムに重要な業務データが格納されていたりするのだが、こうしたデータを適切なタイミングで抜き出してAIに学習させていくためにはどうすれば良いのか、明確なロードマップが見えないことで困惑している企業も少なくないのではないだろうか。

　AI時代に必須となるデータ管理プラットフォームとしてクラウド型データレイクハウスなどに注目が集まるが、サイロ化されたレガシーシステムが多数残存しているような企業では、個々のシステムごとにデータがふぞろいだったりといった品質問題が発生しやすい。こうした問題に対するソリューションとして注目されているのがInformaticaの「Intelligent Data Management Cloud」（IDMC）だ。

　同社は、メタデータを活用したデータカタログを基盤として分散したデータの統合や、システムごとに差異のあるデータを信頼できる単一のデータとしてまとめ上げるためのマスターデータ管理（MDM）などに強みを持つ。ここでは、同社のアジア太平洋・日本地域担当シニアバイスプレジデントのRchard Scott（リチャード・スコット）氏に、AI時代に必須となるデータ管理の基本的な考え方について聞いた。

Salesforceとの統合に向けて

　2025年5月27日に、SalesforceはInformatica買収に関する最終合意を締結したことを発表した。「Agentforce」などの取り組みを通じてエンタープライズユーザーのエージェンティックAIの活用を強力に推進しているSalesforceが、AIを支えるデータ基盤の実現のためにInformaticaとの統合を選んだ形だ。

　統合作業は現在進行中で、現在両社はそれぞれ独立の企業体として事業を継続しており、2025年11月～2026年2月の間で統合が完了することが見込まれているという。この統合についてScott氏は「5000人規模の企業が8万人規模の企業に加わるということで、ワクワクしている。統合後は、Informaticaの『Data Cloud』がアップデートされ、現段階では『Data 360』と呼ばれている新しいデータ基盤となる予定だ。Salesforceが以前に買収済みの『Mulesoft』とIDMCの間には機能の重複がほとんどなく、補完的な形でシナジーを発揮できると考えている」と語った。

　現時点では両社の統合に関する新情報は特に公表されていないが、買収発表時のプレスリリースには「契約完了後、Salesforceはインフォマティカの技術基盤を迅速に統合する計画。これには、Agentforce向けのデータ統合、品質管理、ガバナンス、統一メタデータ管理に加え、Data Cloud上のMDMを含む単一のデータパイプラインが含まれる。この『理解のシステム』をSalesforceエコシステムにシームレスに組み込み、さらなる価値創出を目指す」と書かれており、両社のデータ基盤の深いレベルで統合が実施されることが期待される。

　一方でInformaticaの基本的な戦略は変わることはないという。同氏は「Informaticaはデータの世界における“スイス”（＝永世中立国）であり続けるという戦略を変えない。われわれは5万種ものネイティブコネクターを用意しており、あらゆるベンダーの製品と接続できる」と語り、多種多様な製品／システムを対象にデータ統合が可能であるという強みが今後も維持されると強調した。
AI活用に向けたデータ基盤の課題

　多くの企業が熱狂的にAIに取り組んだが、現実的な課題が明らかになるにつれて熱狂は冷め、超えなくてはならないハードルが意外に高かったことが理解されつつある。マサチューセッツ工科大学（MIT）の調査によると、AI関連で多額の投資が行われている一方、95％の企業／組織はほとんど成果を得られずに終わっているとされていることを踏まえ、Scott氏は多くの企業が直面する課題について説明した。

　同氏が訪問した国内企業では、300ものサイロ化した相互接続されていないシステムを運用しており、データの品質が担保できず、データに対するガバナンスも効いていないため、AIの時代に対応するためにはさまざまな課題があることを認識していたという。

　同氏は国内企業の事例として、ヤマハ、スバル、リコーを挙げ、これらの企業がサイロ化したシステムを同社のソリューションを用いてデータ統合を行い、AIに求められるデータ品質やガバナンスを実現したと紹介した。さらに、こうした取り組みはシンガポールやタイ、オーストラリアといったアジア太平洋地域はもちろん、米国でも同様だと言う。

　特に、AIエージェントやエージェンティックAIの活用は始まったばかりであり、準備が整っていない企業が多いのは日本に限った話ではない。むしろ、米国発のIT関連の最新のトレンドが10年ほど遅れて入ってくることが多かった日本だが、AIに関しては米国とほぼ同じタイミングで入ってきており、日本が遅れているとは思わないという声も聞かれる。

　一方で、DXに関する取り組みが停滞気味であるなど、モダナイズされていないサイロ化されたシステムが多数残存していることも間違いない。こうした日本企業の現状に対し、Scott氏はまずデータ品質を向上させ、ガバナンスを確立することから始めることを推奨する。

　まずはレガシーシステムをモダナイズするというやり方も考えられるが、同氏は「新しい家に引っ越す前に古い問題は片付けてしまい、新しい家に持ち込まないようにした方が良い」という例え話でその意義を強調した。

　同社はメインフレームなどのレガシーシステムを含む多種多様な環境に対応するコネクターを用意しており、既存のシステムの運用を継続しつつ、データ統合を実現することが可能だ。国内ではERPシステムの移行に失敗して長期にわたるシステム停止を招いた例も広く報道され、老朽化したシステムをモダナイズすることは決して簡単なことではなく、万一の場合には大きな損失を発生させるリスクもあることが明らかになった。

　しかし、老朽化したシステムをそのままにしておくだけでは、AI時代に求められる高品質かつガバナンスの効いたデータ基盤を実現することが困難になってしまう。Informaticaであれば、まずはデータ統合を先行して実施し、AI時代への対応を行った上で改めてレガシーシステムのモダナイズについて検討することが可能になる。

　10月29日に発表されたIDMCの最新リリースでは、複雑なデータ管理目標の自動化を可能にするAIエージェント機能などが提供され、自然言語による対話型のインターフェイスを通じて、高度な知識やノウハウを要するデータ管理の専門的なタスクを容易に実行できるようになった。ユーザー企業が独自の業務プロセスを処理できるAIエージェントを開発するのは容易ではないが、IDMCのような専門的な機能を実装したプラットフォーム内で必要な処理を代行するAIエージェントを実装する形であればユーザー企業が利用することも容易になり、ユーザー企業がAIエージェントの運用に慣れていくきっかけにもなりそうだ。

　データ統合やMDMなど、重要だがやや地味な領域に強みを持っていたInformaticaだが、本格的なAI時代の到来により、改めてそのソリューションの価値に注目が集まっている。Salesforceとの本格的な統合によってエージェンティックAI時代をリードする存在になることが期待される。",[],[]
AIが変えるセキュリティの攻防構図--日本マイクロソフト、脅威動向を解説（ZDNET Japan）,https://news.yahoo.co.jp/articles/c04ab584b7253ff7c56ba74238a1989780754609,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20251104-35239957-zdnet-000-5-view.jpg?exp=10800,2025-11-04T10:12:00+09:00,2025-11-04T18:43:05+09:00,ZDNET Japan,zdnet,ZDNET Japan,2139,"MicrosoftのLefferts氏
日本マイクロソフトは10月30日、報道機関向けの説明会を開催し、Microsoft Threat Protection担当 コーポレート バイス プレジデントのRob Lefferts（ロブ・レファーツ）氏が、世界の脅威動向と、それらが新しいテクノロジーにどのような影響を及ぼしているかを説明した。

　Lefferts氏は、Microsoftの「Digital Defense Report 2025」を取り上げ、「現在、攻撃はより速く、規模と巧妙さを増しながら押し寄せている。興味深いのは、『セキュリティ攻撃は常に速度、規模、巧妙さが増している』という表現が、実は20年前からずっと当てはまっていることだ」と指摘する。

　しかし、2025年はこの変化のペースがより速くなっているという。これは、攻撃者側がAIを多様な方法で利用し、攻撃の質と実行速度を向上させていることに起因しており、防御側も同様に、AIを活用して対応を加速し、攻撃を未然に阻止する上での有効性を高める必要があると説く。

　あらゆる規模の組織で攻撃者による“データの抜き取り”が増加している。また、依然としてランサムウェアは世界中で続く問題となっている。ランサムウェア関連の攻撃は、全てのデータを暗号化し、復号のために身代金を要求する脅迫や、データを盗み出し、支払いに応じなければダークウェブやインターネットで公開するという脅迫に加え、バックドアを残し、数年後に企業へのアクセスを転売するといった「三重脅迫」が確認されている。

　攻撃者は、フィッシングの誘い文句や攻撃の生成にAIを使うことで、スペルや文法を違和感のないものにし、標的を絞ったメールの作成まで容易にできるようになった。また、攻撃対象の被害組織ごとにAIがカスタム設計されたマルウェアの生成を支援し、攻撃全体をAIが指揮することさえある。

　さらに、攻撃を実行するために必要なインフラの自動生成にもAIが使われているという。従来、攻撃者が世界中に複数のドメインを持つような広範なインフラを構築するにはコストがかかった。そのため、Microsoftのような企業がそのインフラを停止させることは、攻撃者に対する有効な対抗策であった。しかし現在、攻撃者はAIを使ってインフラを次々と再生成し、無数の新しいドメイン名から攻撃を仕掛けているという。

　日本では、サイバー活動は非常に大規模で、サイバー脅威の影響を受けた顧客数は世界で7位、アジア太平洋地域では1位となっており、顧客の13.1％が影響を受けていることが分かった。

　日本は言語の壁があることで、多くのソーシャルエンジニアリング攻撃から比較的隔離されてきた。しかし、大規模言語モデル（LLM）におけるAIの台頭により、この言語の壁は事実上のリスクとなってしまい、攻撃者は活動を大規模化できるようになった。その結果、日本においてソーシャルエンジニアリングによる攻撃が成功し、ランサムウェアのような継続的な活動につながっているという。

　攻撃者は、政府や研究機関、学術機関を標的とするケースが依然として多いが、テクノロジーのサプライチェーンを侵害するためにIT業界が標的となるケースも多い。さまざまな規模の企業から知的財産を窃盗するため、従業員を内部の協力者（インサイダー）に変える行為も見られるという。

　Microsoftは、セキュリティ対策の推奨事項として、「取締役会レベルでサイバーリスクについて議論すること」や「ID保護」「フィッシング耐性のある多要素認証（MFA）の有効」などを提唱している。2025年は、「AIと量子技術のリスク計画を今すぐ開始すること」を加えている。

　同社は、AIによってセキュリティチームの変革を推し進めたいとしている。セキュリティオペレーションセンター（SOC）が行っていた反復的なタスクをAIが引き継ぐことで、人間はセキュリティ対策をより戦略的に考えられるとしている。将来的には、複数のAIエージェントが連携し、人間のタスクを遂行するといった「デジタル労働力」がセキュリティチームを強化し、脅威の増大に対抗する手段となる。

　その一方で、AIエージェント自体のセキュリティとガバナンスの確保が重要になる。AIエージェントがアクセスするデータの保護、エージェントをインベントリー管理するためのID保護、AIが稼働するクラウドやプラットフォームの保護、そしてこれらを用いてSOCの業務を革新すること、という4つの側面からセキュリティを考える必要があるという。

　同社は、SOCを支援するためのエージェント群を提供しており、アラートやインシデントの調査、データとIDの保護、パッチ適用デバイスの管理といった幅広い業務をカバーするエージェントを発表している。これらのエージェントはセキュリティチームやITチームと連携し、フィードバックを受けながら継続的に学習・改善する。この「フライホイール」のような仕組みによって、組織全体のセキュリティ環境をより強固にし、新たな脅威への対応能力が向上していくという。",[],[]
Perplexity、「Perplexity Patents」を公開--自然言語で特許を検索（ZDNET Japan）,https://news.yahoo.co.jp/articles/a6a48d01e9fc8c5f1ea2fc1a7510611b58843940,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20251104-35240001-zdnet-000-2-view.jpg?exp=10800,2025-11-04T10:03:00+09:00,2025-11-04T11:50:47+09:00,ZDNET Japan,zdnet,ZDNET Japan,1352,"Perplexity、「Perplexity Patents」を公開--自然言語で特許を検索の画像
特許の検索は、発明者にとって時間と手間がかかる上、完璧に行うのが難しい作業だ。ある分野の特許を調べようとすれば、「Google Patents」や米特許商標庁（USPTO）のサイト、各国の特許を検索できるデータベースなど、複数のツールを駆使しなければならない。さらに、特定のキーワードやテーマコードを使って検索したり、場合によっては、より精度を高めるために特許弁護士に依頼したりすることも必要になるだろう。

　人工知能（AI）企業のPerplexityが新たにリリースした「Perplexity Patents」は、誰もが手軽に、しかも素早く特許を検索できるようにすることを目指したツールだ。米国時間10月30日に公開されたこのAIベースのデータベースは、自然言語を利用することで、研究者が目的の特許を見つけ出せるよう支援する。

　「Perplexity Patentsは新たな時代を切り開くものだ。誰もが特許情報にアクセスし、一貫したコンテキストで質問を重ねることで、明確な回答が迅速に得られるようになる」と、Perplexityのチームはブログ記事で述べている。

仕組み

　Perplexity Patentsのユーザーは、特定のキーワードやテーマコードを使うことなく、データベースに尋ねることができる。Perplexityによると、「言語学習向けAIに関する特許はある？」とか、「2024年以降の量子コンピューティングに関する主要な特許は？」といったあいまいな言葉でリサーチを進められるという。そのため、キーワードと正確な表現を慎重に組み合わせた検索クエリーを使う必要がない。

　Perplexity Patentsは、ユーザーから質問を受けると、Perplexityの特許ナレッジインデックスを利用して、特許の一覧と元の特許文書へのリンク先を表示する。

　また、ある種類の特許に関する質問を受けると、関連する特許を併せて提示したり、異なる発明を比べて表示したりした上で、続きの質問を待つ。そのため、ユーザーは最初から検索をやり直すことなく、さらに詳しい情報を得られる。例えば、フィットネストラッカーについて質問すれば、ヘルスケア向けのウェアラブル機器やスマートバンドなど、同じようなテーマの結果も返ってくるはずだ。

　さらにこのツールは、先行技術（ある発明が以前から存在し、すでに特許を取得していることを示す証拠）を、公式の特許データベースだけでなく、ブログ、動画、コード、学術論文、公開ソフトウェアリポジトリーなどの情報源から見つけ出してくれる。

利用方法

　Perplexity Patentsは現在ベータ版で、すべてのユーザーに無料で開放されている。有料プランの「Perplexity Pro」か「Perplexity Max」に加入しているユーザーなら、利用できる回数が増え、追加の設定オプションを利用することも可能だ。試してみたい方は、Perplexityのサイトにアクセスしてほしい。

この記事は海外Ziff Davis発の記事を4Xが日本向けに編集したものです。",[],[]
パスワード管理の甘さが招く企業リスク--1Passwordが提言する5つの改善策（ZDNET Japan）,https://news.yahoo.co.jp/articles/4424941a83dcbb41ad3bc799d0b7bc6bced517a9,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20251104-35239995-zdnet-000-1-view.jpg?exp=10800,2025-11-04T07:41:00+09:00,2025-11-04T07:41:00+09:00,ZDNET Japan,zdnet,ZDNET Japan,2320,"提供：Elyse Betters Picaro / ZDNET
個人アカウントで弱い、あるいは漏えいしたパスワードを使用すること自体が問題であるが、職場でそれを使用することは、本人だけでなく企業全体を危険にさらすことになる。パスワードマネージャーの1Passwordが発表した最新のレポートによれば、この行為は重大なセキュリティ脅威と見なされている。

　2025年版の年次レポート「The Access-Trust Gap」において、1Passwordは、パスワードレス認証への移行が進む中でも、依然としてパスワードが問題を引き起こしている実態を明らかにした。このレポートは、米国、カナダ、英国、ドイツ、フランス、シンガポールの6カ国における5200人の労働者を対象としたオンライン調査に基づいている。調査対象には、一般的なデスクワーク従事者だけでなく、ITおよびセキュリティの専門家も含まれている。

　「自社のセキュリティチームが十分な保護を提供する上で最も影響を与えている要因は何か」との問いに対し、44％の回答者が「従業員による弱い、または漏えいした認証情報の使用」と答えた。調査結果によれば、従業員のパスワード管理の実態は改善されるどころか、むしろ悪化しており、この割合は前年のレポートから増加している。

　実際、約3分の2の従業員が、仕事と個人のアカウントで同じパスワードを使い回していたり、デフォルトの認証情報を使用していたり、メールやメッセージアプリでパスワードを共有していたりすると回答している。皮肉なことに、ITやセキュリティの専門家の方が、非IT従業員よりもリスクの高いパスワードの使い方をしている傾向が見られた。

　例えば、非IT系の従業員のうち15％が仕事と個人のアカウントで同じパスワードを使用していると答えたのに対し、IT専門家ではその割合が24％に上った。

　調査では、パスワードの管理に関する問題も浮き彫りになった。常に複雑でユニークなパスワードを使用していると答えたのは、一般従業員で30％、IT専門家ではわずか23％にとどまった。また、パスワードマネージャーは認証情報の漏えいを防ぐ手段の1つであるにもかかわらず、企業が従業員にそのようなツールを提供していると答えたのは、IT専門家で38％、その他の従業員では26％に過ぎなかった。

　過去3年間にデータ侵害を経験した企業の最高情報セキュリティ責任者（CISO）のうち、50％が「漏えいした認証情報」が原因の1つだったと回答しており、これは「セキュリティ脆弱（ぜいじゃく）性の悪用」に次いで2番目に多い要因であった。その他の要因としては、従業員による未管理または未承認のアプリケーションやデバイスの使用、データの持ち出しなどが挙げられている。

　パスワードレスな未来は、個人にとっても企業にとっても望ましいものである。しかし、その実現には多くの課題が伴っている。企業環境においてさえ、パスワードマネージャーの運用と管理は容易ではなく、パスキー（passkey）も、より多くの人々が採用するには、利便性や普及の面でまだ幾つかのハードルが残されている。

　それでも、パスキーは企業の現場で徐々に浸透しつつある。調査対象となった従業員のうち41％が、利用可能な環境ではすでにパスキーを導入していると回答した。また、セキュリティおよびITの専門家の89％が、自社が従業員に対してパスキーへの移行を推奨している、あるいは今後推奨する予定であると述べている。さらに、回答者の25％は、パスキーが利用可能になれば、喜んでパスワードから切り替えると答えている。

　ただし、パスワードからパスキーへの移行は、単にスイッチを切り替えるような簡単な作業ではない。多くの企業にとって、この移行は数年にわたるプロジェクトとなる見込みであり、技術、業務フロー、規制要件のバランスを取りながら進める必要がある。そのため、移行期間中はパスワードとパスキーが共存することになり、両者の安全性と利便性を確保することが求められる。

　「完全にパスワードを排除した環境は、セキュリティリーダーたちにとって長年の夢である」と、ある回答者は述べている。「しかし、パスワードを完全に廃止するには数年を要する取り組みであり、その過程においても認証は常に可能な限り安全でなければならない」と強調した。

　このような背景を踏まえ、1Passwordは、企業がパスワードレス認証への移行を進めるための5段階の計画を提示している。

　まず、ロードマップとプロセスを策定することが重要である。ここでは、弱いパスワードを強固なものに置き換え、多要素認証（MFA）を導入し、パスキーを含むパスワードレス認証への移行をどのように進めるかを明確にする必要がある。

　次に、従業員が強力なパスワード、MFA、パスワードレスソリューションへとスムーズに移行できるよう、明確なガイドラインとサポートを提供することが求められる。

　また、コンプライアンス担当者には、パスワードレスシステムがISO、SOC 2、GDPRなどの規制ガイドラインに準拠しているかを確認する責任がある。

　移行期間中はパスワードの使用が避けられないため、企業向けのパスワードマネージャーを活用し、パスワードの管理と従業員の利便性を両立させることが推奨される。

　そして、可能な限りSMSコードなどのリスクの高い認証手段は排除すべきである。

この記事は海外Ziff Davis発の記事を4Xが日本向けに編集したものです。",[],[]
PerplexityとGetty Imagesが提携--AI検索の著作権表示と情報精度向上へ（ZDNET Japan）,https://news.yahoo.co.jp/articles/972577892b668e7691603b2c38f81a8ffad7fe3c,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20251104-35239993-zdnet-000-1-view.jpg?exp=10800,2025-11-04T07:24:00+09:00,2025-11-04T07:24:00+09:00,ZDNET Japan,zdnet,ZDNET Japan,1126,"提供：ogeday çelik/iStock/Getty Images Plus via Getty Images
PerplexityとGetty Imagesは、AIによる画像検索結果の向上と、画像制作者のクレジット表示の改善を目的とした複数年にわたるライセンス契約を締結した。両社の提携は米国時間10月31日に発表された。

　発表によれば、Getty ImagesはPerplexityに対し、「高品質で独自性のあるクリエーティブおよび報道用画像」へのアクセスを提供し、より豊かな視覚体験の創出を支援する。一方、Perplexityは画像の表示方法を改善し、クレジットを出典元にリンクさせることで、ユーザーがライセンス画像を合法的に使用する方法についての理解を深めることを目指す。

　この提携により、PerplexityはAI生成画像ではなく、ストック画像や報道用画像を検索結果に表示できるようになる。AI検索エンジンによる情報の帰属表示の精度向上を図るものであり、PerplexityがGettyのコンテンツを用いてAIモデルを学習させることはなく、画像生成機能も含まれていない。

　両社の協業は、AIが情報を表示する際の精度とクリエーターへの敬意を高めることを目的としている。AIツールや検索エンジンは、情報を誤って解釈したり、存在しないURLを生成したりする「ハルシネーション（幻覚）」を起こすことがある。例えば、AIがニュース記事を取り込んで要約する際には、正確性が損なわれる可能性がある。最近の調査では、4つのAIシステムによって生成された回答の約半数に、何らかの不正確さや重大な問題が含まれていたことが判明している。

Gettyのコンテンツにおける画像クレジットの表示を改善することで、Perplexityは情報の出典をより明確に示すことを目指している。Perplexityは、検索結果にリンクと出典表示を含めた最初のAI検索ツールであり、この機能は現在、AIチャットボット全体で標準的なものとなっている。これは、幻覚や誤情報への懸念が高まっていることを背景としている。

　Perplexityのコンテンツおよびパブリッシャーパートナーシップ責任者であるJessica Chan氏は、プレスリリースの中で「帰属表示と正確性は、AI時代において人々が世界を理解するための基本である」と述べており、「Getty Imagesは、AIによる情報探索の未来には、コンテンツ制作者への敬意が不可欠であるというわれわれの信念を共有している」と語っている。

この記事は海外Ziff Davis発の記事を4Xが日本向けに編集したものです。",[],[]
「iPad mini」に待望のOLEDディスプレー搭載か--防水設計や価格改定も視野に（ZDNET Japan）,https://news.yahoo.co.jp/articles/995bf28453ab52f5f130448e01cfb5abadfa3c4f,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20251104-35239992-zdnet-000-1-view.jpg?exp=10800,2025-11-04T07:12:00+09:00,2025-11-04T07:12:00+09:00,ZDNET Japan,zdnet,ZDNET Japan,1696,"提供：Prakhar Khanna/ZDNET
「iPad mini」は、そのサイズ感から筆者にとって最も好ましいApple製タブレットである。ノートPCと一緒に持ち運べるほど小型でありながら、写真編集への情熱を再燃させるほどの性能を備えている。しかし、過去2世代にわたり、Appleがより優れたディスプレーを提供してくれることを願ってきた。現行の液晶（LCD）パネルは、読書や色の正確性において理想的とは言えず、指紋も付きやすい。だが、2026年についにその願いが叶う可能性がある。

　BloombergのMark Gurman氏による報道によれば、iPad miniは早ければ2026年にも有機EL（OLED）パネルを搭載する可能性があるという。また、Appleは今後数年をかけて、「iPad Air」「MacBook Pro」「MacBook Air」にも段階的にOLEDディスプレーを導入していく計画だとされている。

　ただし、MacBook AirにOLEDが搭載されるのは「2028年以前には実現しそうにない」とも報じられている。参考までに、2024年の「iPad Pro」がApple初のOLED搭載タブレットとなったが、iPad Airよりも先にiPad miniがOLED化されるというのは意外な展開である。

　筆者がOLED搭載のiPad miniに期待する理由は明確だ。タブレットの主な用途は、ブラウジング、写真編集、読書、動画視聴の順であり、iPad miniはその全てにおいて理想的なバランスを提供してくれる。「iPhone」はブラウジングには適しているが編集には不向きであり、大型の「iPad」は編集には最適だが読書には重すぎる。その点、iPad miniはフォームと機能の完璧な融合であり、筆者の旅行用バックパックに常に収まっている。

　iPad Airとは異なり、長時間手に持っていても手首に負担がかからず、「Kindle」の電子書籍を読んだり、購読している出版物を閲覧したり、「Lightroom」で旅行写真を編集したりする際にも、手放したくなるような不快感はない。

　そのサイズ感はまるで文庫本のようで、薄く、小さく、どこへでも持ち運べるポータブル性を備えており、旅行時の理想的なパートナーとなっている。

　Gurman氏によれば、次期iPad miniでは筐体の再設計により完全防水化が図られ、「従来のスピーカーホールに代わって振動技術を用いた新しいスピーカーシステム」が搭載される可能性があるという。耐久性が重視されるのであれば、「iPhone 17」シリーズで採用された指紋耐性コーティングも導入してほしいところだ。現行モデルは指紋が付きやすく、視認性に影響を与えることがある。

　さらに、次期iPad miniは100ドルの値上げが予定されており、価格は799ドルからとなる見込みだ。これは、現在Amazonで559ドルで販売されている11インチのM3搭載iPad Airとほぼ同価格帯となる。

　つまり、ユーザーはOLEDディスプレーを搭載した小型タブレット（ただしプロセッサーは「A」シリーズ）を選ぶか、「M」シリーズチップを搭載したLCD画面の大型モデルを選ぶかという選択を迫られることになる。両者が同価格帯でどのような評価を受けるか、非常に興味深い。

　Appleは今後3年間でOLEDディスプレーをさらに多くの製品に導入する計画だ。この流れの中で、iPad miniのOLED搭載版の登場に続き、年内にはOLEDディスプレーを搭載したタッチスクリーン式のMacBook Proが登場するとみられている。その後、OLEDディスプレーは順次iPad AirやMacBook Airにも導入される見込みだ。ただし、2026年春に登場が予定されているMacBook Airについては、引き続きLCDスクリーンが採用される可能性が高いとされている。

この記事は海外Ziff Davis発の記事を4Xが日本向けに編集したものです。",[],[]
分断を乗り越え、連携を取り戻す--情報構造の可視化が導くアプローチ（実践編1）（ZDNET Japan）,https://news.yahoo.co.jp/articles/67a1bf56ee8530ecb68fd4abce4d753eb5d898c0,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20251104-35239793-zdnet-000-1-view.jpg?exp=10800,2025-11-04T07:00:00+09:00,2025-11-04T07:00:00+09:00,ZDNET Japan,zdnet,ZDNET Japan,3506,"製造業とDX
1-1. DXの最初の壁：情報が“受け渡されない構造”が業務連携を鈍化させる

　前回の導入編では、DX推進の出発点は「課題の構造」を理解することだと述べました。しかし、その構造を正しく捉えるためには、まず「どの情報が見えておらず、どのデータが受け渡されていないのか」を明らかにする必要があります。

　製造業における多くの組織では、図面や検査記録、見積履歴、購買データ、日報、手書きメモなど、日々膨大な情報が生まれています。にもかかわらず、それらは適切に引き継がれず、部門やシステムごとに断片的に蓄積されています。結果として、情報は存在しているのに次の工程へ渡らず、“再利用できない・連携できない”構造が定着しています。

「過去の類似設計を参照したいが、どこにあるか分からない」
「品質不良の原因分析を見たいが、別部門にしか残っていない」
「改善報告書を作っても、他拠点では共有されず同じ問題が繰り返される」

　情報が残らず、受け渡されない。この“継承の欠落”こそが、業務の連携を鈍化させ、DXを進めようとする組織の最初の壁になっています。

1-2. 残らないデータ・途切れる関係：失われた情報資産

　現場で「使われていない」と言われる情報の多くは、実は情報間の関係性が途切れた状態で存在しています。

図面データ：変更履歴や派生品が多く、最新版や設計意図が追えない
検査記録：紙帳票やPDFで保管され、他データとの照合ができない
見積履歴：メールやExcel、基幹システムに分散し、履歴の整合性が取れない
手書きメモ・日報：熟練者の知見が個人フォルダにとどまり、他者が参照できない
画像・動画データ：品質検査や点検で撮影しても、製品番号や工程情報と結びついていない

　これらは単体では生かしにくい情報ですが、情報間の関係性を取り戻せば一気に意味を持つ資産に変わります。図面と見積履歴をひも付ければ、過去類似品のコストを即座に推定できる。検査データと生産条件をつなげば、不良傾向を可視化し、改善サイクルを早められます。

　つまり問題は、「データがない」のではなく、「関係性が残っていない」こと。この壁を乗り越えるためには、データを増やすことではなく、断たれた情報間の関係性を再構築することです。

1-3. なぜ受け渡されないのか：「情報の壁」と「心理的ハードル」

　情報間の関係性が断たれてしまう理由は、技術的な問題だけではありません。多くの現場では、「参照したいのに参照しない」「共有したいのに共有できない」という状態が常態化しています。

　背景には、標準化と明確な所掌分担が逆に情報の流れを止めている構造があります。部門ごとに役割が固定化され、「A部門では使っているがB部門では使われていない」という状況が生まれる。結果として、情報の受け渡し経路が制度的に閉ざされてしまうのです。

　さらに、情報へのアクセスや参照そのものの手間、心理的な抵抗も無視できません。

「品質部門の記録を設計段階で確認したいが、別システムで権限がない」
「フォルダ構成が複雑で探すだけで数時間かかる」
「他部門に問い合わせると相手の手を止める気がして遠慮する」

　こうした小さな手間と心理的コストの積み重ねが、“参照できない”ではなく“参照しない”という選択を生みます。担当者は怠けているのではなく、「どうせ見つからない」「自分で進めた方が早い」という諦めに至っているのです。この“無意識の諦め”こそが、情報の関係性構築と組織全体の知見循環を静かに妨げています。
1-4. 部門ごとに分断された「サイロ構造」のジレンマ

　データが受け渡されないもう一つの要因は、部門ごとの最適化の積み重ねにあります。設計は製品情報管理システム（PDM）、調達は購買システム、品質はExcel、営業は顧客関係管理システム（CRM）や営業支援システム（SFA）など、それぞれの仕組みは部門内では合理的に機能していても、全社的に見ると整合性を欠きます。

　さらに、部門ごとの重要業績指標（KPI）や評価制度が、情報共有を阻害しています。「他部門との連携」は成果として評価されにくく、結果として情報を自部門内にとどめる文化が根づいてしまう。これが、情報の受け渡しを阻む「サイロ構造」の本質です。

1-5. 可視化とは「受け渡しの経路」を描き直すこと

　サイロ構造に伴う課題を解決するための第一歩は、情報の可視化です。ここで言う可視化とは、単にデータをグラフにすることではなく、「情報がどこからどこへ流れるべきか」という受け渡しの設計図を描くことです。

具体的なステップは次の3つです。

自部門業務の洗い出し：自部門で対応している各種業務を棚卸しする。
参照情報および参照先の整理：各種業務において、どこから・何の情報を参照しているかを整理する。
あるべき情報流通の整理：本当は見るべき（今参照できていない）情報や、他部門と連携するべき情報の整理。

　可視化を進めると、棚卸しの段階で「こんなに情報が散らばっていたのか」と驚く声が上がります。そしてマッピングを通じて初めて、“全社の流れ”が見えるようになります。この瞬間に各部門では「共有とは負担ではなく、連携の基盤だ」と実感し始めます。

1-6. 情報の構造的な連携により、時間と組織を超えた資産化が可能に

　このように情報の受け渡し経路を描き直すことで、初めて“構造的な連携”が可能になります。ただし可視化の目的は、単に“見える化”することではありません。重要なのは、情報を残し、次の工程や人へ確実に渡せる仕組みを作ることです。

　設計と製造、品質と調達、営業と顧客対応など、部門や拠点が異なっても、同じデータにアクセスできれば、設計変更の意図が正確に伝わり、品質対策も一貫性を持って展開できます。共通基盤を通じて情報が残り、受け渡されるようになると、業務は“属人的な連携”から“構造的な連携”へと変わります。

　情報が残るとは、単にデータが保存されることではありません。過去の知見や判断の意図、トラブルへの対応履歴といった“文脈”が蓄積され、時間を超えて参照できる状態を意味します。これにより、数年前の設計判断が今日の品質改善に生き、地方拠点の改善ノウハウが海外工場でも再現されるようになります。

　さらに、情報が渡るとは、単に部門間で共有されることにとどまりません。階層や役職を超えて、誰もが同じ前提で考え、議論できる状態を生むことです。現場のオペレーターが上流設計の意図を理解し、管理職が現場データをもとに意思決定を下す。こうした“縦横無尽な情報の流れ”が整うと、組織全体が一つの知的体として動き始めます。

　つまり、情報が残り、渡ることによって、時間を越えて知見が生き、組織を越えて連携が進み、レイヤーを越えて意思疎通が可能になる。これこそが、DXの本質である「人と情報の関係性を再設計する」姿なのです。

おわりに--情報は「残り、渡せて」こそ資産になる

　DX推進に向けた第一歩は、社内に散在する情報を正しく結び直し、次の工程に渡せる状態をつくることです。 情報が残り、受け渡されることで、組織は初めて“全体最適”の視界を得られます。

　データが流れ始めると、現場の会話が変わります。起こったトラブルの事実共有だけがされたり、「誰がやったか」が問われたりする非生産的な会議が、「なぜそうなったか」を共有する場に変わる。事実を基にした対話が増えると、DXは単なるIT施策ではなく、“働き方と組織文化を変える取り組み”へと進化していくのです。

　次回の実践編2では、この基盤の上で情報を統合し、“意味ある資産”として活用するプロセスを掘り下げます。

戸田 涼
キャディ 営業推進部・部長
建設業の現場経験を原点に、ウェブ制作、システムインテグレーター（SIer）、リクルートで国内外複数事業の企画・コンサルティング・マネジメントを歴任。2020年、キャディに参画。国内外のマーケティング組織立ち上げを成功させた後、現職である営業推進部の部長に就任。製造業AIデータプラットフォームCADDiの導入コンサルタントとして、大手製造業を中心に100社以上のDX・業務改革プロジェクトをリード。現場の課題とデータ活用の両面に精通したコンサルティングを強みとする。
※「製造業AIデータプラットフォーム」はCADDiの登録商標です。",[],[]
第3回：社内AIチャットボットの次は「AI駆動化」で業務改革--実証実験で見えた、AIとシステムの連携術（ZDNET Japan）,https://news.yahoo.co.jp/articles/3340ac2e1c5e833fadba7ac396c5dacb19325fd1,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20251104-35239654-zdnet-000-1-view.jpg?exp=10800,2025-11-04T07:00:00+09:00,2025-11-04T07:00:00+09:00,ZDNET Japan,zdnet,ZDNET Japan,12180,"データ活用と生成AIの基礎知識：革新技術の仕組みと可能性
0. 本稿の目的

　本稿では“AI駆動開発”について、当社での検証結果を共有するとともに、その導入に必要な中核的要素であるModel Context Protocol（MCP）、AI用プロジェクトドキュメント設計、AI駆動開発エンジニアの役割などについての考察を共有します。

　この検証では、DVDレンタルサービスにおけるパーソナライズドDM（ダイレクトメール）送付プロジェクトを題材に、設計からデータ抽出・レコメンドモデル構築・配信までを「AI駆動開発」で進める流れを確認しました。 AI駆動開発を導入することで、以下の利点が得られることが判明しました。

    生産性の改善：
        
            エンジニア＋データサイエンティストが従来3～4日かかっていた業務を、エンジニアだけで約1日で完了
            約50％の工数削減を実現
        
    
    属人化問題の解消：
        
            データサイエンティストなどの専門人材に依存しない業務プロセスを構築
            ヒューマンエラーの削減
        
    
    業務プロセスの標準化：
        
            AIに共通の開発ドキュメントやコードを参照させることで、業務プロセスの標準化を促進
            ナレッジの蓄積と共有が容易に
        
    

　多くの研究［1、2、3］がAI駆動開発を新しいソフトウェア開発ライフサイクル手法として位置付けており、この検証と同様の結果を報告しているもの［4］もあります。また、Amazon Web Services（AWS）が提唱する「AI-Driven Development Lifecycle（AI-DLC）」は、この概念を具現化した先進の業界事例として極めて重要です。AI-DLCでは、AIが単なるツールではなく「中心的な協力者（central collaborator）」として位置付けられ、自らワークフローを開始し、人間に対して明確化のための質問を投げかけ、重要な意思決定は人間に委ねるという協調モデルが提唱されています［5］。

　私たちの検証でも、これらの研究でもAI駆動開発には独自のシステム構築や最適化が必要であり、エンジニアの中核的役割すら変化することが示されています。もしあなたがエンジニアリングに何らかの形で関わっているならば、そろそろAI駆動開発の準備を始めるべき時が来ているのかもしれません。

1.  AI駆動化とは

　本稿では論文［1、2］を参考に「AI駆動化（AI-Driven）」を以下のように定義し、この実用性を検証しました。

　AIが実行主体として、権限管理されたツール・API・データソースへアクセスし、タスク分解、実行、検証、次アクション提案までを担う。一方で、人間は「設計（要件・境界・権限の設計）」「監督（ヒューマンインザループによる承認ゲート）」「責任（運用・回復・説明責任）」に注力する。このようなAIと人間の協調的な開発運用形態のこと。

　ここで重要なのは、AI駆動化とはエンジニアのスキルを拡張するための枠組みであって、最終責任をAIに委任するものではないという点です。意思決定の質と説明責任は常に人間側に残ります。これは「ChatGPT」などを使ってAIにプログラミングをさせる「AI支援」とも違いますし、まだまだ実用化に至っていない「全自動化」とも異なります。この3つの形態の比較を以下に整理します。

    AI支援（AI-Assisted）：人間が主役となり、AIはコード補完や下書き生成、説明文の作成などを担います。実装・実行・最終判断は人間が行い、AIはあくまで編集支援の範囲にとどまります。Vibe Codingなどもこのカテゴリーと見なします。
    AI駆動（AI-Driven）：前述の通り、AIがより主体的に提案や処理を進めながらも、人間が最終判断を行う形態です。
    全自動（Full automation）：人の承認なしに本番変更まで自律実行する形態です。限定的で厳格に管理された場面を除き、一般の業務システムでは高リスクであり、本稿の対象外とします。

　それでは、AI駆動化を実現するための中核的要素について、当社での検証結果をもとに解説します。

2. AI駆動開発の検証

　本検証では、DVDレンタル事業を模したダミーデータベースに対して、会員離脱防止を目的としたパーソナルDM配信システム開発を行います。本来であれば、エンジニアとデータサイエンティストが協力して、以下の工程を行いますが、本検証ではこれらの工程をAI駆動化し、エンジニア1人で完結します。

    分析用データ抽出（レンタル履歴.csvの取得）：
        データベースにSQLを実行し、分析用のレンタル履歴データをcsvとして抽出する。
    施策設計（離脱スコア、レコメンドモデル構築）：
        csvを分析し、ユーザーIDごとにサービス離脱スコアを算出する。さらにDVDのレコメンド（推薦）モデルを構築する。離脱スコア上位ユーザーをDM送付対象リストとし、レコメンドモデルでそのユーザーごとにおすすめのDVDを出力しておく。
    施策実行（パーソナライズDM作成・配信）：
        DM送付対象リストのユーザーごとに、最適化されたおすすめDVDをメール本文に記載し、個別に配信する。

　AI駆動開発ツールとして、「Visual Code Studio（VS Code）」と「GitHub Copilot」を採用しました。操作画面のイメージについては以降で解説します。

2.0. 操作画面の説明

　AI駆動開発ツールの基本的な画面ウィンドウは3種類です。（1）フォルダー／ファイルツリー、（2）エディター、（3）AIチャットです。 AIは、（1）のファイル群を参照しながら、（3）に入力されるタスク指示に基づいて、ソースコードやドキュメントを生成し、人間が（2）でレビュー・修正・承認します。

2.1. 操作画面：分析用データ抽出（レンタル履歴.csvの取得）

2.2. 操作画面：施策設計（離脱スコア、レコメンドモデル構築）

2.3. 操作画面：施策実行（DMライティング・配信）

　上記のようなAI駆動開発を実現できるようになるまで、数多くの試行錯誤が必要でした。以降では、チーム全体の仕組みとして取り組むべきAI駆動開発の中核的設計について解説します。

3. AI駆動開発ツールの中核的設計

　AI駆動開発を実現するためには、AIが効果的にタスクを理解し実行できるよう、プロジェクト構造とツール設定を最適化する必要があります。当社の検証から得られた幾つかの設計要素を解説します。

　まずプロジェクトフォルダー構造を以下の図に示します。AIはこのフォルダー構造を参照しながら、タスクを理解し、コードやドキュメントを生成します。

当社が構築したAI駆動用のプロジェクトフォルダー構成

　上記の特に重要なファイルを以降で解説します。

3.1. 「指導ポリシー」としてのカスタム命令

　カスタム命令とは、AIエージェントへの「行動憲章」や「指導ポリシー」で、システムプロンプトとも呼ばれます。.github/.copilot-instructions.mdなどのファイルで、AIがタスクを実行する際に必ず最初に参照されるドキュメントです。

　当社の実証実験では、試行錯誤の末、以下のインストラクションファイルを作成しました。 ここで重要なのは、インストラクションファイル内で、README.mdやMakefileなどのプロジェクトドキュメントを参照するようAIに指示していることです。これらのファイルはプロジェクトの方針定義を行うものであり、人間のエンジニア同様、AIにもこれらを参照してから開発するように指示しています。

　実証実験当時は利用しませんでしたが、カスタム命令にはファイル分割や適用するファイルの指定などが可能です［6］。

.copilot-instructions.mdの内容
---
applyTo: '**'
---
#
利用可能なMCPツール
#
#
#
1. UC Genie MCP
Unity Catalog Genieスペースへの自然言語クエリー. データセット概要取得. SQL結果の自然言語解釈.
#
#
#
2. HubSpot MCP
CRM データ操作. オブジェクトの作成、更新、検索. レポート・ダッシュボード連携.

---
#
チャットセッション開始時のセットアップ手順
**重要**: チャットセッション開始時に以下を読み込んでプロジェクト情報を把握。
 - **README.md**: プロジェクトの概要、機能、セットアップ方法
 - **Makefile**: 利用可能なコマンドとタスク
#
#
#
1. チャットセッション開始
```bash
cat README.md
cat Makefile
```
#
#
#
2. 分析チャットセッション開始
```bash
mkdir -p work/{session_name}
cd work/{session_name}
```
#
#
#
3. 依存関係確認
```bash
make install
make check-env
```

---
#
エラーハンドリング
#
#
#
環境設定エラー
```bash
make check-env
```
#
#
#
依存関係エラー
```bash
make install
```

---
#
チャット終了時チェックリスト
- [ ] クエリーを再実行可能な形で保存
- [ ] 不要な一時ファイルを削除

---
#
注意事項
1. **機密データ**: 実データをGitにコミットしない
2. **環境変数**: `.env`を絶対にコミットしない
3. **大容量ファイル**: 大きなCSVは`.gitignore`で除外
4. **再現性**: 依存関係とステップを明確に記録

3.2. 「プロジェクトの設計図」としてのREADME.mdなどのプロジェクトドキュメントファイル

　AI駆動ツールではAIは、プロジェクト内のドキュメントを「コンテキスト」として参照します。README.mdには、プロジェクト概要、ディレクトリー構造、主要タスクの実行方法を明示的に記述していきます。

　Makefileやscript/ディレクトリーには、AIに使って欲しいコマンドやプログラムを定義できます。これにより、AIが自社の独自環境の使い方を習得できます。

　本検証では、試行錯誤した結果、以下のようなREADME.mdとMakefileを作成しました。

　現在では仕様駆動開発（Spec-Driven Development）と呼ばれる手法に近く、私たちが採用したVS Code専用のフレームワークや統合開発環境（IDE）が登場していることは留意すべきかもしれません。

README.mdの内容
#
分析業務支援プロジェクト
このリポジトリーは、AI駆動によるデータ分析業務を効率化するための共通ソースコードや分析環境を管理します。
#
本プロジェクトのディレクトリー構造
```
.
├── src/
#
共通ソースコード
├── work/
#
分析ケースごとの作業フォルダー
├── scripts/
#
ヘルパースクリプト
├── .env
#
外部環境や依存APIの接続キー情報
├── requirements.txt
#
Python依存関係
├── Makefile
#
タスク管理（推奨）
└── README.md
#
このファイル
```
#
#
#
src/ - 共通ソースコード
- 全ての分析ケースで共通して使用するプログラムを配置
- 再利用可能なライブラリやユーティリティ
#
#
#
work/ - 分析作業ディレクトリー
- 分析ケースごとにサブディレクトリーを作成
#
#
#
scripts/ - ユーティリティスクリプト
- プロジェクト管理用のヘルパースクリプト
- 開発効率向上のためのツール
#
#
#
Makefile - タスク管理
- 共通的なタスクと依存関係を管理
- 環境セットアップからクエリー実行まで統一されたインターフェース
- 主要なコマンド:
  - `make help`: 利用可能なコマンド一覧
  - `make install`: 依存関係のインストール
  - `make run ARGS=""...""`: SQLクエリー実行
  - `make sample`: サンプルクエリ実行
  - `make clean`: 生成ファイルのクリーンアップ
#
#
ベストプラクティス
#
#
#
分析ケースのワークフロー（例）
1. 分析名を決定
2. `work/{分析名}/` ディレクトリーを作成. 以降の生成ファイルはすべてこのディレクトリーに保存
3. SQLクエリーを `query.sql` に記述
4. `Makefile` の `make run` コマンドでクエリーを実行
5. データ抽出結果を `results.csv` に保存
6. 分析コードを `analysis.py` に記述し実行
7. 分析結果レポートを `report.md` に文書化
8. ユーザーへの備考は `README.md` に記述
#
#
#
ワークフローの実行例
```bash
#
1-2. 新しい分析ケースを作成
./scripts/create_analysis.sh my_new_analysis
#
3. SQLクエリーを編集
vim work/my_new_analysis/query.sql
#
4. SQLクエリーを実行
make run ARGS=""--query-file work/my_new_analysis/query.sql --output work/my_new_analysis/results.csv""
#
以降は、ユーザーの指示に従って分析を進めてください
```
#
#
#
バージョン管理
- `src/` の変更は慎重に行い、十分にテストする
- `work/` の内容は基本的に分析ケースごとに独立させる
- 重要な分析結果やクエリーは適切にドキュメント化してコミット

Makefileの内容
#
使用方法:
#
make help          - ヘルプを表示
#
make install       - 依存関係をインストール
#
make check-env     - 環境設定をチェック
#
make run-sql       - SQL実行（引数が必要）
#
make sample        - サンプルクエリを実行
#
make clean         - 生成ファイルを削除

.PHONY: help install check-env run sample clean test
#
デフォルトターゲット
.DEFAULT_GOAL := help
#
変数定義
PYTHON := python3
PIP := pip
SRC_DIR := src
WORK_DIR := work
SAMPLE_DIR := $(WORK_DIR)/sample
PYTHONPATH := $(shell pwd)/$(SRC_DIR)
#
カラー定義
BOLD := \033[1m
GREEN := \033[32m
BLUE := \033[34m
YELLOW := \033[33m
RED := \033[31m
RESET := \033[0m

help:
#
#
ヘルプを表示
@echo ""$(BOLD)Databricks SQL Executor$(RESET)""
@echo """"
@echo ""$(BOLD)利用可能なコマンド:$(RESET)""
@awk 'BEGIN {FS = "":.*?
#
#
""} /^[a-zA-Z_-]+:.*?
#
#
/ {printf ""  $(GREEN)%-15s$(RESET) %s\n"", $$1, $$2}' $(MAKEFILE_LIST)
@echo """"
@echo ""$(BOLD)使用例:$(RESET)""
@echo ""  $(BLUE)make run ARGS=\""--query 'SELECT * FROM table LIMIT 10'\""$(RESET)""
@echo ""  $(BLUE)make run ARGS=\""--query-file work/sample/sample_rfm_query.sql --output results.csv\""$(RESET)""
@echo ""  $(BLUE)make sample$(RESET)""

install:
#
#
依存関係をインストール
@echo ""$(YELLOW)依存関係をインストールしています...$(RESET)""
@if ! $(PIP) show databricks-sql-connector > /dev/null 2>&1; then \
$(PIP) install -r requirements.txt; \
else \
echo ""$(GREEN)依存関係は既にインストールされています$(RESET)""; \
fi

check-env:
#
#
環境設定をチェック
@echo ""$(YELLOW)環境設定をチェックしています...$(RESET)""
@if [ ! -f .env ]; then \
echo ""$(RED)エラー: .envファイルが見つかりません。.env.templateを参考に作成してください。$(RESET)""; \
exit 1; \
else \
echo ""$(GREEN).envファイルが見つかりました$(RESET)""; \
fi
@if [ ! -f ""$(SRC_DIR)/databricks_sql_executor.py"" ]; then \
echo ""$(RED)エラー: $(SRC_DIR)/databricks_sql_executor.pyが見つかりません$(RESET)""; \
exit 1; \
else \
echo ""$(GREEN)Python実行ファイルが見つかりました$(RESET)""; \
fi

run-sql: check-env install
#
#
SQL実行（ARGS変数で引数を指定）
@if [ -z ""$(ARGS)"" ]; then \
echo ""$(RED)エラー: ARGS変数でオプションを指定してください$(RESET)""; \
echo """"; \
echo ""$(BOLD)例:$(RESET)""; \
echo ""  make run-sql ARGS=\""--query 'SELECT * FROM table LIMIT 10'\""""; \
echo ""  make run-sql ARGS=\""--query-file sample_rfm_query.sql\""""; \
echo ""  make run-sql ARGS=\""--output results.csv\""""; \
echo ""  make run-sql ARGS=\""--catalog my_catalog\""""; \
echo ""  make run-sql ARGS=\""--schema my_schema\""""; \
exit 1; \
fi
@echo ""$(YELLOW)Python版Databricks SQL Executorを実行します...$(RESET)""
@PYTHONPATH=""$(PYTHONPATH)"" $(PYTHON) $(SRC_DIR)/databricks_sql_executor.py $(ARGS)

test: check-env install
#
#
テストクエリーを実行
@echo ""$(YELLOW)テストクエリーを実行します...$(RESET)""
@PYTHONPATH=""$(PYTHONPATH)"" $(PYTHON) $(SRC_DIR)/databricks_sql_executor.py --query ""SELECT 1 as test_column"" --output $(WORK_DIR)/test_results.csv

clean:
#
#
生成ファイルを削除
@echo ""$(YELLOW)生成ファイルを削除しています...$(RESET)""
@find $(WORK_DIR) -name ""*.csv"" -type f -delete 2>/dev/null || true
@find . -name ""__pycache__"" -type d -exec rm -rf {} + 2>/dev/null || true
@find . -name ""*.pyc"" -type f -delete 2>/dev/null || true
@echo ""$(GREEN)クリーンアップ完了$(RESET)""

dev-setup:
#
#
開発環境セットアップ（.env.templateがある場合の例）
@echo ""$(YELLOW)開発環境をセットアップしています...$(RESET)""
@if [ -f .env.template ] && [ ! -f .env ]; then \
cp .env.template .env; \
echo ""$(GREEN).env.templateから.envファイルを作成しました。必要な値を設定してください。$(RESET)""; \
fi
@make install

info:
#
#
プロジェクト情報を表示
@echo ""$(BOLD)Databricks SQL Executor Project Info$(RESET)""
@echo ""Python version: $$($(PYTHON) --version)""
@echo ""Project directory: $$(pwd)""
@echo ""Source directory: $(SRC_DIR)""
@echo ""Work directory: $(WORK_DIR)""
@echo ""PYTHONPATH: $(PYTHONPATH)""
@if [ -f .env ]; then \
echo ""Environment file: ✅ Found""; \
else \
echo ""Environment file: ❌ Missing""; \
fi
#
デバッグ用：引数を表示
debug:
#
#
デバッグ情報を表示
@echo ""$(BOLD)Debug Information$(RESET)""
@echo ""ARGS: $(ARGS)""
@echo ""PYTHON: $(PYTHON)""
@echo ""PYTHONPATH: $(PYTHONPATH)""
@echo ""SRC_DIR: $(SRC_DIR)""
@echo ""Current directory: $$(pwd)""

3.3. 「安全なゲートウェイ」としてのMCPの設定

　MCPは、AIエージェントが外部システムへ安全にアクセスするための通信規格で、AIには必要最小限のアクセス権のみを付与し、重要な操作には必ず人間の承認ゲートを設けます［7］。今後、企業が従業員にAI駆動開発環境を提供する際の標準的な方法になると考えられています。

　本検証では、「HubSpot CRM」と「Databricks SQL」を操作するためのMCPを設定しました。.vscode/mcp.jsonにこの設定を定義します。このJSONファイルの中身は以下のようになります。

注意：以下の@hubspot/mcp-server@0.3.3はVS Codeのバージョンによっては動作しない場合があるようです。

{
  ""servers"": {
    ""uc-genie-mcp"": {
      ""command"": ""npx"",
      ""args"": [
        ""mcp-remote"",
        ""https://adb-***.*.azuredatabricks.net/api/2.0/mcp/genie/***"",
        ""--header"",
        ""Authorization: Bearer dapi***""
      ]
    },
    ""hubspot"": {
      ""command"": ""npx"",
      ""args"": [""@hubspot/mcp-server@0.3.3""],
      ""env"": {
        ""PRIVATE_APP_ACCESS_TOKEN"": ""pat-***""
      }
    }
  }
}

4. AI駆動開発の全体アーキテクチャー

　これまで解説した要素を統合すると、本検証で構築したAI駆動開発の全体アーキテクチャーは以下の3つの環境で構築されます。

    業務用ローカル環境（図中のAI駆動ツール）：VS Code＋GitHub Copilotが開発ツール兼クライアントとして機能します。ここでカスタム命令、プロジェクトドキュメント（README.md、Makefileなど）、MCP設定（.vscode/mcp.json）を配置します。AIエージェントはこれらを参照しながら、MCPを通じて外部システムと連携します。
    サードパーティーMCP（図中のCRMツール）：HubSpot MCPのように、ベンダーが提供するMCPサーバーです。
    独自データベース用MCP（図中のデータプラットフォーム）：Databricks Genie MCPのように、自社データベースへのアクセスを提供します。

5. まとめ：AI駆動開発の導入に向けて

　本稿では、AI駆動開発の実証実験を通じて、カスタム命令、プロジェクトドキュメント設計、MCPといった中核的要素を解説しました。導入にはチームとエンジニア個人の役割分担が重要です。

　チームは、AI駆動開発ツールの標準化（VS Code＋GitHub Copilotなど）、MCP設定の統一管理（セキュリティポリシー準拠）、そして成功事例やテンプレートを「Git」で共有するナレッジ基盤の整備を担います。一方、エンジニアは各プロジェクトでREADME.mdやMakefileを整備し、AIが業務コンテキストを理解できる環境を構築します。その上でAIと対話しながら開発を進め、生成されたコードのレビュー・承認・デプロイまでの最終責任を負います。

　ぜひ、あなたの開発チームでもAI駆動開発の導入を検討し、業務効率化と品質向上を実現してください。

参考文献・ソース
［1］：AI-Driven Innovations in Software Engineering: A Review….

［2］：Artificial Intelligence in Software Engineering: A Systematic Exploration…（ResearchGate）.

［3］：AI-Driven Software Development: Opportunities and Good Practices（Diva Portal）.

［4］：Peng, Sida, et al. ""The impact of ai on developer productivity: Evidence from github copilot."" arXiv preprint arXiv:2302.06590 (2023).

［5］：AI-Driven Development Life Cycle（AWS DevOps Blog）.

［6］：GitHub Copilot のリポジトリー カスタム命令を追加する

［7］：Model Context Protocol.

内田匠
インキュデータ株式会社 R&D室
京都大学卒業後、Web広告代理店での広告設計と効果改善業務、リクルートでのデータサイエンティスト・AIエンジニアとしての経験を経て、2021年よりインキュデータに参画。筑波大学社会人大学院でレコメンドAIの研究を経て博士（システムズ・マネジメント）を取得し、現在は筑波大学非常勤講師。インキュデータでは、生成AIを活用した新規事業および業務効率化の検討など、データとAIに関する専門性を生かしたプロジェクトを推進。",[],[]
「Windows 11」でタスクマネージャーが増殖？--プレビュー更新プログラムに潜むバグ（ZDNET Japan）,https://news.yahoo.co.jp/articles/44825aeace76191389b36367bf7fcec31ce656d3,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20251104-35239991-zdnet-000-1-view.jpg?exp=10800,2025-11-04T06:54:00+09:00,2025-11-04T06:54:00+09:00,ZDNET Japan,zdnet,ZDNET Japan,1675,"提供：Screenshot by Lance Whitney/ZDNET
「Windows」のタスクマネージャーは、実行中のアプリ、サービス、プロセスを表示し、どれがメモリーを大量に消費しているかを確認して、必要に応じて終了できる便利なツールである。しかし、現在タスクマネージャーに影響を及ぼす新たな不具合が発生しており、ツール自体が通常以上にメモリーを消費する原因となっている。

　この不具合は、Microsoftが米国時間10月28日に公開した「Windows 11」向けの10月のプレビュー更新プログラムに起因している。このアップデートでは、スタートメニューなどの主要機能が強化され、複数のバグが修正されたが、同時にタスクマネージャーに新たな不具合が持ち込まれた。Windows Latestがこの問題を報告している。

　このアップデートをインストールした環境では、タスクマネージャーを起動すると、現在実行中のアプリやバックグラウンドプロセスの名称、CPUやメモリーの使用状況などが表示される。通常であれば、作業が終わった後にウィンドウ右上の「×」をクリックしてタスクマネージャーを終了する。しかし、問題はこの終了操作にある。

　「×」をクリックしても、タスクマネージャーは完全には終了せず、メモリー上で動作を続ける。再度タスクマネージャーを開くと、新たなインスタンスが起動する。これを繰り返すと、開くたびに新しいインスタンスがメモリー上に展開され、最終的には複数のタスクマネージャーが同時に動作し、システムリソースを圧迫する事態となる。

　実際に、「KB5067036」プレビュー更新プログラムをインストールしたWindows 11のノートPCでこの現象を確認した。タスクマネージャーを開いて閉じ、再度開くという操作を繰り返すと、最終的に20以上のインスタンスが起動していた。これを確認するには、タスクマネージャー内の「バックグラウンドプロセス」セクションまでスクロールし、複数の「Task Manager」エントリーを探せばよい。

　なお、このアップデートはプレビュー更新プログラムと呼ばれるもので、翌月の更新プログラムの予告版として提供される。今回の場合は11月のアップデートに先駆けたものであり、主に技術者や開発者、上級ユーザー向けに公開されている。プレビュー更新プログラムは任意であり、強制的に適用されるものではない。そのため、インストールする際には一定のリスクを伴う。

　この不具合に遭遇した場合、まずは全てのタスクマネージャーのインスタンスを終了させる必要がある。手動で行う場合は、各インスタンスを右クリックし、表示されるメニューから「タスクの終了」を選択する。より迅速に処理するには、管理者権限でコマンドプロンプトを開き（検索フィールドに「cmd」と入力し、「管理者として実行」を選択）、「taskkill /im taskmgr.exe /f」のコマンドを入力する。この操作により、全てのインスタンスが終了したとの通知が表示されるはずである。

　そもそもこの不具合を回避するには、タスクマネージャーを「×」で閉じるのではなく、タスクマネージャー自身から終了させる必要がある。具体的には、バックグラウンドプロセスの一覧から「Task Manager」のエントリーを右クリックし、「タスクの終了」を選択する。

　一般的なユーザーが1日に何度もタスクマネージャーを開閉することは少ないかもしれない。しかし、特定のアプリやプロセスのトラブルシューティングを行っている場合や、システムのパフォーマンスやメモリー使用量を測定している場合には、このバグの影響を受ける可能性がある。Microsoftはこの問題を11月11日の更新プログラムで修正する見込みである。それまでは、タスクマネージャーのインスタンスが増殖しないよう、注意が必要である。

この記事は海外Ziff Davis発の記事を4Xが日本向けに編集したものです。",[],[]
