headline,mainEntityOfPage,image,datePublished,dateModified,author,media_en,media_jp,str_count,body,images,external_links
"AI開発の「高すぎる壁」を壊す──元OpenAI CTOが3,000億円調達で放つ新兵器（AMP［アンプ］）",https://news.yahoo.co.jp/articles/8e15ad4431756f1b76b65395a99a03a69af52d6c,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20260112-00010000-ampreview-000-1-view.jpg?exp=10800,2026-01-12T06:00:10+09:00,2026-01-12T06:00:10+09:00,AMP［アンプ］,ampreview,AMP［アンプ］,4110,"コードを書いてLLMを鍛える──Tinkerが下げる参入障壁
AI開発の「高すぎる壁」を壊す
AI活用が企業の競争力を左右する時代になったが、自社専用のAIモデルを構築するには、多額の資金と専門家チームが必要だ。フルスクラッチでAIモデルを訓練するには数億円規模の投資が必要になることも珍しくない。多くのスタートアップや中小企業は、この高額な参入障壁を前に、汎用的な市販のAIサービスに頼らざるを得ない状況が続いている。しかし、この常識を覆す動きが始まりつつある。

Thinking Machinesが発表したTinkerは、こうしたハードルを大幅に下げるツールだ。同社は、ChatGPT開発企業であるOpenAIの元最高技術責任者（以下、CTO）だったミラ・ムラティ氏が立ち上げたAIスタートアップであり、動向が注視されている注目株の1つ。

Tinkerはドラッグ&ドロップで操作できる簡易ツールではなく、Python言語を使って損失関数や学習ループを細かく設計できる開発者向けのAPI。研究者や開発者が「どのように」AIを訓練するかをコントロールする仕組みを提供する。

具体的には、forward_backwardやsampleといった基本的な命令を組み合わせることで、さまざまな学習方法を自由に設計できる仕組みだ。対応するAIモデルの幅も広く、小規模なものから、複数の専門知識を持つAIを組み合わせたQwen-235B-A22Bのような大規模モデルまで扱える。モデルの切り替えも簡単で、Pythonコード内でたった1カ所の文字列を変更するだけで完了する。

さらに、LoRAという効率的な技術を採用している点も重要だ。これは、AIモデル全体を学習し直すのではなく、必要な部分だけを調整する手法で、計算負荷を大幅に軽減できる。複数の学習作業で同じ計算資源を共有できるため、コスト削減にもつながる仕組みとなっている。

学習処理そのものはThinking Machinesが管理するクラウドインフラ上で実行される。スケジューリング、リソース割り当て、障害復旧といったインフラ管理の煩雑な作業は同社が引き受けるため、研究者やエンジニアはアルゴリズムの改良やデータの選定といった本質的な開発作業に集中できる。従来であれば、分散学習環境の構築だけで数週間から数カ月を要することも珍しくなかったが、Tinkerを使えば即座に大規模な学習を開始できる。

Hugging FaceのTrainer APIやOpenAIの微調整エンドポイント、MosaicMLのComposerといった、既存ソリューションと比較した際のTinkerの特異点は、低レベルの基本コマンドを公開している点である。これにより、既存の手法に縛られることなく、新しい学習手法を自由に試すことが可能だ。さらに、同社はオープンソースのライブラリ「Tinker Cookbook」も公開し、一般的な学習手法の実装例を提供。これにより、ゼロから実装する必要がなく、既存の手法を土台に独自の改良を加えていくことが可能となる。
大学研究が証明──15%から50%へ精度3倍の衝撃
Tinkerの本格稼働前から、複数の研究機関がこのAPIを使った実験を進めてきた。その成果は、理論だけでなく実用面でも注目に値する。

プリンストン大学のチームは、数学の定理を自動的に証明するAIモデルの訓練にTinkerを活用した。通常、AIモデルの精度を高めるには大量のデータが必要とされるが、同チームはTinkerとLoRAという効率的な学習手法を組み合わせることで、全データのわずか20%を使うだけで、従来の全パラメータ学習と同等の性能を達成。MiniF2Fと呼ばれる数学ベンチマークでは88.1%の正答率を記録し、自己修正機能を加えた場合は90.4%まで向上した。この数値は、より大規模なクローズドモデルを上回る結果だという。

化学分野でも劇的な成果が報告されている。スタンフォード大学のロツコフ研究室は、化学物質の名称から分子式への変換という課題に取り組んだ。TinkerでLLaMA 70Bモデルに強化学習を適用した結果、正答率は15%から50%へと3倍以上に跳ね上がった。研究チームは、「大規模なインフラ支援がなければこの改善は実現できなかっただろう」と述べており、Tinkerの貢献を高く評価している。

一方、バークレー大学のSkyRLグループは、複数のAIが協調しながら学習する複雑な実験を行った。AIが複数回のやり取りを重ねながらツールを使いこなす訓練プロセスを構築したが、これもTinkerの柔軟性があってこそ実現できたという。

また、Redwood Researchは、Qwen3-32BモデルをAI制御という特定の課題に特化させる訓練を実施。同組織の研究者エリック・ガン氏は、複数のコンピュータにまたがる大規模な学習環境の構築が常に大きな壁だったと指摘したうえで、「Tinkerがなければこのプロジェクト自体に取り組むことはなかっただろう」と述べている。

数学的推論から化学反応の予測、複雑な制御タスクまで、多様な分野で成果を上げている事実は、汎用性の高さを物語る。特に注目すべきは、データ量を大幅に削減しながら高い精度を維持できる点で、これは資金やデータに限りがある企業・組織にとって大きな利点となる。
元OpenAI CTOが仕掛ける逆襲──クローズドへのアンチテーゼ
Thinking Machinesは、OpenAI 元CTOのムラティ氏が2024年9月に同社を離れた後、2025年2月に設立したスタートアップだ。

2025年7月、製品もなく売上もない創業直後の企業としては異例の規模となる20億ドル（約3,000億円）の資金調達を発表、評価額は120億ドル（約1兆8,000億円）に達した。この調達ラウンドはベンチャーキャピタルのAndreessen Horowitzが主導し、半導体大手のNVIDIAをはじめ、Accel、ServiceNow、AMDなど名だたる企業が参加した。

シリコンバレー史上最大級のシード資金調達が実現した背景には、AI業界における人材争奪戦の激化がある。ムラティ氏はOpenAI在籍中にChatGPTや画像生成モデルDALL-Eの開発を監督し、より人間らしい思考プロセスを持つGPT-4oの開発にも重要な役割を果たしてきた実績を持つ。さらに同社チームには、研究責任者のバレット・ゾフ氏や共同創業者のジョン・シュルマン氏といった実力者が名を連ねるだけでなく、約3分の2が元OpenAI従業員で構成されている点が異例の評価額を後押しした。

Thinking Machinesが掲げる企業理念は3本の柱から成る。第一に「ユーザー固有のニーズに合わせたAIシステムの構築支援」、第二に「信頼性の高い安全なAI基盤の確立」、そして第三に「モデル、コード、研究のオープンリリースによるオープンサイエンスの推進」だ。

特に最後の点は、大規模AI企業がクローズドな開発を進める中で際立つ姿勢といえる。ムラティ氏自身も、資金調達発表時に「数カ月以内に最初の製品を発表する。それには重要なオープンソース要素が含まれ、カスタムモデルを開発する研究者やスタートアップにとって有用なものになる」と明言。また同社設立時には「インフラの信頼性、効率性、使いやすさが研究生産性を大きく左右する」とし、長期的視点での開発インフラ構築を重視する姿勢を示していた。
中小企業でも手が届く──3つの壁を崩すTinkerの実力
Tinkerのような開発基盤が登場したことで、日本の中小企業にも自社専用AIを構築するチャンスが広がりつつある。これまで多くの企業が直面してきた壁は主に3つ。

第一は初期コストの高さ。GPU調達やクラウド設定、セキュリティ対策まで含めると最初の一歩はかなり重労働となる。第二は人材不足。学習処理の仕組みをいじるには分散処理や障害監視、ログ管理といった専門スキルが求められる。第三はスピードの問題で、やってみないと分からない状況にもかかわらず、1回の実験に多くの時間がかかり、成果が明確になる前に諦めるケースも少なくない。

Tinkerが変えるのは、この構図そのものだ。分散実行やGPU調達、障害復旧やログ管理といった煩雑な作業はクラウド側が引き受けるため、利用者は「何を、どう学ばせるか」というアルゴリズムの設計とデータの選定だけに集中できる。結果として、概念実証（以下、PoC）から検証、本番展開までのサイクルが小さく速く回るようになる。

具体的な応用例を見てみよう。カスタマーサポート部門では、自社の過去の問い合わせ履歴や回答ナレッジを学習させたメール・チャット対応AIを構築できるはずだ。見積もり業務では、過去案件の受注確度データを使った確度予測モデルで、営業リソースの最適配分が可能になる。製造業なら、設備の稼働ログと故障記録から予知保全モデルを訓練し、突発的なダウンタイムを削減することも不可能ではない。人事部門では、採用データからマッチング精度を高めるモデルを作り、早期離職リスクの低減につなげることもできるだろう。

実装までの道筋も明確だ。従来は「1〜2カ月（4〜8週間）」が一般的だったPoCを、Tinkerの分散運用の肩代わりとLoRA前提の軽量微調整によって、同等の判断を「概ね2〜4週間」で下せる体制に近づけられる。スコープを絞り、データ準備と評価をテンプレ化できている場合は「1〜2週間」まで圧縮できる余地もある。成功したら、同じ方法で他部署・他業務へ横展開しやすい点も見逃せない。

従量課金とLoRAによるコスト管理も可能で、日本の中小企業にとって現実的な選択肢となりつつある。元OpenAI CTOが仕掛けるAI民主化の波は、どこまで広がるだろうか。
文：細谷 元（Livit）",[],[]
