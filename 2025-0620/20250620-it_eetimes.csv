headline,mainEntityOfPage,image,datePublished,dateModified,author,media_en,media_jp,str_count,body,images,external_links
「エッジでLLM」を実現するNXPの戦略　鍵はKinara買収とRAG（EE Times Japan）,https://news.yahoo.co.jp/articles/b23760cc343e1777bee49ad06c657c487f094c0a,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20250620-00000025-it_eetimes-000-1-view.jpg?exp=10800,2025-06-20T18:30:12+09:00,2025-06-20T18:30:12+09:00,EE Times Japan,it_eetimes,EE Times Japan,3179,"NXPのツールフロー「GenAI Flow」［クリックで拡大］ 出所：NXP Semiconductors
NXP Semiconductors（以下、NXP）のAI戦略／技術部門担当グローバルディレクターを務めるAli Ors氏は、米国カリフォルニア州サンタクララで2025年5月20～22日に開催された「Embedded Vision Summit 2025」において、エッジデバイス上で大規模言語モデル（LLM）推論を実現するという同社の戦略の詳細を説明した。
KinaraのLLM特化型チップを活用
NXPのアプリケーションプロセッサ「i.MX-8M+」および「i.MX-95」はいずれもNPU（Neural Processing Unit）を搭載している。i.MX-95は、自動車や産業、スマート家電などのアプリケーションにおいて約40億パラメータ未満のLLM向け推論に対応するという。より規模の大きいLLMの場合には、外部アクセラレーターチップが必要になる。

　NXPは2025年2月、AIアクセラレーターチップのスタートアップKinaraを買収することを発表した。KinaraのLLM特化型チップ「Ara-2」は、最大40eTOPS（40TOPSのGPUに相当。KinaraのアクセラレーターはMACアレイではないので、同社はTOPSという単位での数値を提示していない）を提供する。Ara-2は、最大16GB（ギガバイト）のLPDDR4を備え、高速データレート転送が可能なDDRをサポートする。NXPアプリケーションプロセッサや他のCPUなどのホストとは、PCIe／USB経由で接続するという。

　Ors氏は「現在i.MX-8M+またはi.MX-95を使用している機器では、Ara-2を追加することでより大型のLLMをサポートできる。さらなる拡張が必要な場合は他のアクセラレーターも使用できる。Ara-2は、複数のデータストリームやモデルを同時に実行できるので、エージェントベースのワークロード向けとして不可欠だ。このような機能は、NXPにとって特に魅力的だ」と述べている。

　ディスクリートAIアクセラレーターと連携するということは、アプリケーションプロセッサを単体で使用する場合よりも優れた性能を提供できるということだ。また、新しいオペレーター／モデルなどの変化するニーズや、エージェントAI／フィジカルAIといった新しいパラダイムに適応しうる汎用性も提供できる。

　KinaraのこれまでのAIアクセラレーターチップ出荷数量は約50万個に達していて、その大半は組み込みアプリケーションやLenovoのAI PCなどを含むアプリケーションに向けたパイロットプログラムやユースケース評価で使われている。

　Kinaraは既にPoC（Proof of Concept）を完成させ、NXPのアプリケーションプロセッサ上での実証を行っているという。Ors氏は「これは、ソフトウェア統合のレベルを示しているのではなく、単に両社のソフトウェアスタックが意図した通りに機能するということを示す」と述べている。

　Ors氏は、プレゼンを終えた後に行った米EE Timesのインタビューの中で「顧客確保を追求し、Kinaraと共に勝利を目指していく中で、Kinaraが複数の大手企業の重要なデザインウィンを獲得するのを目の当たりにした。これはつまり、同社の技術が独立検証されていたということだ」と述べる。
RAGや音声UIモデルなどエッジ特化のツールを用意
NXPは、Kinaraの買収に加えて、LLMと生成AIをエッジで実行するための新しいツールフローについても発表している。「GenAI Flow」は音声認識や音声合成（text-to-speech）モデルを含む機能的なライブラリを備える。Ors氏は「組み込み機器はシステムにキーボードやディスプレイを備えていない場合があるので、音声UIに関連するモデルは非常に重要だ」と指摘する。こうしたモデルは、i.MX95の場合はアプリケーションプロセッサのNPUで、より旧型の場合はCPUでなど、ホスト上でも実行可能だという。

　Ors氏は「GenAI Flowはワークロードの一部をコア間で分割できるが、個々のモデルを複数のコアに分割するのは非常に困難だ」と述べる。

　また同氏は「モデルは動的なもので、そのバランスのとり方に注意する必要がある。グラフを分割し始めると必ず、それまでは存在していなかったボトルネックが発生する。何度もあちこちデータを受け渡していると、計算よりも受け渡しに多くの時間を費やすようになる可能性がある。このため、何か利用できるものを探してそのコアにデータを送信するよりも、動作は遅くても全てを1カ所で実行する方が良いかもしれない」と述べる。

　GenAI Flowには、RAG（Retrieval-Augmented Generation：検索拡張生成）向けのツールもある。RAG技術は、事実に基づいていることが重要視される自動車や産業、ヘルスケアなどのエッジユースケースにおいて、LLMにコンテキストを与える。

　「畳み込みニューラルネットワーク（CNN）の場合、多くの製品化がカスタムモデルで行われる。LLMではカスタムモデルのコストがかなり大きく、より多くのデータや計算、特殊な専門知識が必要とされ、それを確保することは非常に難しい。専門知識や計算のコストも高く、特にデータ収集のコストは非常に高い」（Ors氏）

　こうした理由から、ほとんどのエッジ顧客がLlamaのようなオープンソースLLMをRAGなどの技術と組み合わせて利用する方を好む。RAGによって、LLMは事実に基づいたコンテンツのデータベースにアクセスできる。エッジアプリケーションの場合、RAGデータベースのサイズは数百キロバイトで、アプリケーションプロセッサ上で動作する。GenAI Flowは、NXPハードウェア向けにあらかじめ最適化されたオープンソースモデルのライブラリを備えている。

　Ors氏は「これらのモデルは現在もありとあらゆるトレーニングの最中だが、RAGはそのトレーニングにコンテキストを提供できる」と述べる。

　「LLMが医療機器上で実行されている場合は、その機器の使用方法に関する全てのマニュアルを組み込める。そうすればLLMに質問したときに、インターネットで見た心電図の画像を参考にするのではなく、その医療機器が実際にやろうとしていることに基づいた応答を得られる。RAGは、事実に基づいて応答することが非常に重要な用途において、オープンソースLLMにコンテキストを与える」（Ors氏）

　重要なのは、RAGはモデルを修正しないということだ。Ors氏によれば、今後のAI規制を懸念する顧客の多くがそれを望んでいるという。認証されたモデルにRAGを介してコンテキストを与える際に、追加の認証を受ける手間を省略できる可能性があるからだ。

　全体としてGen AIは、LLMをエッジに導入して顧客の摩擦を取り除くことを目的にしている。エッジにおける完全なエージェントAIの実現はまだ先のことだが、Ors氏によるとこの技術はエッジユースケースにおいて大きな可能性を秘めているという。

　「生産現場で防犯カメラが事故を捉えたとき、エージェントAIが現場の状況を読み解き、管理者や救急隊を呼び、報告書の作成を開始し、関連する機械を停止させる。これらは全て実現可能なことだ」（Ors氏）

※米EE Timesの記事を翻訳／編集したものです。
EE Times Japan",[],[]
