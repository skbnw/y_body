headline,mainEntityOfPage,image,datePublished,dateModified,author,media_en,media_jp,str_count,body,images,external_links
今すぐLLMを比較したい　性能が一目で分かる「リーダーボード」はどれ？（TechTargetジャパン）,https://news.yahoo.co.jp/articles/fae07a9591aae4b93bc0b7844787216360a0c3dc,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20250810-00000005-zdn_tt-000-1-view.jpg?exp=10800,2025-08-10T20:00:12+09:00,2025-08-10T20:00:12+09:00,TechTargetジャパン,zdn_tt,TechTargetジャパン,2856,"（写真：TechTargetジャパン）
大規模言語モデル（LLM）の性能は日々更新され、さまざまなベンチマークが存在する。そのため、ベンチマークテストの実行やツールの比較は簡単ではない。本稿は、ベンチマークやLLMを比較検討するためのリーダーボード（ランキング表）を掲載しているWebサイトを紹介する。

　LLMやベンチマークの情報を収集するのであれば、AI（人工知能）モデルやデータセットの共有・公開サービス「Hugging Face」を訪れるのは一つの手だ。Hugging Faceは、エンドユーザーがLLMの性能をランキング形式で掲載するリーダーボードを提供している。注目すべきリーダーボードには以下がある。

・Big Benchmarks Collection
・Chatbot Arena LLM Leaderboard
・OpenVLM Leaderboard
・GAIA Leaderboard

　ベンチマークを選定するに当たっては利用料がどの程度発生するかも考慮に入れる必要がある。オープンソースソフトウェア（OSS）であれば、ソースコードが公開されており、無料または安価な費用で利用できる。エンドユーザーの手元にある端末で実行可能だ。一方、ソースコードが公開されていないクローズドソースソフトウェアはその性能やセキュリティといった要素の評価が難しい上、利用料も発生する。

　Hugging Face以外でベンチマークの評価、測定値といった情報を入手したい場合、以下の情報が役に立つ。

・AIアプリケーション開発ツールVellumのWebサイトが提供する「LLM Leaderboard」
・・2024年4月以降に公開された、主要なLLMベンチマークの最新版の性能をリーダーボードで評価している。Vocifyが運営している。

・「SWE-bench」のWebサイト
・・SWE-benchは、ソースコード管理ツールGitHubに存在するIssue（課題）から作られたデータセットを基にしたベンチマークだ。Issueを自動的に解決するLLMの能力を測定する。SWE-benchのWebサイトでは、リーダーボードでLLMの問題解決率を掲載している。

・Berkeley Function-Calling Leaderboard
・・LLMが、サードパーティーツールをどれだけ正確に選び、結果を解釈できているか、能力を測定し、評価を掲載している。

・「LiveBench」のWebサイト
・・LiveBenchは、推論、データ分析、数学、コーディング、言語理解、指示への追従の6つの分野、17のタスクで構成されるベンチマークだ。LLMの推論や問題の処理能力、簡単なコーディングタスクの性能を測定し、リーダーボードに掲載している。

・LLMベンチマーク「Humanity's Last Exam」の紹介サイト
・・AI技術の安全性に関するNPO（非営利団体）CAIS（Center for AI Safety）とAIベンダーScale AIは、ベンチマーク「Humanity's Last Exam」を提供している。同ベンチマークは、人間の高度な知識でAI技術の限界を試すことを目的に設計された。Scale AIが運営するWebサイト「scale.com」では、Humanity's Last Examを使ったLLMの性能の評価をリーダーボードに掲載している。
ベンチマークの限界は？
LLMのベンチマークには、組織が抱える特定の課題を反映できないという制約がある。代表的な例が「HumanEval」だ。HumanEvalは、プログラミング言語「Python」で記述したソースコードをどの程度実用的かつ正確に生成できるかを評価する。

　HumanEvalを使えば、単純な英語のプロンプト（質問や指示）からソースコードをどの程度の精度で生成できるかを評価することは可能だ。一方、以下の要素はHumanEvalの評価対象外だ。

・3D（3次元）画像の処理能力
・すでにあるソースコードのリファクタリング（ソースコードの動作を変えずに内部構造を整理すること）
・ソースコードの読みやすさ
・CI/CD（継続的インテグレーション／継続的デリバリー）ツールとの連携
・Python以外のプログラミング言語で記述したソースコードの生成

　一般的に、ベンチマークの大半は、ツールの処理速度やレイテンシ（遅延）、システムやセキュリティといった運用面の課題は評価の対象外だ。LLMベンチマークであれば、クラウドサービス、自社のサーバあるいは端末でLLMを稼働させた場合、性能がどう異なるのかは評価できない。例としてHumanEvalは、GUI（グラフィカルユーザーインタフェース）で生じる画面サイズ変更時のエラー、フォントの視認性、アクセシビリティー（利用のしやすさ）といった項目は評価対象外だ。

　AIモデルが自律的に意思決定して行動する「AIエージェント」についても、信頼性の高い網羅的な評価手法は存在していない。一部の専門家は、AIエージェントが自律的にソースコードをソースコード管理ツールに登録したり、データベースを更新したり、CIツールを実行して本番環境にソースコードを反映させたりするようになると期待している。一方で、AIエージェントに関するベンチマークには、2022年に公開された「MARL-eval」や2024年に公開された「Sotopia-π」があるが、信頼性の確立は途上にある。
オープンアクセスリポジトリarXiv.orgに公開されている論文
MARL-eval：Towards a Standardised Performance Evaluation Protocol for Cooperative MARL
Sotopia-π：SOTOPIA-: Interactive Learning of Socially Intelligent Language Agents

　LLMは言語の翻訳、数学的な問題への回答、3D画像の形状調整、データの中にある不適切な項目の識別、事実の記憶、段落の構成など、単一のモーダル（データ形式）を処理するのが得意だ。

　現行のLLMのベンチマークでは、感情的知性、人間性、誠実さといった要素の測定は難しい。そのため、どのLLMを、誰が、いつ、どの業務に使うかは、各組織が自ら判断する必要がある。多角的な評価指標を持つことで、自組織にとって適切なLLMを見極める助けになる。

本記事は米国Informa TechTargetの記事「Benchmarking LLMs: A guide to AI model evaluation」を翻訳・編集したものです。一部、翻訳作業に生成AIを活用しています。
TechTargetジャパン",[],[]
