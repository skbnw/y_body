headline,mainEntityOfPage,image,datePublished,dateModified,author,media_en,media_jp,str_count,body,images,external_links
「FPGAの世界一に」　Altera、エッジ向けAgilexの受注開始（EE Times Japan）,https://news.yahoo.co.jp/articles/34622fb0860c2293cac6fdfb403ff70c46c09b93,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20250404-00000073-it_eetimes-000-1-view.jpg?exp=10800,2025-04-04T18:30:12+09:00,2025-04-04T18:30:12+09:00,EE Times Japan,it_eetimes,EE Times Japan,1477,"左＝Alteraの戦略、右＝Alteraの製品ポートフォリオイメージ［クリックで拡大］ 出所：Altera
Alteraは2025年3月、エッジ向けの新しいFPGAと事業戦略についての記者会見を実施した。AlteraがIntelの1部門から独立子会社になった2024年1月1日以降、日本では初めての記者会見だった。
左＝Alteraの戦略、右＝Alteraの製品ポートフォリオイメージ［クリックで拡大］ 出所：Altera
エッジ向け「Agilex 3」の受注開始
Altera CEOのSandra Rivera氏はIntelからの独立を受けて「Alteraは業界で唯一、エッジ向けから最高性能のシステムまでのポートフォリオを持つ独立系のFPGAサプライヤーだ」と強調する。「Alteraのビジョンはさまざまな規模感やユースケースに対応した使いやすいソリューションを提供することである。将来に向けては、ウォーターフォールの最初から最後まで徹底的に自社のIP（Intellectual Property）を再利用し、業界をリードするFPGAを提供することで、世界一のFPGAプロバイダーになりたい」と述べた。

　Alteraは、消費電力やコスト、サイズの制約が厳しいエッジ向けの製品としてこれまで「MAX」「Cyclone」を展開していたが、2025年3月にドイツ・ニュルンベルクで開催された展示会「embedded world 2025」に合わせて新たに「Agilex 3」の受注開始を発表した。第2世代のHyperflexアーキテクチャを採用し、従来品である「Cyclone V」と比べて消費電力を最大38％低減、性能は最大1.9倍に向上した。

　ロジックエレメント（LE）数は2万5000～13万5000で、Arm-Cortex A55コアを2つ搭載。トランシーバー転送速度は12.5Gbps、AI処理性能は最大2.8TOPSだ。

　Rivera氏は「Agilex 3は高い計算能力をエッジでも提供し、遅延も最小限に抑える。自動車や産業IoTといったスピードが求められる用途には最適だ。また、柔軟性が高いため、特定のタスクに対して適切なリソース構成が実現する。セキュリティ管理機能『セキュア・デバイス・マネジャー（SDM）』によってデータ処理の安全性も確保している。Agilex 3は組み込み機器市場におけるAlteraの地位を確固たるものにしてくれる」と述べた。

　用途としては、データセンターの電力マネジメントやシステムモニタリング、ロボット制御やスマートファクトリー、モビリティの物体検知や衝突回避、画像診断を行う医療機器などを想定しているという。
技術者不足の日本　ユニバーシティープログラムを検討
同記者会見に登壇したAltera Japan 社長のSam Rogan氏は日本の半導体業界について「有線／無線通信、医療機器や産業機器、自動車、民生機器、計測器まで全ての市場があることが魅力的だ。今後はAI市場も立ち上がるだろう。これまでAI市場では学習が注目されていたが、エッジでの推論も重要になってくる。それを得意とするのがAgilex 3だ。Alteraとしてもエッジでの推論に注力したい」と語った。

　さらにRogan氏は「この数年、各省庁が日本の半導体業界を盛り上げようとエネルギーを使っているが、エンジニアが不足している。Alteraでもユニバーシティープログラムを検討している」とした。
EE Times Japan",[],[]
KOKUSAI ELECTRIC、米国デモセンター新設へ（EE Times Japan）,https://news.yahoo.co.jp/articles/977f3296137baaefcb57c08146bd7e5c1dae1fff,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20250404-00000070-it_eetimes-000-1-view.jpg?exp=10800,2025-04-04T09:03:33+09:00,2025-04-04T09:03:33+09:00,EE Times Japan,it_eetimes,EE Times Japan,665,"（写真：EE Times Japan）
2026年9月に稼働予定、投資額は約200億円

　KOKUSAI ELECTRICは2025年3月31日、米国オレゴン州に「米国デモセンター」を新設すると発表した。米国半導体デバイスメーカー向けの「デモ評価機能」および「サポート体制」を強化するのが狙い。2026年9月の稼働を目指す。

　半導体デバイスは、データセンター用装置向けを中心に、需要が拡大している。また、最先端の領域では、微細加工技術の進化に加え3次元積層技術などの導入により、デバイスの構造も複雑になってきた。このため半導体デバイスメーカーは、半導体製造装置メーカーに対して、デモ評価などを通じた開発支援を強く求めているという。

　KOKUSAI ELECTRICはこれまで、富山事業所で米国半導体デバイスメーカーに向けたデモ評価を行ってきた。新たに米国デモセンターを新設することで、これまで以上に顧客が抱える課題を迅速かつ正確に把握することが可能となる。これにより、顧客に対する開発支援の取り組みを一段と強化していく。

　米国デモセンターは2025年9月に着工し、2026年9月に竣工予定。敷地面積は約3万4000m2で、投資額は約200億円となる。今後の需要動向に応じて、設備の拡張やサポート体制の拡充も検討していくという。

　なお、韓国でもグループ会社のKook Je Electric Korea平澤工場で、デモ評価エリアを拡張するなど、顧客に対する開発支援体制を強化している。
EE Times Japan",[],[]
NVIDIAはどのようにHopper推論性能を30倍向上させたのか（EE Times Japan）,https://news.yahoo.co.jp/articles/ff70b9e14ba084cff556020b3f4fbf9444a48a52,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20250404-00000069-it_eetimes-000-1-view.jpg?exp=10800,2025-04-04T09:03:04+09:00,2025-04-04T09:03:04+09:00,EE Times Japan,it_eetimes,EE Times Japan,3197,"（写真：EE Times Japan）
NVIDIAは、新しいデータセンター推論オーケストレーションソフトウェア「Dynamo」を用いることで、GPUの推論性能を劇的に向上させた。このDynamoは「Triton Inference Server」の後継であり、データセンター事業者が大規模言語モデル（LLM）トークンの生成による収益を最大化できるよう設計されている。「Hopper」世代のGPUでは、既にこの新しいソフトウェアによってトークン/秒/ユーザー（token/s/user）性能が30倍に向上しているという。一体どのように機能しているのだろうか。
出所：NVIDIA
NVIDIAのバイスプレジデント兼ハイパースケール／HPC部門担当ゼネラルマネジャーであるIan Buck氏は「それは、トレードオフの関係にある。ユーザー1人当たりのトークン数と、当社のAIファクトリーからの合計トークン数をトレードオフにできる。GPU上で実行される作業によって、AIファクトリー全体が最適化されている」と述べる。

　このトレードオフのスイートスポットは、AIファクトリーの収益性において非常に重要であり、アプリケーションによって異なる場所に存在する可能性がある。例えば、双方向である必要性がないディープリサーチと、超高速のシングルユーザートークン速度が求められるチャットボットでは、異なる場合があるということだ。

　Buck氏は「どのAIファクトリーも、最高のサービスや最高のユーザーエクスペリエンスを提供したり、GPU効率やトークン当たりのトータルコスト効率、そしてもちろん収益を最大化しようとしている」と述べる。

　既存のAIファクトリーは、かつて1台のGPUサーバでLLMを実行できた初期の展開当時とは大幅に異なり、何十万基ものGPUが複数モデルを実行している可能性がある。Buck氏は「リーズニングのような最先端の推論技術には、数千個規模の“思考”トークンが必要になるだろう。『DeepSeek-R1-671B』は6710億個のパラメータを持ち、出力の生成を開始する前に1万個もの思考トークンを生成する」と述べている。

　「これらのモデルは、AIを全く新しいレベルの知識や有用性、エンタープライズ生産性に到達させるという点で非常に重要であり、われわれにはこの課題に対応することが可能なソフトウェアスタックが必要だ」（Buck氏）

　Buck氏が「AIファクトリーのOS」と表現するDynamoは、データの待ち時間を省くために、大規模GPUフリートを管理できる。

　特に重要なのが、実質的にはモデルのワーキングメモリであるKV（Key-Value）キャッシュだ。これは、会話全体のコンテクストを維持するために、ユーザーが以前に質問したことに関する情報を保存する。最新のAIファクトリーは、ユーザーごとにKVキャッシュを保持する必要があり（例えばChatGPTの1カ月当たりのユーザー数は10億超）、ユーザーのリクエストが正しいGPUに送られるよう、そのキャッシュがユーザーごとにどのGPU上に存在するのかを把握し、必要に応じて遅れることなく変更に対応しなければならない。

　Dynamoは、インテリジェントなルーティングメカニズムを搭載しており、KVキャッシュ値が既にシステム内のどこかに存在する場合に再計算せざるを得なくなるという状況を回避する。KVキャッシュの高いヒット率を実現することで、推論の大幅な高速化が可能になる。
AIモデルを効率的に分割する分散処理
Dynamoの性能向上におけるもう1つの重要な要素となっているのが、分散処理だ。最新のLLMモデルは大きすぎるため、単一のGPUや、1台のGPUサーバ上でも実行できない。Dynamoは、最高性能を実現できるよう、大量のGPU全体でモデルを効率的に分割する設計となっている。

　またDynamoは、入力トークンの処理（プレフィル段階）と出力トークン（デコード段階）の生成とを分割する。ワークロードのこれら2つの部分は十分に異なっているため、個別に実行することで、大きな性能メリットをもたらす最適化を実現することが可能だ。

　Buck氏は「われわれは、Hopperクラスタ上でLlama-70Bを実行し、Dynamoをオフからオンに変更することで、Hopperデータセンターのスループットを倍増させた。これはつまり、顧客にとっては売上高が2倍になるということだ。DeepSeekのような、レイヤー当たり257エキスパートを保有する『MoE（Mixture of Experts／混合エキスパート）』構造を採用するモデルでは、異なるエキスパートを異なるGPU上に分散させることで、30倍の高速化を実現している。このため、われわれにとっては非常に重要なソフトウェアだといえる」と述べている。

　Buck氏が説明しているように、入力トークンは全て同時にモデルに提示されるため、並列処理が可能であり、質問を一度に取り込むことができる。DeepSeekは、生成に関しては自己回帰型で、生成された出力トークンがそれぞれKVキャッシュに追加され、次のトークンが同時に1つずつ生成される。

　Buck氏は「これらの2つの段階を分割することで、入力トークン段階を劇的に圧縮できる。並列処理を行い、高密度FP4演算を実行できるようにして、全ての入力トークンを並列処理するためにモデルを最適化することが可能だ。出力に関しては、可能な限りNVL72ラック全体に分散させることで、可能な限りの高速で実行したいため、NVLink帯域幅と、できるだけ多くのGPUを確保することを重要視している」と述べる。

　システムではかつて、ワークロードの両方の部分で優れた結果が出るようバランスが取られていたが、それは、特にDeepSeekのような超大規模MoEモデルの場合、もはや最善策ではなくなっている。1年前、NVIDIAは16のエキスパートを持つモデルを「大規模」と見なしていたとBuck氏は説明する。DeepSeekには、レイヤーごとに257のエキスパートがある。

　DeepSeek-R1の論文によると、中国のAI企業であるDeepSeekは、プレフィル／入力ステージに32個のGPUを使用し、生成／出力ステージに少なくとも320個のGPUを使用したが、そのためにはNVIDIAのコンピューティングコアをカスタムDMA（Direct Memory Access）エンジンに変換するための独自ソフトウェアを作る必要があった。

　Buck氏は、DeepSeek-R1の推論は、Hopper世代のハードウェアでは1秒当たり約50の思考トークンだったが、次世代の「B200」GPUでは約120思考トークンにまで改善したと説明した。同氏によると、B200の目標は1秒当たり350トークンであるが、「GB300」ではDeepSeek-R1は1秒当たり1000思考トークンを実現し、実質的にリアルタイムで動作するようになるという。
推論は「信じられないほど難しい」
新しいハードウェアによってトークンレートを向上させるが、Dynamoなどの新しいソフトウェアからも多くの革新が生まれる。「Dynamoの使命は、パフォーマンスを向上させる分散化を実現し、インフラ全体のGPU群を管理して、それらを順調に稼働させ続けることだ」とBuck氏は語った。「われわれの使命は、AIファクトリーを徹底的に加速させることだ。推論は信じられないほど難しいのである」と付け加えた。

※米国EE Timesの記事を翻訳、編集しました。
EE Times Japan",[],[]
「AIのスケーリング則はまだ続く」　OpenAIが強調（EE Times Japan）,https://news.yahoo.co.jp/articles/cdad67fc84d0cc5d6834f4ba56db40862466001c,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20250404-00000068-it_eetimes-000-1-view.jpg?exp=10800,2025-04-04T09:01:59+09:00,2025-04-04T09:01:59+09:00,EE Times Japan,it_eetimes,EE Times Japan,3280,"（写真：EE Times Japan）
OpenAIのハードウェア責任者であるRichard Ho氏によると、「DeepSeek-R1」のようなモデルによってトレーニングと推論の計算要件が削減されるという進歩があったにもかかわらず、AIコンピューティングの需要は今後も増え続けると予想されるという。
OpenAIのハードウェア責任者であるRichard Ho氏 出所：Synopsys
Ho氏は、技術者向けイベント「Synopsys Snug」（2025年3月19～20日、米国カリフォルニア州サンタクララ）の基調講演で「蒸留（大規模な事前トレーニング済みモデルの学習内容をより小規模なモデルに転送して使用することで、実行時間やコストを削減する技術）のような技術をベースにすると、より小規模なモデルでも優れた結果が得られることが研究で示されている」と述べた。

　同氏は「業界では、より安価に動作するこうした小型モデルが主流になるかどうかについて、多くの議論が交わされている。これは、スケーリング則＊）が終わったということなのか、それほど多くのコンピューティングが必要なくなったということなのか、それともインフラへの過剰投資ということなのか。スケーリング則は、追加の機能を提供するために（コンピューティングニーズを）拡大し続けるように思えるが、フロンティアトレーニングからポストトレーニングやテスト時のコンピューティングへと移行しているだけである」と述べている。

＊）自然言語処理モデルのパラメーター数（＝モデルのサイズ）や、データセットのサイズ、トレーニングに使用される計算（Compute）量が増えるにつれて、損失（Loss、誤差）が「べき乗則」に従って減少する、という法則（「言語モデルのスケーリング則（Scaling Laws for Neural Language Models）とは？」（＠IT、2023年5月24日）から引用

　Ho氏は、思考の連鎖（CoT：Chain-of-Thought）と推論を、より多くのトークンの生成が必要で、より多くのコンピューティングを使用して、小規模なモデルの性能を劇的に向上させることができる技術として強調した。
言語モデルの開発に不可欠なコンピューティング能力
ここ数年、コンピューティングのスケーリングは言語モデルの開発に非常に重要になっている。OpenAIが2020年に発表した論文では、モデルのトレーニングに使用するコンピューティングを2倍にすると、次の単語を予測する能力が対数的に向上することが示された（この研究は「GPT-3」と「GPT-4」につながっている）

　Ho氏は「コンピューティングの規模が劇的に拡大したことで、コヒーレントなテキスト生成やゼロショット転送（さまざまなタスクに適応する能力）、文脈学習（文脈から新しいタスクをモデル化する能力）、現実世界のさまざまなタスクを有用な方法で実行する能力など、新しいモデルの動作が出現した」と述べている。

　「われわれが考える進歩は、非常に速いペースで起こっている。実際に、指数関数的なレベルで起こっている」と同氏は述べている。

　AIの進化とその社会的影響を研究するための研究機関であるEpoch AIのデータによると、フロンティアモデルのトレーニングに使用されるコンピューティングは、ムーアの法則や低精度のコンピューティング、システム規模の拡大、実行時間の長時間化の可能性によって、2018年までは年間6.7倍増加し、それ以降も年間4倍以上のペースで増加している。
AIは「電気」のようなユーティリティーに
スケーリング則の継続に伴って、AIインフラもスケーリングする必要が生じる。Ho氏によれば、OpenAIは、AIの能力は、電気のような今日のユーティリティーによく似たものになると見ているという。

　「AIはわれわれの日常生活を楽にするのに役立つ。AIを使用するかどうかではなく、どれだけ使用されるのか、そしてサプライヤーが幅広いユーザーや地域で公平なアクセスをいかに確保できるかが重要だ」（Ho氏）

　OpenAIは現在、Microsoftの「Azure」クラウドでモデルAPI（Application Programming Interface）を実行しているが、将来的には5000億米ドル規模の「Stargate」プロジェクトの一環としてデータセンターを運営する予定だという。業界全体では、AIインフラの構築にほぼ1兆米ドルが投じられている。

　Ho氏はAMDの元CEOであるJerry Sanders氏の有名な言葉「Real men have fabs（真の男ならファブを持つ）」を言い換えて、「Real AI labs have data centers（本物のAIラボにはデータセンターがある）」と言った。

　今日のインターネットインフラストラクチャは、Ho氏が「倉庫サイズのコンピュータ」と呼ぶように、既に巨大だが、「今後、AIインフラストラクチャは“地球規模のコンピューティング”に向けて桁違いに大きくなる」と同氏は言う。Ho氏は「大規模なAIトレーニングジョブは今日、さまざまな地域の複数のクラスタにまたがっていて、必要な規模のコンピューティングにアクセスできる」と指摘した。

　これらのコンピュータに搭載されるチップがどのようなものになるかについて、Ho氏は「GPUが大きな部分を占めるだろう」と述べている。「しかし、AI研究室の観点から言えば、モデルやコンパイラ、チップ、システム、カーネルなど全てを搭載したインフラの完全な共同設計を見たいと思う」（Ho氏）

　Ho氏は、OpenAIが独自のAIアクセラレーターシリコンの開発に取り組んでいるという報道を裏付けるかのように、AIアクセラレーターチップは、レイテンシやスループット、メモリ容量、メモリ帯域幅、ダイ間およびチップ間の帯域幅のバランスを必要とするため、「（スタックの中で）変更が最も難しい部分の1つ」だと説明した。

　Ho氏は、「アムダールの法則によれば、全体的な性能の向上を実現するにはスタックのあらゆる部分を改善する必要があるため、われわれのチームは総合的なアプローチを取っている」と語った。

　「ピーク値は多くのデバイスで公表されているが、実際に実現されているものではない。コンパイラが何をしているか、命令がどのように実行されているか、ボトルネックはどこか、待機状態はどこかなど、スタック全体を見ることができて初めて、スループットがどうなるか、レイテンシがどうなるかが実際に分かる。だからこそ、コード設計が極めて重要だと考えている」（Ho氏）
チップ設計期間の短縮も重要に
Ho氏が指摘したもう1つの重要な問題は、AI研究の動きが非常に速く、チップの設計プロセスはそれに比べると遅いことだ（一般的にチップの設計はテープアウトまで18～24カ月かかる）。モデルとチップの組み合わせを最適化することは、ハードウェアにある程度のプログラマビリティがあっても、依然として難しいと同氏は述べる。

　「私の夢は、この長いシリコン設計のタイムラインを短縮することだ」とHo氏は付け加える。「製造プロセスについては多くのことができるとは考えていないが、アーキテクチャからテープアウトへの移行については、確実に対処できるだろう」（同氏）

　Ho氏は、将来的にはチップ設計における物理的な実装と検証の両方にAIが使われるようになるだろうと示唆した。同氏は「AIインフラは迅速に適応する必要がある。従来の方法にとらわれる必要はない」と述べた上で、コンピュータアーキテクチャとシリコンにおける業界の取り組みは「知能（インテリジェンス）の未来を定義する」と語った。

※米国EE Timesの記事を翻訳、編集しました。
EE Times Japan",[],[]
