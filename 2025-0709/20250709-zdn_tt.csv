headline,mainEntityOfPage,image,datePublished,dateModified,author,media_en,media_jp,str_count,body,images,external_links
LLM、どれを使えばいいの？　性能を評価するための「ベンチマーク」とは（TechTargetジャパン）,https://news.yahoo.co.jp/articles/8659862b84763a5cad84910a3dfb658e86ba3f86,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20250709-00000071-zdn_tt-000-1-view.jpg?exp=10800,2025-07-09T20:00:12+09:00,2025-07-09T20:00:12+09:00,TechTargetジャパン,zdn_tt,TechTargetジャパン,2400,"（写真：TechTargetジャパン）
大規模言語モデル（LLM）は、質問への回答、ソースコードの作成、そのテスト方法の提案が可能だ。しかし、LLMが生成する内容が常に信頼できるとは限らない。さまざまなLLMの中から、自組織に最適なものをどう選ぶべきか迷った際に有用なのが、LLMを客観的に評価する「ベンチマーク」だ。LLMのベンチマークは何を調べ、どのように測定するのか。

　LLMのベンチマークは、LLMの性能を評価するための標準化された手順や指標だ。LLMに実行させる一連のタスクを提示し、特定の指標に基づいてLLMのタスク達成能力を評価し、その指標に基づいてスコアを算出する。ベンチマークを利用して、コーディング、推論、文章の要約、読解、事実知識の想起など、さまざまなタスクを実行する能力を測定できる。

　ソフトウェアの品質評価ではメモリの使用量、処理速度、消費電力といった指標に注目する。これに対してLLMを評価する場合は、その問題解決能力に着目する。

　組織はベンチマークを使って、複数のLLMの性能を同一タスクで比較したり、LLMを微調整して性能を向上させる際の指針を得たりすることが可能だ。
　ベンチマークを使ったテストは、以下の順に実行する。

1. セットアップ
ベンチマーク用のデータセットを用意する。このデータセットには、文章の理解や要約、質問への応答の能力を測るためのテキスト文書、プログラミング能力を測るための問題、論理的思考や計算能力を測るための数学の問題などが含まれる。

2.テスト
準備したデータを使い、課題に対するLLMの準備度合いに応じてベンチマークテストを実施する。

・ゼロショット（Zero-shot）
・・タスクの手本を示さずに、LLMの汎用（はんよう）的な知識のみで課題を解かせる評価手法。

・フューショット（Few-shot）
・・プロンプト（質問や指示）内で幾つかの具体例を示し、LLMにタスクの意図を理解させてから課題を解かせる評価手法。

・ファインチューニング（Fine-tuning）済みモデルでの評価
・・特定用途向けの小規模データセットを用いてパラメーターを微調整したLLMを評価する手法。

3. 採点
以下の指標を基に、0～100でLLMの性能を測定する。
・正解率（Accuracy）
・再現率（Recall）
・困惑度（Perplexity）

　ベンチマークでは、特定の能力に焦点を当てることが一般的だが、複数の分野にまたがる場合もある。対象とする分野には、歴史、読解力、数学的推論、科学などがある。このうち読解力については、プロンプトの内容理解、論理的証明、推論といった分野に細分化できる。与えられた文章の段落の中から代名詞の主語を特定する能力や、「熱が出たらどうすればいいのか」「牛乳を常温に何分までなら置いても問題ないのか」といったプロンプトに対して、常識的なアドバイスをする能力を評価するベンチマークもある。

　これらのベンチマークテストには、人間の試験と同様、採点について課題がある。誰が採点しても同じ結果となる問題を出せば、効率よく採点できる。そうではない場合、採点には時間がかかりやすい。そのためベンチマークテストでは、答えが同一となる数学や選択式の問題が選ばれる傾向にある。LLM同士で性能を比較しやすくするために、ベンチマークテストのスコアの総合点やその内訳を生成する必要がある。ベンチマークテストを実施する一部の組織は、その内容を外部に公開しておらず、スコアのみを公開している。テストの内容をあらかじめ公開すれば、AIベンダーがその内容を学習データに組み込み、不正に評価点数を高めることができる恐れがあるからだ。この現象を過学習（オーバーフィッティング）と呼ぶ。過学習は、学習データの特定の情報を記憶してしまい、その情報に関する回答の精度は高いものの、新しいデータに適合できず適切に回答することが難しい状態を指す。

　LLMのベンチマークテストを実施する際は、スクリプト（簡易プログラム）やAPI（アプリケーションプログラミングインタフェース）を使う。プログラミング言語「Python」で書かれたプログラムが、データベースやテキストファイルから問題を読み込み、API経由でLLMにアクセスするといった具合だ。

　ベンチマークテストを実施する際は、出力の再現性を確保し、その再現性を制御できるようにすることが重要だ。イーロン・マスク氏が創業したAI技術ベンダーX.AIの「Grok」、Anthropicの「Claude」、OpenAIのLLM「GPT-4」、Meta Platformsの「Llama」といったLLMは、人間らしいコミュニケーションを実現できるよう、ランダム性を取り入れている。そのようなランダム性を低減するための設定が利用可能なLLMもある。OpenAIのAIチャットbot「ChatGPT」は、ランダム性を調整する設定「Temperature」（温度）を用意している。Temperatureを小さくすれば、ランダム性は下がる。他にも、回答のばらつきを決めるための値「シード値」を固定する、質問するたびに新規ユーザーとして質問するといった方法でランダム性を減らすことが可能だ。使用しているLLMにそのような機能がない場合、テストを複数回実施し、その結果の平均値や中央値を取ることでおおよそのスコアを得ることができる。

本記事は米国Informa TechTargetの記事「Benchmarking LLMs: A guide to AI model evaluation」を翻訳・編集したものです。一部、翻訳作業に生成AIを活用しています。
TechTargetジャパン",[],[]
