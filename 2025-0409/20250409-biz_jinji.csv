headline,mainEntityOfPage,image,datePublished,dateModified,author,media_en,media_jp,str_count,body,images,external_links
その仕事、AIに任せて大丈夫？　今一度整理したい、AIに適した業務・ヒトが担うべき業務（日本の人事部）,https://news.yahoo.co.jp/articles/9100e774950d4f600de67be9c3afb399a3b13228,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20250409-00010000-biz_jinji-000-1-view.jpg?exp=10800,2025-04-09T07:30:17+09:00,2025-04-09T07:30:17+09:00,日本の人事部,biz_jinji,日本の人事部,6545,"中央大学 国際情報学部教授・学部長 平野晋さん
業務効率化のためにAIを活用することは、珍しいことではなくなりました。人事領域でも、給与計算などの定型業務を中心に導入が拡大。採用の過程でAIを活用する企業も徐々に増えていますが、中央大学 国際情報学部 教授・学部長の平野晋さんは、採用や評価といった人事領域での使用は慎重になるべきと訴えます。さまざまなバイアス、AIの特徴や限界、ヒトとAIが協働する道などについて、平野さんに聞きました。
「信頼しすぎる」偏見と「信頼しなさすぎる」偏見
――人事領域に焦点を当てた論文「AIに不適合なアルゴリズム回避論：機械的な人事採用選別と自動化バイアス」を執筆した経緯を教えてください。

これまで内閣府「人間中心のAI社会原則検討会議」の構成員や総務省「AIネットワーク社会推進会議」副議長、「AIガバナンス検討会」座長などを務め、人間社会でAIをどのように活用すべきかを議論してきました。

「AIに不適合なアルゴリズム回避論」を執筆したきっかけは、ゼミ生の経験談です。まだ新しい学部なので、私が受け持つゼミの1期生は2023年3月に卒業したばかりですが、1期生10人中3人から、就職活動中にAIによる面接を受けたと聞きました。母数は小さいですが、3割は決して「珍しい」と片づけられる数字ではありません。人生を左右する決断をAIに任せていいのか。そんな問題意識を抱き、論文の執筆に至ったのです。

――AIに判断を任せることは、一見効率的に思えます。どのような点に問題があると感じたのでしょうか。

大きな問題点として、まずヒトが持つ「自動化バイアス」が挙げられます。ヒトは、自動システムの決定を疑うよりも、むしろ信頼しすぎる傾向があります。AIの予測や判断の正しさ・公正さを精査せずに、AIの言いなりになって追認しがちなのです。

自動化バイアスによる偏見のおそれは、欧州連合（EU）や米国で指摘されています。日本でも、例えば内閣府の「人間中心のAI社会原則」は「AIが活用される社会において、人々がAIに過度に依存したり、AIを悪用して人の意思決定を操作したりすることのないよう、我々は、リテラシー教育や適正な利用の促進などのための適切な仕組みを導入することが望ましい」と明記しています。

「確証バイアス」も考慮しなければなりません。自分が信じるものに関する情報ばかりを探し、相反する情報は無視しがちな偏見のことです。また、自動化が進むにつれてヒトの監視が緩む「自己満足（不適切な監視）」という傾向もあります。

こういった偏見による弊害を根拠に、「ELSI（倫理的・法的・社会的課題）」的な価値観に基づき、AIの慎重な利用を呼び掛ける声が上がっています。EUの『AI規則』では、雇用や人事採用選考などでAIを利用することを「ハイリスク」と分類。米大統領府の『AI権利章典の青写真』でも、労働や教育などを「機微な分野」に位置付け、場合によってはヒトが介入してAIの決定を撤回する必要があるとしています。

一方で、そのような慎重さを批判する立場から、真逆の偏見も指摘されています。「アルゴリズム回避」です。ヒトは、アルゴリズムが予測を誤るのを一度でも目の当たりにすると、次回からは、正確さにかかわらずヒトによる予測・決定を選択する傾向がある。言い換えると、ヒトの誤りには寛容でも、アルゴリズムが犯す誤りには非寛容という考えです。

自動化バイアスのようにAIを“信頼しすぎる偏見”が社会に弊害を生むのなら、逆にアルゴリズム回避のように、“信頼しなさすぎる偏見”も弊害があるのではという主張は、一般論としてはそれなりに説得力がありますよね。

ただ、私が懸念しているのは、アルゴリズム回避の問題意識ばかりを声高に主張すること。AIへの過度な依存が生む悪影響を無視して、AIの普及を促進させようとする姿勢です。私はこれを「アルゴリズム回避論」と呼んでいます。近年、特にビジネスの世界では、アルゴリズム回避論が強すぎるのではないでしょうか。つまり、AIがはらむ課題を無視して、ただ「AIは効率的だから活用するべき」という考えがまん延しているように感じます。

たしかに、AIの高度化や普及は、物事を効率化し、社会問題の解決や経済発展に寄与する大きなエンジンとなります。一方、エンジンが強力になるのであれば、ブレーキやハンドルも同時に高度化しないと、暴走してしまうのです。
AIは万能ではない。使用すべきでないケースも
――どのようなブレーキ・ハンドルが必要だとお考えですか。

まず、AIは万能ではなく、使用すべきでないケースがあることを理解しなければなりません。


【AIを使用すべきでないケース】

1：「AIの予測、推奨、又は判断等がヒトよりも正しい」という立証責任を果たさない場合
（正確性を欠いている場合）

2：AIの特性的・限界的に、利用が妥当ではない場合
・文脈やニュアンスを読まなくてはならない場合
・数値化できない場合
・先例のない事態を予測する場合
・AIサービス提供業者 が十分な資力を持たない場合
・正確な予測に必要なビッグデータが利用不可能な場合
・社会の価値観を反映できる新しいデータを利用できない場合

3：公正さ、尊厳、人道の観点から、決定をAIに移譲すべきでない場合


例えば、総務省の検討会の中で人事の研究者から、人事業務は将来の予測が難しく、AIによる予測精度の向上の幅が狭いという指摘がありました。これは1のケースに当てはまります。このように、そもそも正確性に疑問が残る分野では、「AIの予測、推奨、又は判断等がヒトの判断等よりも正確である」ことを立証する責任が伴うべきですが、その立証責任が十分に果たされていないようです。

AIは、不透明性が大きな欠点です。現状では、人事業務を支援するAIサービスがあっても、そのアルゴリズムは営業秘密を理由に公開されず、従って学者による検証も行われません。つまり、AIによる判断等の論理的な根拠が明示されていない。これでは正確かどうかわかりません。

では、どうすれば立証責任を果たしたと言えるのでしょうか。人生を左右する採用・評価といった重大かつ「ハイリスク」で「機微な分野」では、厳格な対応が必要です。具体例を挙げると、内部監査に加えて、独立・中立的な第三者による監査の実施。その監査結果の公表に加え、AIによる判断等の根拠の説明が求められるでしょう。第三者の研究者によって、AIがヒトよりも正しいことが検証されることも望まれます。

金融庁の抜き打ち検査のようなことまで求めるのは厳しすぎるかもしれませんが、業務効率化を理由に不透明なまま使用していることは問題です。現実的にどこまでやるのか、これから議論を重ね、着地点を模索しなければなりません。

ちなみに、日本の『AI利活用原則』では、意思決定に対して納得できる理由を必要とする場合の例として人事評価を挙げ、「社員に対し評価の理由を説明できることが期待される」と説明しています。

――そもそも人事採用業務では、不合格の理由を細かく説明することが少ないと思います。それでも、AIを活用する際は厳格な対応が必要でしょうか。

講演会などでAIの問題点を指摘すると、「ヒトの判断にも誤りがあるのに、AIによる判断にだけ透明性や説明責任を求めるのはおかしい」という声を聞くことがあります。これはつまり、「ヒトの誤りが見過ごされてきたのだから、AIも見逃されるべき」と主張していることになります。私は法学者なので、この主張は受け入れがたいし、社会も許すべきではありません。「AIにだけ」ではなく、ヒトとAIの双方に対して策を講じるべきです。

実際に社会の価値観は、ヒトによる決定にも透明性や説明責任を求める方向に変化しています。修士号の学位の資格審査では、単に合否という結論だけを恣意的に決めることは許されません。ディプロマ・ポリシー（学位授与方針）を満たす要素を事前に決め、評価対象者が各要素をどの程度満たしたかを点数化し、複数人の合議で決めます。このように、必要な場合は理由や根拠を提示できる体制が、ヒトによる決定にも求められています。AIも、こういった社会の価値観に従うべきです。

――「AIの特性的・限界的に、利用が妥当ではない場合」とは、どのようなケースでしょうか。

例えば、一定期間無職で過ごした者の履歴書を、AIで自動的に不合格とするケースを考えてみましょう。「一定期間無職」という基準に沿って判断するので、公平な方法だと感じるかもしれません。しかし、無職の理由が結婚や出産、育児などであればどうでしょうか。今すぐ働ける状態で、意欲も能力もある求職者なのに、面接で事情を聞く機会すら設けないことになります。求職者にとってはもちろんのこと、企業にとっても大きな機会損失です。

採用は、求職者のさまざまな事情を考慮すべき業務ですが、利便性や効率、低コストなどを理由に、具体的妥当性を欠き、正確性も疑わしく、柔軟性のないAI利用が広がっているのではないでしょうか。そもそも人間に関する決定を機械に許すことは、人間を物として取り扱うことになる、という倫理的な批判もあります。人間の尊厳に関わる重要な問題提起です。
使う側・提供する側ともにトレーニングが必要
――最終的な判断はヒトが担うという運用方法であれば、問題点は解消できますか。

形式的に「最終判断はヒトが行う」工程を設けるだけでは意味がありません。先ほど指摘した通り、ヒトは機械に対してさまざまなバイアスを持っています。逆に「ヒトが監視・判断しているから絶対安心」という誤ったメッセージにもなりかねません。重要なのは、AIを監視し最終的に判断するヒトが、AIの特徴や限界を正しく理解していることです。

履歴書の合否に活用するケースでは、採用担当者が、AIによって不合格となった履歴書に目を通した上で最終判断すれば大丈夫、という考え方もあります。では、その担当者はしっかりとトレーニングを受けているのでしょうか。AIを信頼しすぎる「自動化バイアス」の存在を認識していなければ、担当者によるチェックは意味をなしません。「有意な判断」になっていないのです。

まずは人事担当者が、自身が持つバイアスやAIの特徴・限界を理解できるトレーニングを受けられる環境が必要です。利用する側だけの問題ではありません。AIサービスを提供するベンダーも同様です。

少し極端な例ですが、ある中小企業が、日本企業の膨大な人事データを読み込ませたAIを使って「長く働いてくれそうな人材」を予測させて採用基準にしたとします。これは、男性を優先する間接差別につながりかねません。AIは蓄積した膨大なデータを基に計算・予測するため、「日本企業の平均勤続年数は女性より男性の方が長い」という統計的には正しい客観的なデータから、「男性を採用すべき」という、女性が不利益を被る結論を導いてしまう可能性があります。アルゴリズムを組んでサービスを提供する側も、単に統計や数値だけに頼って推奨や判断等をさせるのではなく、公平性や法令順守等のトレーニングも求められるのです。

ただ、AIの特徴や限界が学べる体系的なプログラムは、まだ整備されていないようです。政府は積極的にAI活用を推奨しているので、国を挙げて検討しなければならない課題ですね。AIの分野では「STEM教育」が重要視されてきました。つまり、科学・技術・工学・数学です。今後は「ELSI教育」、倫理的・法的・社会的課題に対する教育が大切になります。ベンダーがELSIに対して高い意識を持てば、企業から「長く働いてくれそうな人材」というオーダーを受けた際、「このような理由により、差別的な結論になりかねません」といったアドバイスも可能です。

――トレーニングを受けた上で、実務ではどのような対策が考えられますか。

事前にチェックシートを作成すると有効でしょう。自社はどのような基準で採用の可否を決めるのかをしっかりと記し、そのチェックシートを基にAIの判断等を監視する。バイアスの影響を抑え、客観的に対処できます。AIを抜きにしても、「チェックシートのこの部分に沿っていないから不合格」「この基準に合致しているので合格」と、自分の言葉で説明できなくてはなりません。もちろん、AIの判断にNOを出せる権限を採用担当者に与える必要もあります。

この他、自社の採用にAIを活用すると明らかにした上で、合否の判断基準を発信する手もあります。個人情報保護に関する法整備が進んでいない頃、自社のコンプライアンスを示すため、各社が独自に個人情報保護に関する取り組みを開示していましたね。さらに、例えばプライバシーマークが信用の基準になっているのと同様に、AI活用に関してもオープンな姿勢を示すことで求職者から信頼される。今のうちに先駆けて取り組めば、信頼性の高い会社という、一種のブランディングにつながります。

――政府にはどのような対応が求められるでしょうか。

まずは、AIの判断により、採用や異動などで不利益な処分を受けた人に対して、処分の理由や根拠を知り、異議申し立てができる権利を与えるべきです。『OECD AI原則』には、AIを使った企業は「どういったアルゴリズム、データ、理由によって、このような判断になった」と説明する義務があると記載されています。日本はこのAI原則の策定を推進しておきながら、異議申し立てに関する取り組みが進んでいません。一方、米国には条例で、AIによる選別以外の審査を受ける権利を保障しているところもあります。日本は欧米と比べて後進国ですが、先進国の良い例を見て学ぶことはできます。

これまで日本では法規制が必要か、AIに賛成か反対か、といった抽象的な議論が活発でしたが、もっと具象から考えるべきです。どのような問題が具体的に起きているかを、アメリカの先進的な事例等から個別で考えてみると、今の法律で対策できる問題があるかもしれない。男性優位・差別的なアウトプットに基づいて採用を行っているという問題があるならば、男女雇用機会均等法違反に基づいて法執行するという道もあります。さらに、もし今の法律では規律できない問題として「野放しにしてはまずい」という社会の価値観が集まれば立法も必要になるでしょうし、すでに経済産業省がまとめたガイドラインなどもあるので、参考になります。
AIに適した業務・ヒトが担うべき業務の見極めを
――人事の方にメッセージをお願いします。

AIを活用するのであれば、その特徴や課題、対策を勉強して、AIに「適合した」使い方をすべきです。客観的なルールに基づいた判断は得意でも、採用や評価など、さまざまな考慮要素を踏まえた裁量に基づく総合判断は、今のAIには難しい。

少し前に、AIによって仕事が奪われる職業は何かという議論が盛り上がりました。しかし、職業がなくなるというアバウトな指摘は正確ではなく、ある職業の一部のタスクをAIが担うことになる、という分析の方が精緻です。人事業務のうち、どれがAIに適していて、どの役割はヒトが担うべきなのか。AIとヒトのタスクをしっかりと分けるべきです。まだ正解はありませんが、それを模索し、適合する業務を見極めたハイブリッドな運用を目指してほしいですね。
プロフィール
平野 晋さん（中央大学 国際情報学部 教授・学部長）
ひらの・すすむ／中央大学法学部法律学科卒業。コーネル大学大学院修了（法学修士）。ニューヨーク州弁護士。株式会社NTTドコモ法務室長、コーネル大学ロースクール客員研究員、中央大学総合政策学部教授などを経て、2019年の中央大学国際情報学部（iTL）の立ち上げに携わり、同年より同学部教授・学部長。",[],[]
