headline,mainEntityOfPage,image,datePublished,dateModified,author,media_en,media_jp,str_count,body,images,external_links
マイクロソフト「Phi-4」が示す小型モデルの衝撃、オープンソースモデル競争も激しさ（ビジネス＋IT）,https://news.yahoo.co.jp/articles/db3d8c5a2ec5ac80b09e0d322f5418455baecdbb,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20250301-00158760-biz_plus-000-1-view.jpg?exp=10800,2025-03-01T07:10:05+09:00,2025-03-01T07:10:05+09:00,ビジネス＋IT,biz_plus,ビジネス＋IT,4120,"マイクロソフト「Phi-4」の強みとは？（出典：Hugging Face）
中国DeepSeekが大きな話題を集めていた陰で、マイクロソフトが発表した小型言語モデル「Phi-4」が、前モデル「Phi-3.5」以上の衝撃の結果をもたらしている。140億パラメータという比較的小規模なモデルでありながら、数倍～数十倍も大きな大規模モデルを凌駕する性能を示したためだ。同モデルはオープンソースで公開されているのも特徴で、Mac miniなど一般的なマシンでも快適に動作する機敏性も備えている。Phi-4の実力を探ってみたい。
【詳細な図や写真】小規模なモデルで数十倍大きな大規模モデルを凌駕（Photo：Rokas Tenys / Shutterstock.com）
マイクロソフト「Phi-4」とは？高性能の秘密
2024年12月、マイクロソフトが新しい言語モデル「Phi-4」を発表した。140億パラメータという比較的小規模なモデルながら、OpenAIのGPT-4o miniや、700億パラメータのLlama 3.3など、はるかに大きなモデルに匹敵、あるいは凌駕する性能を示し、AIコミュニティに衝撃を与えた。


　トレーニング手法を刷新したことがこのブレイクスルーにつながったようだ。

　従来の言語モデルがWebコンテンツやコードなどのオーガニックデータを主体にプレトレーニングを行うのに対し、Phi-4は合成データを戦略的に活用。前モデルのPhi-3シリーズでは、フェーズ1でフィルタリングされたWebデータを中心に学習し、フェーズ2で合成データと高品質なWebデータを組み合わせるという2段階の戦略を採用していた。

　しかし、合成データの規模と複雑さが増すにつれ、非合成データの効果が限定的であることが判明。実際、ベンチマークテストでは、合成データでの追加トレーニングの方が、新しいWebデータの追加よりも効果的という結果が得られている。

　この知見を活かし、Phi-4では合成データの割合を大幅に増加。トレーニングデータの40％を合成データが占める。このほか、Webリライトと呼ばれる合成データが15％、フィルタリングされたWebデータが15％、コードデータが20％、その他の取得データが10％という構成となっている。


　このデータ構成の刷新により、Phi-4は前モデルから大幅な性能向上を実現。たとえば、学部レベルの知識を測るMMLUでは84.8％を記録し、Phi-3（77.9％）を約7ポイント上回った。さらに、大学院レベルの推論能力を測るGPQAでは56.1％を達成し、Phi-3（31.2％）から実に25ポイント近い改善を見せている。

　特筆すべきは、これらの性能向上がモデルサイズを大きくすることなく実現されている点だ。同じ140億パラメータのQwen 2.5-14B instructと比較しても、MMULで約5ポイント、GPQAで約13ポイントの優位性を示している。
Phi-4の強み1：難関テストで30倍近い大きなモデルを凌駕
Phi-4の強みが特に際立つのが、大学院レベルの科学的推論能力を測るGPQAテストでの結果だ。生物学、物理学、化学の分野で448問の質問を用意し、そのうち198問からなるセットで評価を行うこのテストで、Phi-4は56.1％という高スコアを記録。これはClaude 3 Opus（50.4％）やGPT-4o（53.6％）をも上回る数値となっている。

　GPQAテストの特徴は、単なる知識の暗記や表面的なWeb検索では対応できない「Google-proof」な設計にある。生物学、物理学、化学の各分野における専門性の高い問題が出題され、単に情報を組み合わせるだけでは正解にたどり着けない。

　実際、専門分野で博士号を持つ、あるいは博士課程に在籍する専門家でさえ、平均65％程度の正答率にとどまる。また、Web検索が許可された非専門家のバリデーターは、1問あたり30分以上の時間をかけても、正答率は34％に留まったと報告されている。

　そのような高難度のテストで、Phi-4が56.1％という数値を叩き出した意義は大きい。同じく140億パラメータのQwen 2.5-14B instructが42.9％、700億パラメータのLlama 3.3が49.1％という結果を示していることを考えると、Phi-4の効率性が際立つ。

　また、30倍ほど大きなメタのLlama3.1 405B（4050億パラメータ）のGPQAスコア（51.1％）と比べても、Phi-4の高効率性をうかがうことができる。


　GPQAテストの問題を解くには、深い科学的理解と高度な推論能力が要求される。たとえば、物理学の問題では単に公式を知っているだけでなく、その公式がどのような条件下で適用可能か、またその結果がどのような物理的意味を持つのかを理解していなければならない。化学の問題では、分子の振る舞いや反応メカニズムについての理解が必要で、生物学では複雑な生命システムの相互作用を理解し、その影響を推論する能力が問われる。

　このような深い理解と推論を必要とする問題に対し、Phi-4は人間の専門家に迫る性能を披露。これは、同モデルがテキストの表面的な特徴や統計的なパターンだけでなく、科学的概念の本質的な理解を獲得できていることを示唆するものといえるだろう。
Phi-4の強み2：米数学オリンピック問題で大規模モデルを圧倒
Phi-4は数学分野でも、そのパフォーマンスの高さを示している。特に米国数学協会（MAA）が主催する数学コンペティション「AMC-10/12」における成績で圧倒的なパフォーマンスを見せつけた。


　2024年11月に実施された同テストで、Phi-4は平均91.8ポイントという高スコアを記録。Gemini Pro 1.5（89.8ポイント）、Claude 3.5 Sonnet（74.8ポイント）、GPT-4o（77.9ポイント）など、競合モデルを大きく引き離す結果となった。

　AMC-10/12テストは米国数学オリンピックの予選を兼ねており、毎年15万人以上の学生が参加する。4つのバージョン（10A/10B/12A/12B）が用意され、それぞれに150点満点の25問の問題が含まれている。問題の難度は徐々に上がり、後半の問題では高度な数学的推論が要求される。

　Phi-4のトレーニングデータは2024年11月のテストより前に収集されており、このテストの情報は含まれておらず、同モデルは初見で90％以上のスコアを達成。単なる暗記や過去問の学習ではなく、真の数学的推論能力に基づく能力を持つことが示された格好となる。

　興味深いのは、テスト時の温度設定（temperature）が0.5と比較的高めに設定されている点だ。温度設定を下げれば、より安全な回答を選択できる可能性があるにもかかわらず、Phi-4は高めの温度設定でも安定して高得点を記録した。これは、モデルが数学的な推論を確実に行えていることの証左となる。

　このような高度な数学的推論能力は、他のベンチマークでも確認されている。数学オリンピックレベルの問題を含むMATHベンチマークでは80.4％を記録し、同規模のQwen 2.5-14B instruct（75.6％）やGPT-4o mini（73.0％）を上回る結果を示した。
Mac miniでもサクサク動く機敏性も魅力
Phi-4の魅力は、高い性能だけでなく、そのコンパクトなモデルサイズにもある。140億パラメータという比較的小規模なモデル設計により、Mac mini M4など一般的なマシンでもスムーズな動作を実現。これまで高性能なGPUや大容量メモリが必須とされてきた大規模言語モデルの導入ハードルを、大幅に引き下げている。

　実際、Mac mini M4 Pro（GPUコア16基、メモリ24GB）でも、Phi-4は1秒あたり約10トークンの生成速度を維持することが複数のテストで確認されている。これは日常的な会話や文章生成に十分な速度だ。一方、700億パラメータクラスのモデルでは、同じマシンで1秒あたり5トークン程度まで速度が低下、またはメモリ不足でストップすることもあり得る。

　AIモデルのメモリ使用量をGPUベースで見ると、一般的に6GBのVRAMで80億パラメータ、12GBで180億パラメータ、16GBで230億パラメータまでのモデルが動作可能とされる。Phi-4は140億パラメータということで、Mac mini M4 Pro（メモリ24GB）でも余裕を持って動作する計算となる。

　特に、Apple Siliconに搭載されたNeural Engineは、AI処理に最適化されている。これにより、専用GPUがなくても効率的なモデル実行が可能だ。実際、Mac向けのLM StudioやOllamaなどのツールは、このNeural Engineを最大限活用するよう最適化されており、Phi-4の性能を余すことなく引き出すことができる。

　このような機敏性は、企業のAI導入を考える上で重要な意味を持つ。大規模なGPUクラスタやメモリを必要としないことで、初期投資を抑えられるだけでなく、運用コストも大幅に削減できるためだ。また、ローカル環境で動作することで、データのプライバシーやセキュリティも確保しやすい。さらに、インターネット接続を必要としないため、クラウドサービスの障害や通信の遅延に影響されることもない。

　Phi-4はLlamaやDeepSeek と同様、オープンソースで公開されており、Hugging Faceからダウンロードして利用することができる。また、Ollamaなどでも利用可能になっている。これらオープンソースモデルの競争も目を離せない。
執筆：細谷 元、構成：ビジネス＋IT編集部",[],[]
