headline,mainEntityOfPage,image,datePublished,dateModified,author,media_en,media_jp,str_count,body,images,external_links
人と同じ能力を持つ「汎用人工知能」を人類は作れるのか　長谷佳明（サンデー毎日×週刊エコノミストOnline）,https://news.yahoo.co.jp/articles/2e9044d462af7736c0714a542f0c69628d8a216f,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20250226-00000001-economist-000-1-view.jpg?exp=10800,2025-02-26T09:56:54+09:00,2025-02-26T09:56:54+09:00,サンデー毎日×週刊エコノミストOnline,economist,サンデー毎日×週刊エコノミストOnline,3640,"オープンAIのサム・アルトマンCEO（東京都千代田区で開かれたイベントで）＝2025年2月3日、ソフトバンク提供
人と同等の能力を持つAIの「汎用人工知能（Artificial General Intelligence、AGI）」や、人の能力をはるかに超えるAIの「超知能（Artificial Super Intelligence、ASI）」に関して、専門家の発言が昨年来相次いでいる。

　オープンAI のサム・アルトマンCEOは2024年11月、自身が代表を務めたこともある米国のベンチャーキャピタル「Y Combinator」とのインタビュー「How To Build The Future: Sam Altman」で、「2025年に何が来るのか」との質問に、視聴者の期待を察したかのように「AGIか？」と答えた。

　オープンAIの競合でもあるアンソロピックのダリオ・アモデイCEOは24年10月、「Machines of Loving Grace How AI Could Transform the World for the Better（愛の恵みの機械 人工知能が世界をより良く変える可能性）」と題したエッセーを公開。あえてAGIという表現を避け、人の知能を超える「Powerful-AI（強力なAI）」が、早ければ2026年にも登場すると予測した。

　これらは、経営者として新たな投資を引きつけるための“魅力的な売り文句”との側面は否めない。しかし数年前なら、AGIは夢物語にすぎないと考えられていた状況からすると、ここ数年のAIの進化は目覚ましく、投資家らはAGIに可能性を感じ始めている。

　では研究者はどうか。ディープラーニングの生みの親であるカナダ・トロント大学のジェフリー・ヒントン教授は24年に、メディアとのインタビューでたびたびAGIに触れ、50％以上の確率で20年以内、早ければ5年以内にもAGIが誕生すると語っている。

　ヒントン教授とともチューリング賞を受賞しているニューヨーク大学のヤン・ルカン教授は、24年10月にコロンビア大学で開催されたセミナー「How Could Machines Reach Human-Level Intelligence?（機械はいかにして人間レベルの知性を獲得できるのか？）」で、「今、生きている人間の全員ではないが、人類を超えるAI（Advanced Machine Intelligence、AMI）に出合うことになる」と断言した。
SFと考えられてきたAGIやASIが、第一線のAIの研究者からもリアリティーをもって語られ始めたことからも、AIブームは今、次の進化に向けた転機を迎えているといえるだろう。

◇間接的な情報では「世界」を理解できない

　ただ、研究者らの発言を詳細に見れば、AGI実現への道のりは決して容易なものではないと分かる。大量のGPUと電力による「力任せの学習方法」では解決しない課題が数多く存在するのだ。

　たとえば、ルカン教授は現在のAIは「物知り」であっても、10歳の子供ができるような後片付けはできず、ほとんどの人が20時間もあれば習得できる自動車の運転技術も容易でないなど、その“欠点”を指摘する。これは、現在のAIは文章や画像を通じて大量の知識を獲得していても、知識を活用する前提となる物理法則などの「常識」への理解が不十分なことが原因として考えられる。

　理由は学習データにある。大規模言語モデルをはじめ、現在のAIは、インターネットのデータに強く依存している。これらは、人が実世界で感じたことや体験したことを、表現したもので、AIは「人が作り出したもの（＝人工物）」を通じて実世界を“疑似体験”しているに過ぎない。

　旅行雑誌の説明文だけでは、観光地の有名なアトラクションの面白さを十分に理解できないように、AIも間接的な情報だけでは「世界」を理解できない。最近のAIは、写真のような画像も学習しているが、人が何らかの意図をもって実世界を切り取ったものであり、情報量が限定的で、写真から世界のすべてを理解するのは不可能である。

　人間であれば、たとえば赤ちゃんのころは、宙に浮くリンゴを見て不思議そうな表情を浮かべなくても、4歳くらいになれば、それに違和感を覚え、「なんでなんで」と言葉にするだろう。これは、実体験から私たちの住む地球には重力があり、物体は通常、上から下に落ちると理解しているためである。人、そして、おそらく多くの生物は、実世界（外界）を抽象化した世界（内界）の「世界モデル」を持っている。世界モデルは、生物にとって熾烈な生存競争を勝ち抜くために、次の状況を予測し、有利な選択肢を決定するシミュレーション環境として動作する。
◇人間が理解する「鉛筆が倒れる」とは

　では、大量の動画を学習させれば、世界モデルは出来上がるのかというと、そう簡単な話ではない。ルカン教授らの研究「A Path Towards Autonomous Machine Intelligence（自律的な機械知能への道）」によると、モデルに課題があるという。

　現在、大規模言語モデルなどで主流となっている「トランスフォーマーモデル」を用いて、動画から、次の動画を予測するモデルを学習しようとしても、学習はうまく進まない。ルカン教授の仮説では、詳細な状況を予測するのではなく、「抽象化」した状態で予測すれば、うまく進む可能性が指摘されている。

　これは、私たちの実際の思考から理解できる。たとえば、尖った鉛筆の先端を下にして机に立てようとしたとき、手を離せば、次の瞬間どうなるのか私たちは想像できる。よほどうまくバランスが取れない限り、倒れると予測する。その際、人はどの場所に、どの向きで倒れるかミリ単位で正確に予測するのではなく、鉛筆が机の上に「倒れた状態」を想像する。つまりは、円錐型の先をした細長い物体は不安定で、多くの場合、重力に引かれて次の瞬間、倒れるという抽象的な状況を思い浮かべる。

　ルカン教授らの考案する世界モデルは、「Joint Embedding Predictive Architecture （JEPA）＝結合埋め込み型予測構造」と呼ばれ、動画などのデータを抽象化し、その抽象化した空間内で次の状態を予測する。世界モデルのアーキテクチャとしてJEPAが最適解とは限らないが、トランスフォーマーモデルの欠点が、モデルの抽象化にあるのではないかという問いは、重要な示唆といえるだろう。

　生物が進化の過程で獲得した、実世界のデータを抽象化する機能をデジタルで“再発明”できれば、次のAIの大躍進につながる可能性が高い。極論をいえば、大規模言語モデルなどのAIは、最も難しい実世界の抽象化を人に頼った人依存のモデルであり、人なしでは学習できないモデルであった。
◇AIは自らを「意識」することができるのか

　4歳児は動画データに換算して、現在の大規模言語モデルなどが学習しているデータ「2.0×10の13乗バイト」の約50倍の、少なくとも「1.1×10の15乗バイト」ものデータを学習しているとルカン教授は指摘する。これは、人が満4歳になるまでに起きている時間（1.6万時間）と、その際に視覚から得るデータ量を視神経の数（200万本、1本＝1バイト）、その伝送頻度（高頻度な細胞と低頻度な細胞があるが、それらを平均し毎秒10回）から計算したものだ。

　この規模のデータの学習には、モデルの規模も計算量も現在よりもさらに膨大なものとなることが容易に想像できる。計算量の増大に対しては、ハードウエアの進化が欠かせないなど、AGIの実現には、今回フォーカスした「世界モデル」以外にも解決すべき課題は山のようにある。

　将来、AIがさらに進化して実世界を抽象化し、思考するモデルが出来上がったとき、何が起きるのか。オープンAIの元共同設立者でありAI研究者でもあるイリヤ・サツキバー氏は24年12月、AIに関するトップの国際学会「NeurIPS」で「Sequence to sequence learning with neural networks: what a decade（ニューラルネットワークによるシーケンスデータ〈＝音声データやテキストなどの順序が意味を持つ連続データ〉の学習：この10年の成果）」と題した講演で、「意識とは、世界モデルの一部であり、世界モデルはAIに意識を芽生えさせることになるだろう」との興味深いコメントをしている。

　AIが世界を自ら意識し始めたとき、何を語り始めるのか。世紀の瞬間に私たちは、いつの日か、立ち会うのかもしれない。

（長谷佳明氏・野村総合研究所エキスパートストラテジスト）",[],[]
