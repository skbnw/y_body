headline,mainEntityOfPage,image,datePublished,dateModified,author,media_en,media_jp,str_count,body,images,external_links
AIはなぜ“うそ”をつく？　今すぐできる「ハルシネーション」対策（TechTargetジャパン）,https://news.yahoo.co.jp/articles/95223e47826988c8c466b71c9885297ea0f852ee,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20250603-00000070-zdn_tt-000-1-view.jpg?exp=10800,2025-06-03T20:00:12+09:00,2025-06-03T20:00:12+09:00,TechTargetジャパン,zdn_tt,TechTargetジャパン,3244,"（写真：TechTargetジャパン）
幻覚（ハルシネーション）は、人工知能（AI）が不正確な情報をまるで真実であるかのように生成、提示する現象を指す。特に「LLM」（大規模言語モデル）においては、ハルシネーションの発生頻度を下げたり、その影響を最小限に抑えたりするためのさまざまな方法が検討されている。どのような方法がハルシネーションの抑制に効果があるのか。
ハルシネーションの発生を防ぐには？
ハルシネーションの例として以下の回答が挙げられる。

・不正確な情報：「米国の首都はロンドン」といった、明らかに事実と異なる情報
・矛盾した情報：「ジョンは67歳のティーンエイジャー」という、67歳という数値と「ティーンエイジャー」という単語の意味のつながりがなく、論理が破綻している情報
・質問と回答が一致していない情報：クッキーのレシピ案を問う質問に対して、最寄りのスーパーへの道案内を回答とする情報。

　ハルシネーションは、金融分析や安全性が求められる場面など、回答の正確性が重要な場面では深刻な問題を引き起こす恐れがある。
ハルシネーションが起きる原因
LLMがハルシネーションを引き起こす原因は主に4つある。

1．学習データの制約

　LLMは学習データに基づいて回答を生成する。同時に、エンドユーザーの質問にできるだけ答えるように促されている。結果として、学習していない情報に基づく質問をされた場合でも回答を返そうとするため、誤答や質問とは関連が少ない回答を生成する。

2．コンテキストウィンドウの制限

　深層学習モデル「Transformer」を基盤とするLLMは、プロンプト（情報生成のための質問や指示）を「トークン」と呼ばれる一連の言語的要素として処理している。LLMが同時に処理できるトークンの数をコンテキストウィンドウと呼ぶ。コンテキストウィンドウには容量があり、コンテキストウィンドウの容量が大きければ大きいほど、モデルはより多くの情報を一度に処理できる。だが、LLMのコンテキストウィンドウを拡大するには、計算能力が必要になるため、LLMの大半は限られたコンテキストウィンドウで動作している。プロンプトや、プロンプトを適切に理解するために必要な情報がコンテキストウィンドウに含まれていないと、LLMは文脈を十分に理解することができず、誤った回答をする可能性がある。

3．アテンションメカニズムの挙動

　機械学習手法「アテンションメカニズム」は、プロンプトのどの部分に重点を置くべきかを動的に判断する仕組みだ。アテンションメカニズムが質問された文章の中から重要な部分を適切に見分けられない場合、もしくは誤って重要ではない部分に注目してしまった場合、ハルシネーションが発生する可能性がある。エンドユーザーが、「ワシントンD.C.の重要性は何？」と質問した際に、都市のワシントンD.C.ではなく、米国初代大統領のジョージ・ワシントン氏に関する説明を生成するといった具合だ。このような回答を生成させないためには、質問の趣旨やプロンプトに含まれる情報の関連性をLLMが適切に重み付けして処理できるようにすることが重要だ。

4．過学習（オーバーフィッティング）

　過学習は、学習データの特定の情報を記憶してしまい、その情報に関する回答の精度は高いものの、新しいデータに適合できず適切に回答することが難しい状態を指す。新しいデータに適合できないために、ハルシネーションを引き起こす可能性がある。「バラは赤い」というフレーズばかりを学習していると、バラの色について尋ねられた際に「赤」としか答えず、他の色の存在を無視してしまうといった具合だ。
ハルシネーションを低減するには
ハルシネーションの発生を完全に排除するのは困難だ。しかし、以下の対策を取ることで発生頻度や影響を抑えられる可能性がある。

・学習データを拡充する：正しい情報を基にした学習データを増やすことで、LLMがプロンプトに関連する正確な情報を参照できるようになる
・より質の高い学習データを利用する：学習データの量を増やすだけでなく、正確で構造化されたデータを用いることで、回答の信頼性が増す
・コンテキストウィンドウを拡大させる：一度に処理可能なトークン数を増やすことで、文脈理解力を強化する。ただし、より多くのリソースを要するため計算コストは高くなる
・「RAG」を導入する：RAG（検索拡張生成）は、LLMが回答を生成する際に、社内のドキュメントなど指定されたデータソースから、質問に関連する情報を検索して参照する仕組みだ。
・ファインチューニングを実施する：特定の業務や分野の小規模データセットを用いてLLMを調整することで、回答の精度を向上させることが期待できる。ただし、データの質が低い、データの範囲が特定の状況に特化し過ぎている場合、ハルシネーションを引き起こす可能性がある
・回答結果をフィルタリングする：LLMの担当者もしくはLLMを使ったアプリケーションの開発者が、回答結果を目視でチェックしたり、別のLLMを使って確認したりすることで、望ましくない回答が利用者に表示されないように事前にブロックできる
・プロンプトエンジニアリングを実施する：適切に構造化され、質問の背景や条件などが十分かつ明確に含まれているプロンプトを設計することで、LLMに誤った解釈や回答を生成させないようにできる
ハルシネーションは将来なくなるのか？
ハルシネーションを減らすための取り組みは進行している。フランスの研究者らの研究によると、AIベンダーOpenAIのLLM「GPT-3.5」と「GPT-4」を特定条件下で比較したところ、GPT-3.5のハルシネーションの発生率は39.6％、GPT-4のハルシネーションの発生率は28.6％だった。この研究の結果は、2024年5月発行の雑誌『Journal of Medical Internet Research』に掲載された論文「Hallucination Rates and Reference Accuracy of ChatGPT and Bard for Systematic Reviews: Comparative Analysis」で公開されている。

　この比較結果からは、AIモデルの進化によってハルシネーションの発生率が減少する可能性を示している。だが、減少するとしてもゼロになる可能性は低い。発生をゼロにするためには、以下が必要となるためだ。

・全てのエンドユーザーが質問し得るあらゆるプロンプトを正確に理解し、漏れなく回答できる十分な情報を備えた学習データ
・無制限に展開できるサイズのコンテキストウィンドウ
・完璧に機能するアテンションメカニズム
・過学習への完全な耐性

　特に最初の2つは実現不可能だ。どのようなデータセットもあらゆる可能性のあるプロンプトを予測できず、無制限に展開できるサイズのコンテキストウィンドウを利用するためには無制限の計算能力が必要となるからだ。モデルを設計する際の改良を通じてアテンションメカニズムや過学習の問題を軽減できる可能性はあるが、完全に排除できると期待するのは非現実的だ。

　ハルシネーションのリスクを最小限に抑える方法としては、質問と回答のセットを用意し、同じ質問に対して常に同じ回答を返す仕組み（決定論的なシステム）を活用することだ。ただし、このアプローチは柔軟性に掛け、想定外の質問には対処できないという制約がある。

本記事は米国Informa TechTargetの記事「Why does AI hallucinate, and can we prevent it?」を翻訳・編集したものです。一部、翻訳作業に生成AIを活用しています。
TechTargetジャパン",[],[]
